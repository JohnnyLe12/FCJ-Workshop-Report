[{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.7-security/5.7.1-encryption-at-rest/","title":"Encryption at Rest","tags":[],"description":"","content":"Encryption at Rest - Data Protection What is Encryption at Rest? Encryption at Rest means encrypting data when it\u0026rsquo;s stored on disk (at rest), as opposed to data in transit (being transmitted over network).\nWhy it matters:\nProtects data if physical storage is compromised Compliance requirement (GDPR, HIPAA, PCI-DSS) Defense in depth security strategy Minimal performance impact with AWS managed encryption Implementation Overview We implemented encryption for:\nDynamoDB Tables (6 tables) - Using AWS KMS S3 Buckets (2 buckets) - Using AES-256 DynamoDB Encryption Tables Encrypted All DynamoDB tables now use AWS KMS encryption:\nArticlesTable UserFavoritesTable GalleryPhotosTable GalleryTrendsTable UserProfilesTable LocationCacheTable Configuration CloudFormation Template Changes:\nArticlesTable: Type: AWS::DynamoDB::Table Properties: TableName: !Sub \u0026#34;articles-${Environment}\u0026#34; # ... other properties ... # ‚úÖ ADDED: KMS Encryption SSESpecification: SSEEnabled: true SSEType: KMS How it Works 1. Application writes data to DynamoDB ‚Üì 2. DynamoDB encrypts data using KMS key ‚Üì 3. Encrypted data stored on disk ‚Üì 4. When reading, DynamoDB decrypts automatically ‚Üì 5. Application receives decrypted data Key Points:\nEncryption/decryption is automatic No code changes required Transparent to application Uses AWS-managed KMS keys Benefits ‚úÖ Security:\nData encrypted at rest Protection against disk theft Compliance with regulations ‚úÖ Ease of Use:\nFully managed by AWS Automatic key rotation No key management burden ‚úÖ Performance:\nMinimal latency impact (\u0026lt;1ms) No application changes needed Cost DynamoDB KMS Encryption:\nKMS key: $1/month API calls: ~$0.03 per 10,000 requests Estimated: $5-10/month for typical usage S3 Encryption Buckets Encrypted Both S3 buckets now use AES-256 encryption:\nArticleImagesBucket (user uploads) StaticSiteBucket (frontend assets) Configuration CloudFormation Template Changes:\nArticleImagesBucket: Type: AWS::S3::Bucket Properties: BucketName: !Sub \u0026#34;travel-guide-images-${Environment}-${AWS::AccountId}\u0026#34; # ... other properties ... # ‚úÖ ADDED: AES-256 Encryption BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 BucketKeyEnabled: true How it Works 1. Application uploads file to S3 ‚Üì 2. S3 encrypts file using AES-256 ‚Üì 3. Encrypted file stored on disk ‚Üì 4. When downloading, S3 decrypts automatically ‚Üì 5. Application receives decrypted file Key Points:\nEncryption is server-side Automatic for all objects No client-side changes needed Uses S3-managed keys (SSE-S3) Benefits ‚úÖ Security:\nAll files encrypted Protection against unauthorized access Compliance ready ‚úÖ Cost:\nFREE - No additional cost for SSE-S3 No performance impact ‚úÖ Simplicity:\nEnabled at bucket level Applies to all objects No code changes Deployment Step 1: Update CloudFormation Template File: travel-guide-backend/core-infra/template.yaml\nChanges made:\nAdded SSESpecification to all DynamoDB tables Added BucketEncryption to all S3 buckets Step 2: Deploy Stack cd travel-guide-backend # Deploy core infrastructure ./scripts/deploy-core.sh staging # Or manually: sam build -t core-infra/template.yaml sam deploy \\ --stack-name travel-guide-core-staging \\ --parameter-overrides Environment=staging \\ --capabilities CAPABILITY_IAM Step 3: Verify Encryption Verify DynamoDB:\n# Check table encryption aws dynamodb describe-table \\ --table-name articles-staging \\ --query \u0026#39;Table.SSEDescription\u0026#39; # Expected output: { \u0026#34;Status\u0026#34;: \u0026#34;ENABLED\u0026#34;, \u0026#34;SSEType\u0026#34;: \u0026#34;KMS\u0026#34; } Verify S3:\n# Check bucket encryption aws s3api get-bucket-encryption \\ --bucket travel-guide-images-staging-123456789012 # Expected output: { \u0026#34;ServerSideEncryptionConfiguration\u0026#34;: { \u0026#34;Rules\u0026#34;: [ { \u0026#34;ApplyServerSideEncryptionByDefault\u0026#34;: { \u0026#34;SSEAlgorithm\u0026#34;: \u0026#34;AES256\u0026#34; }, \u0026#34;BucketKeyEnabled\u0026#34;: true } ] } } Testing Test DynamoDB Encryption # Create test article curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Test Encryption\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Sensitive data\u0026#34;, \u0026#34;latitude\u0026#34;: 10.8231, \u0026#34;longitude\u0026#34;: 106.6297 }\u0026#39; # Verify data is encrypted in DynamoDB # (You cannot see encrypted data directly - it\u0026#39;s transparent) # Read article back curl https://api.example.com/articles/{article-id} \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; # Data should be readable (decrypted automatically) Test S3 Encryption # Upload test image aws s3 cp test-image.jpg \\ s3://travel-guide-images-staging-123456789012/test/ # Check object encryption aws s3api head-object \\ --bucket travel-guide-images-staging-123456789012 \\ --key test/test-image.jpg \\ --query \u0026#39;ServerSideEncryption\u0026#39; # Expected: \u0026#34;AES256\u0026#34; Compliance Regulations Supported ‚úÖ GDPR (EU)\nArticle 32: Security of processing Encryption at rest required ‚úÖ HIPAA (Healthcare)\nTechnical safeguards Encryption required for PHI ‚úÖ PCI-DSS (Payment)\nRequirement 3.4: Encryption of cardholder data Encryption at rest mandatory ‚úÖ SOC 2\nSecurity principle Encryption controls Monitoring CloudWatch Metrics DynamoDB:\nMonitor KMS API calls Track encryption errors Alert on decryption failures S3:\nMonitor encryption status Track unencrypted uploads (should be 0) CloudTrail Logging Enable CloudTrail to audit:\nKMS key usage S3 encryption changes Access to encrypted data # Check CloudTrail logs aws cloudtrail lookup-events \\ --lookup-attributes AttributeKey=ResourceType,AttributeValue=AWS::KMS::Key \\ --max-results 10 Best Practices 1. Use AWS-Managed Keys ‚úÖ Recommended: AWS-managed KMS keys\nAutomatic rotation No key management Lower cost ‚ùå Avoid: Customer-managed keys (unless required)\nManual rotation Key management burden Higher cost 2. Enable Encryption by Default ‚úÖ Do: Enable at bucket/table level\nApplies to all data Cannot be bypassed ‚ùå Don\u0026rsquo;t: Rely on object-level encryption\nEasy to forget Inconsistent protection 3. Monitor Encryption Status ‚úÖ Do: Set up CloudWatch alarms\nAlert on encryption disabled Track unencrypted objects 4. Document Encryption Keys ‚úÖ Do: Document which keys encrypt what\nKMS key IDs Key policies Rotation schedules Troubleshooting Issue 1: KMS Access Denied Error:\nAccessDeniedException: User is not authorized to perform: kms:Decrypt Solution:\nAdd KMS permissions to Lambda execution role Grant kms:Decrypt and kms:DescribeKey { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kms:*:*:key/*\u0026#34; } Issue 2: S3 Encryption Not Applied Problem: Old objects not encrypted\nSolution:\nEncryption only applies to new objects Re-upload existing objects: # Copy object to itself (re-encrypts) aws s3 cp \\ s3://bucket/key \\ s3://bucket/key \\ --metadata-directive REPLACE Issue 3: Performance Impact Problem: Slight latency increase\nSolution:\nNormal with encryption (\u0026lt;1ms) Use S3 Bucket Keys to reduce KMS calls Already enabled: BucketKeyEnabled: true Cost Optimization DynamoDB KMS Reduce costs:\nUse AWS-managed keys (cheaper) Enable S3 Bucket Keys (reduces KMS API calls) Monitor KMS usage with Cost Explorer S3 AES-256 No cost:\nSSE-S3 (AES-256) is FREE No additional charges No performance impact Key Takeaways Encryption at rest protects data on disk DynamoDB uses KMS - automatic encryption/decryption S3 uses AES-256 - free and transparent No code changes required - fully managed Compliance ready - meets GDPR, HIPAA, PCI-DSS Minimal cost - $5-10/month for DynamoDB KMS "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.6-cloudfront-s3-location/5.6.1-s3-presigned-upload/","title":"S3 Pre-signed URL Upload","tags":[],"description":"","content":"Amazon S3 Upload Web (Pre-signed URL) Why Use Pre-signed URLs? Instead of uploading images through the backend (consuming bandwidth/risking timeout), we use a two-step approach:\nFrontend requests a temporary upload URL from backend Frontend uploads directly to S3 Benefits:\nFast, reduces backend load Time-limited URL (security) No AWS credentials needed on client Direct upload to S3 S3 Bucket Configuration ArticleImagesBucket Setup Key configurations:\nVersioning: Enabled CORS: Allows uploads from frontend Access: Private (accessed via CloudFront OAI) Event notifications: Triggers SQS for image processing CORS Configuration [ { \u0026#34;AllowedHeaders\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;AllowedMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;HEAD\u0026#34;], \u0026#34;AllowedOrigins\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;ExposeHeaders\u0026#34;: [\u0026#34;ETag\u0026#34;] } ] Note: In production, replace \u0026quot;*\u0026quot; with specific domains.\nAPI: Generate Pre-signed URL Lambda Function Function: GetUploadUrlFunction\nAPI Path: POST /upload-url\nAuthentication: Cognito JWT required\nRequest Flow Frontend calls POST {API_BASE}/upload-url with JWT token Backend generates pre-signed URL Backend returns: uploadUrl: Temporary S3 upload URL key: Object path in S3 expiresIn: 900 seconds (15 minutes) Response Example { \u0026#34;uploadUrl\u0026#34;: \u0026#34;https://s3.ap-southeast-1.amazonaws.com/...\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;articles/\u0026lt;articleId\u0026gt;/raw/\u0026lt;uuid\u0026gt;.jpg\u0026#34;, \u0026#34;articleId\u0026#34;: \u0026#34;\u0026lt;uuid\u0026gt;\u0026#34;, \u0026#34;expiresIn\u0026#34;: 900 } Lambda Code Structure import boto3 import uuid from datetime import datetime s3_client = boto3.client(\u0026#39;s3\u0026#39;) BUCKET_NAME = os.environ[\u0026#39;ARTICLE_IMAGES_BUCKET\u0026#39;] def lambda_handler(event, context): # Get user info from Cognito user_id = event[\u0026#39;requestContext\u0026#39;][\u0026#39;authorizer\u0026#39;][\u0026#39;claims\u0026#39;][\u0026#39;sub\u0026#39;] # Generate unique key article_id = str(uuid.uuid4()) file_key = f\u0026#34;articles/{article_id}/raw/{uuid.uuid4()}.jpg\u0026#34; # Generate pre-signed URL upload_url = s3_client.generate_presigned_url( \u0026#39;put_object\u0026#39;, Params={ \u0026#39;Bucket\u0026#39;: BUCKET_NAME, \u0026#39;Key\u0026#39;: file_key, \u0026#39;ContentType\u0026#39;: \u0026#39;image/jpeg\u0026#39; }, ExpiresIn=900 # 15 minutes ) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;uploadUrl\u0026#39;: upload_url, \u0026#39;key\u0026#39;: file_key, \u0026#39;articleId\u0026#39;: article_id, \u0026#39;expiresIn\u0026#39;: 900 }) } Frontend Upload Implementation React Example // Step 1: Get pre-signed URL const getUploadUrl = async () =\u0026gt; { const response = await fetch(`${API_BASE}/upload-url`, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Authorization\u0026#39;: `Bearer ${accessToken}`, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } }); return await response.json(); }; // Step 2: Upload file directly to S3 const uploadImage = async (file) =\u0026gt; { // Get upload URL const { uploadUrl, key, articleId } = await getUploadUrl(); // Upload to S3 const uploadResponse = await fetch(uploadUrl, { method: \u0026#39;PUT\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: file.type }, body: file }); if (uploadResponse.ok) { console.log(\u0026#39;Upload successful!\u0026#39;); return { key, articleId }; } else { throw new Error(\u0026#39;Upload failed\u0026#39;); } }; // Usage const handleFileSelect = async (event) =\u0026gt; { const file = event.target.files[0]; try { const result = await uploadImage(file); console.log(\u0026#39;Image uploaded:\u0026#39;, result); } catch (error) { console.error(\u0026#39;Upload error:\u0026#39;, error); } }; IAM Permissions Lambda Execution Role The Lambda function needs s3:PutObject permission:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::article-images-bucket/*\u0026#34; } ] } Note: Frontend doesn\u0026rsquo;t need AWS credentials because it uses pre-signed URLs.\nOperations \u0026amp; Troubleshooting Common Issues 1. 403 AccessDenied on PUT Causes:\nURL expired (\u0026gt; 15 minutes) Content-Type mismatch (if enforced in pre-signed URL) Bucket policy blocking access Solution:\nRequest new upload URL Ensure Content-Type matches Check bucket policy 2. CORS Error Causes:\nCORS not configured on bucket Wrong method/headers in request Solution:\nVerify CORS configuration on ArticleImagesBucket Ensure frontend sends correct headers Check browser console for specific CORS error 3. Object Uploaded but Can\u0026rsquo;t Load Cause:\nBucket is private Solution:\nLoad images via CloudFront (OAI) Or generate pre-signed download URLs Monitoring CloudWatch Metrics to track:\nNumber of pre-signed URL requests Upload success/failure rate Average upload time S3 PUT request count Security Best Practices Time-limited URLs: Keep expiration short (15 minutes) Content-Type validation: Enforce image types only File size limits: Set max file size in API User authentication: Always require JWT token Bucket encryption: Enable S3 encryption at rest Logging: Enable S3 access logs for audit Cost Optimization Strategies:\nUse S3 Intelligent-Tiering for infrequently accessed images Enable S3 Transfer Acceleration for global users (optional) Set lifecycle policies to delete incomplete multipart uploads Monitor S3 storage costs with Cost Explorer Key Takeaways Pre-signed URLs enable secure, direct uploads without credentials CORS configuration is essential for browser uploads Time-limited URLs enhance security CloudFront should be used for serving images (not direct S3) IAM least privilege - Lambda only needs PutObject permission "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.3-backend-articles/5.3.1-lambda-service-backend/","title":" Backend ‚Äì Article Service ","tags":[],"description":"","content":"Template Code Structure Lambda Functions Overview API Gateway Integration Summary of Lambda Functions This section provides a concise overview of all Lambda functions used in the Article, Profile, and Favorites modules. These functions collectively form the backend workflow of the content system.\n1. Article Functions (CRUD + Search + Upload) CreateArticleFunction\nEndpoint: POST /articles (Auth required) Role: Creates a new article, processes image metadata, resolves geolocation using AWS Location, saves data into DynamoDB and S3. GetArticleFunction\nEndpoint: GET /articles/{articleId} (Public) Role: Retrieves a single article and images while applying visibility rules. ListArticlesFunction\nEndpoint: GET /articles (Auth required) Role: Lists public articles enriched with profile data and image metadata. UpdateArticleFunction\nEndpoint: PATCH /articles/{articleId} Role: Verifies ownership and updates article content, visibility, location, and images. DeleteArticleFunction\nEndpoint: DELETE /articles/{articleId} Role: Permanently deletes the article and related images from S3. SearchArticlesFunction\nEndpoint: GET /search (Public) Role: Performs keyword/tag search using DynamoDB queries. GetUploadUrlFunction\nEndpoint: POST /upload-url (Auth required) Role: Generates a presigned URL for uploading images directly to S3. 2. User Profile Function GetUserArticlesFunction Endpoint: GET /users/{userId}/articles Role: Retrieves all public articles created by a specific user for their profile page. 3. Favorites Functions FavoriteArticleFunction\nEndpoint: POST /articles/{articleId}/favorite Role: Adds (userId, articleId) mapping into UserFavoritesTable. UnfavoriteArticleFunction\nEndpoint: DELETE /articles/{articleId}/favorite Role: Removes a favorite record. ListFavoriteArticlesFunction\nEndpoint: GET /me/favorites Role: Returns all articles favorited by the authenticated user. Overall Summary These Lambda functions integrate DynamoDB, S3, Cognito, and AWS Location to create a modular, scalable backend powering the article management system.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.5-auth-cognito-iam/5.5.1-cognito-setup/","title":"AWS Cognito Setup","tags":[],"description":"","content":"AWS Cognito - User Authentication Service Introduction to AWS Cognito Amazon Cognito is an Identity Management service built to help your application handle user registration, login, and authentication without designing a complex authentication system from scratch. Cognito supports horizontal scaling, high availability, and security according to AWS Security standards.\nThrough Cognito User Pool, applications can authenticate users using email, phone number, password, or social login providers (Google, Facebook, Apple). Additionally, Cognito issues JWT tokens so Frontend and Backend applications can control access without storing session state on the server.\nUser Pool is responsible for:\nManaging user identities OTP authentication Storing user profiles Generating tokens App Client and Hosted UI provide integrated login interface, serving Web and Mobile applications well without building login screens from scratch.\nWorkshop Overview In this workshop, you will deploy a complete authentication system based on Cognito, simulating all components commonly found in enterprise environments.\nWe will use two application environment groups:\n\u0026ldquo;Frontend App\u0026rdquo; This is a React application simulating the user interface (Client Application). This application will integrate Cognito\u0026rsquo;s Hosted UI and manage id_token, access_token, and refresh_token.\nFrontend will be configured to:\nConnect to Cognito Domain Receive tokens after user login Call protected APIs using Access Token \u0026ldquo;Backend API\u0026rdquo; This simulates the enterprise Service Layer. Backend includes API Gateway, Cognito Authorizer, and Lambda functions for processing logic.\nAPIs in this environment will:\nRequire valid access_token for access Validate tokens using Cognito before forwarding to Lambda Read user information from JWT Simulate authentication \u0026amp; authorization behavior of production backend systems Step 1: Create User Pool Navigate to AWS Console ‚Üí Cognito ‚Üí User Pools ‚Üí Create user pool\nConfiguration:\nPool name: TravelGuideUserPool-staging Sign-in options: Email Attributes: email, name Step 2: Password Policy Configure password requirements:\nAt least 1 number At least 1 lowercase letter At least 1 uppercase letter Why these requirements?\nEnsures strong passwords Meets security compliance standards Protects against brute force attacks Step 3: Create App Client Navigate to: App integration ‚Üí App clients ‚Üí Create app client\nSteps:\nEnter app client name Select Authentication flows: ALLOW_USER_PASSWORD_AUTH ALLOW_REFRESH_TOKEN_AUTH ALLOW_USER_SRP_AUTH Create app client Authentication Flows Explained:\nUSER_PASSWORD_AUTH: Direct username/password authentication REFRESH_TOKEN_AUTH: Token refresh capability USER_SRP_AUTH: Secure Remote Password protocol (recommended) Step 4: Create Test User Navigate to: User Pool ‚Üí Users ‚Üí Create user\nUser Information:\nUsername Email address Phone number (optional) Temporary password Note: User will need to change password on first login.\nStep 5: Sign Up Flow Cognito automatically handles OTP verification - no backend code needed!\nSign Up Process:\nUser enters email and password Cognito sends verification code to email User enters code to verify Account is activated Benefits:\n‚úÖ No custom email service needed ‚úÖ Built-in rate limiting ‚úÖ Automatic retry logic ‚úÖ Secure code generation Token Management JWT Tokens Explained Cognito issues three types of tokens:\n1. ID Token Contains user identity information Used by frontend to display user data Short-lived (1 hour default) { \u0026#34;sub\u0026#34;: \u0026#34;user-uuid\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;cognito:username\u0026#34;: \u0026#34;johndoe\u0026#34; } 2. Access Token Used to authorize API requests Sent in Authorization header Validated by API Gateway Authorization: Bearer \u0026lt;access_token\u0026gt; 3. Refresh Token Used to obtain new ID and Access tokens Long-lived (30 days default) Stored securely on client Token Flow 1. User Login ‚Üì 2. Cognito validates credentials ‚Üì 3. Cognito issues tokens ‚Üì 4. Frontend stores tokens ‚Üì 5. Frontend calls API with Access Token ‚Üì 6. API Gateway validates token ‚Üì 7. Lambda processes request Frontend Integration React Example import { CognitoUserPool, CognitoUser, AuthenticationDetails } from \u0026#39;amazon-cognito-identity-js\u0026#39;; const poolData = { UserPoolId: \u0026#39;us-east-1_XXXXXXXXX\u0026#39;, ClientId: \u0026#39;your-app-client-id\u0026#39; }; const userPool = new CognitoUserPool(poolData); // Sign In function signIn(username, password) { const authenticationData = { Username: username, Password: password, }; const authenticationDetails = new AuthenticationDetails(authenticationData); const userData = { Username: username, Pool: userPool }; const cognitoUser = new CognitoUser(userData); cognitoUser.authenticateUser(authenticationDetails, { onSuccess: (result) =\u0026gt; { const accessToken = result.getAccessToken().getJwtToken(); const idToken = result.getIdToken().getJwtToken(); const refreshToken = result.getRefreshToken().getToken(); // Store tokens localStorage.setItem(\u0026#39;accessToken\u0026#39;, accessToken); localStorage.setItem(\u0026#39;idToken\u0026#39;, idToken); localStorage.setItem(\u0026#39;refreshToken\u0026#39;, refreshToken); }, onFailure: (err) =\u0026gt; { console.error(err); } }); } Backend Integration API Gateway Cognito Authorizer Configuration:\nCreate Cognito User Pool Authorizer Specify User Pool ID Token source: Authorization header Attach to API routes Lambda receives user info:\ndef lambda_handler(event, context): # User info from Cognito claims = event[\u0026#39;requestContext\u0026#39;][\u0026#39;authorizer\u0026#39;][\u0026#39;claims\u0026#39;] user_id = claims[\u0026#39;sub\u0026#39;] email = claims[\u0026#39;email\u0026#39;] username = claims[\u0026#39;cognito:username\u0026#39;] return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: f\u0026#39;Hello {username}!\u0026#39;, \u0026#39;userId\u0026#39;: user_id }) } Key Takeaways Cognito User Pool manages user identities JWT Tokens provide stateless authentication Hosted UI simplifies frontend integration Cognito Authorizer secures API Gateway No session management needed on backend Scalable and highly available by default Best Practices Use SRP authentication for enhanced security Enable MFA for sensitive operations Rotate refresh tokens regularly Store tokens securely (not in localStorage for production) Implement token refresh logic Use HTTPS for all communications Set appropriate token expiration times "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.2-iac-multistack/5.2.1-iac-strategy/","title":"IaC Strategy &amp; Tool Selection","tags":[],"description":"","content":"Infrastructure as Code Strategy Why Infrastructure as Code? Infrastructure as Code (IaC) allows us to:\nVersion Control: Track infrastructure changes in Git Reproducibility: Deploy identical environments consistently Automation: Reduce manual errors and deployment time Documentation: Code serves as living documentation Collaboration: Team members can review and contribute Why CloudFormation/SAM? For the Travel Guide Application, we chose AWS CloudFormation with SAM (Serverless Application Model) for the following reasons:\nAdvantages 1. Native AWS Integration\nFirst-class support for all AWS services No additional state management required Automatic rollback on failures Built-in drift detection 2. SAM Simplification\nSimplified syntax for Lambda, API Gateway, DynamoDB Local testing capabilities (sam local) Built-in best practices for serverless Automatic IAM role generation 3. No Additional Cost\nCloudFormation is free (pay only for resources) No need for external state storage No licensing fees 4. AWS Ecosystem\nIntegrates with CodePipeline, CodeBuild CloudWatch integration for monitoring AWS Console visualization Trade-offs vs Alternatives Feature CloudFormation/SAM Terraform AWS CDK Pulumi Learning Curve Medium Medium High High Syntax YAML/JSON HCL TypeScript/Python TypeScript/Python/Go State Management AWS-managed Manual/Cloud AWS-managed Cloud Multi-Cloud ‚ùå No ‚úÖ Yes ‚ùå No ‚úÖ Yes Cost Free Free (Cloud paid) Free Free (Cloud paid) AWS Integration Native Good Native Good Serverless Support Excellent (SAM) Good Excellent Good Template Organization Strategy Our templates are organized as follows:\ninfrastructure/ ‚îú‚îÄ‚îÄ core/ ‚îÇ ‚îî‚îÄ‚îÄ template.yaml # Core stack (DynamoDB, S3, Cognito) ‚îú‚îÄ‚îÄ services/ ‚îÇ ‚îú‚îÄ‚îÄ auth/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ template.yaml # Auth service Lambda functions ‚îÇ ‚îú‚îÄ‚îÄ articles/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ template.yaml # Article service Lambda functions ‚îÇ ‚îú‚îÄ‚îÄ media/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ template.yaml # Media processing Lambda functions ‚îÇ ‚îî‚îÄ‚îÄ ... ‚îú‚îÄ‚îÄ parameters/ ‚îÇ ‚îú‚îÄ‚îÄ staging.json # Staging environment parameters ‚îÇ ‚îî‚îÄ‚îÄ prod.json # Production environment parameters ‚îî‚îÄ‚îÄ scripts/ ‚îú‚îÄ‚îÄ deploy.sh # Main deployment orchestration ‚îú‚îÄ‚îÄ deploy-core.sh # Core stack deployment ‚îî‚îÄ‚îÄ deploy-service.sh # Service stack deployment Basic CloudFormation Template Structure AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: AWS::Serverless-2016-10-31 Description: Travel Guide - Core Infrastructure Parameters: Environment: Type: String Default: staging AllowedValues: [staging, prod] Resources: # DynamoDB Tables ArticlesTable: Type: AWS::DynamoDB::Table Properties: TableName: !Sub \u0026#39;${AWS::StackName}-articles\u0026#39; BillingMode: PAY_PER_REQUEST AttributeDefinitions: - AttributeName: articleId AttributeType: S KeySchema: - AttributeName: articleId KeyType: HASH Outputs: ArticlesTableName: Description: Articles DynamoDB Table Name Value: !Ref ArticlesTable Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableName\u0026#39; SAM Transform Example SAM simplifies Lambda and API Gateway definitions:\n# Without SAM (CloudFormation only) CreateArticleFunction: Type: AWS::Lambda::Function Properties: FunctionName: create-article Runtime: python3.11 Handler: index.handler Code: S3Bucket: my-bucket S3Key: function.zip Role: !GetAtt LambdaRole.Arn # With SAM CreateArticleFunction: Type: AWS::Serverless::Function Properties: CodeUri: ./src Handler: create_article.handler Runtime: python3.11 Events: CreateArticle: Type: Api Properties: Path: /articles Method: post Key Takeaways CloudFormation/SAM is ideal for AWS-only deployments SAM significantly reduces boilerplate for serverless applications Native integration eliminates state management complexity Template organization is crucial for maintainability Trade-offs exist - multi-cloud requires different tools When to Consider Alternatives Terraform: If you need multi-cloud support or team is already familiar AWS CDK: If you prefer programming languages over YAML Pulumi: If you want full programming language power with multi-cloud "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-image-processing/5.4.1-content-moderation/","title":"Lambda Content Moderation","tags":[],"description":"","content":"Purpose Automatically moderate image content to detect violations such as:\nExplicit/suggestive content Violence Drugs/alcohol Offensive symbols Main Code Explanation a. Receive message from SQS Extract bucket and key information from S3 event:\nfor sqs_record in event.get(\u0026#39;Records\u0026#39;, []): try: # Parse S3 event from SQS body s3_event = json.loads(sqs_record[\u0026#39;body\u0026#39;]) # Process each S3 record for s3_record in s3_event.get(\u0026#39;Records\u0026#39;, []): try: bucket = s3_record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = s3_record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] b. Call Rekognition detect_moderation_labels AWS Rekognition analyzes the image and returns violation labels:\nmoderation_result = moderate_image(bucket, key) if \u0026#39;error\u0026#39; in moderation_result: print(f\u0026#34;Moderation failed: {moderation_result[\u0026#39;error\u0026#39;]}\u0026#34;) results[\u0026#39;errors\u0026#39;] += 1 continue results[\u0026#39;processed\u0026#39;] += 1 if moderation_result[\u0026#39;passed\u0026#39;]: results[\u0026#39;approved\u0026#39;] += 1 mark_article_as_approved(article_id) # Only forward to next queue if approved final_status = { \u0026#39;moderationStatus\u0026#39;: \u0026#39;approved\u0026#39;, \u0026#39;processed\u0026#39;: True } forward_to_next_queue(bucket, key, article_id, final_status) else: results[\u0026#39;rejected\u0026#39;] += 1 action_result, owner_id = handle_moderation_failure( bucket, key, article_id, moderation_result ) if action_result in results[\u0026#39;actions\u0026#39;]: results[\u0026#39;actions\u0026#39;][action_result] += 1 c. Handle violations - Publish SNS to trigger SES email If content violates policies, send admin notification:\n# Always send admin notification for deleted/quarantined content if action_result in [\u0026#39;deleted\u0026#39;, \u0026#39;quarantined\u0026#39;]: print(f\u0026#34;üìß Sending admin notification for {action_result} content\u0026#34;) send_admin_notification(article_id, key, moderation_result, owner_id) elif moderation_result[\u0026#39;maxSeverity\u0026#39;] in [\u0026#39;critical\u0026#39;, \u0026#39;high\u0026#39;]: print(f\u0026#34;üìß Sending admin notification for {moderation_result[\u0026#39;maxSeverity\u0026#39;]} severity\u0026#34;) send_admin_notification(article_id, key, moderation_result, owner_id) # DO NOT forward to next queue if rejected/deleted # Pipeline stops here, user already received deletion email print(f\u0026#34;‚ö†Ô∏è Pipeline stopped for rejected image: {key}\u0026#34;) print(f\u0026#34; User notification already sent via send_user_deletion_email()\u0026#34;) Demo: Email Notification for Violent Content When violent content is detected, the system automatically sends email notifications:\nProcessing Flow Image uploaded to S3 S3 event ‚Üí SQS Queue Lambda reads from SQS Rekognition analyzes content If passed ‚Üí Forward to Detect Labels queue If failed ‚Üí Send notification + Stop pipeline Result Approved images continue to label detection Violated images are quarantined/deleted Admin and user receive email notifications "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"AI/ML/GenAI on AWS Workshop Date: Saturday, November 15, 2025\nTime: 8:30 AM - 12:00 PM\nLocation: AWS Office\nSpeaker Lineup Lam Tuan Kiet Sr DevOps Engineer - FPT Software\nDevOps expert with experience in deploying and operating ML/AI systems on AWS.\nDanh Hoang Hieu Nghi AI Engineer - Renova Cloud\nAI Engineer specializing in machine learning and cloud-based AI solutions.\nDinh Le Hoang Anh Cloud Engineer Trainee - First Cloud AI Journey\nCloud Engineer in training, sharing learning experiences and hands-on practice with AWS AI/ML services.\nEvent Purpose The \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop was organized with the following objectives:\nProvide comprehensive overview of AI/ML services on AWS Introduce and guide hands-on practice with Amazon SageMaker - end-to-end ML platform Explore Generative AI through Amazon Bedrock Practice building chatbots using GenAI Learn about Prompt Engineering, RAG (Retrieval-Augmented Generation), and advanced techniques Connect and network with the AI/ML community in Vietnam Highlights AWS AI/ML Services Overview Amazon SageMaker - End-to-end ML platform:\nData preparation and labeling Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough Generative AI with Amazon Bedrock Foundation Models:\nComparison and selection guide: Claude, Llama, Titan Prompt Engineering:\nBasic and advanced techniques Chain-of-Thought reasoning Few-shot learning Retrieval-Augmented Generation (RAG):\nRAG architecture Knowledge Base integration Bedrock Agents:\nMulti-step workflows Tool integrations Guardrails:\nSafety and content filtering Live Demo: Building a Generative AI chatbot using Bedrock\nKnowledge Gained Through this event, I learned:\n1. Amazon SageMaker Understanding end-to-end ML workflow on AWS Effective data preparation and labeling techniques Model training and tuning strategies Deployment strategies and best practices MLOps workflow and automation 2. Generative AI with Amazon Bedrock Differences between Foundation Models (Claude, Llama, Titan) How to select appropriate models for specific use cases Prompt Engineering techniques: Writing effective prompts Chain-of-Thought reasoning for improved results Few-shot learning for specific tasks 3. RAG (Retrieval-Augmented Generation) RAG architecture and operating principles Integrating Knowledge Base with LLM Real-world enterprise use cases for RAG 4. Bedrock Agents and Guardrails Building multi-step workflows Integrating tools and APIs Setting up safety measures and content filtering 5. Hands-on Experience Practical chatbot building with Bedrock Working with SageMaker Studio Applying learned techniques to real exercises Personal Experience Initial Impressions The workshop was professionally organized at the AWS Office with complete facilities and an ideal learning environment. The ice-breaker session helped me quickly get acquainted with other participants and created a comfortable atmosphere for the day\u0026rsquo;s learning.\nTheoretical Sessions All speakers had practical experience and presented content in an easy-to-understand manner. I was particularly impressed with the Amazon SageMaker introduction - a comprehensive platform for ML workflow. The comparison of Foundation Models helped me better understand the strengths and weaknesses of each model.\nLive Demos The hands-on demo sessions were the workshop highlights:\nSageMaker Studio Walkthrough: Witnessed the complete process from data preparation to model deployment Bedrock Chatbot: Built a complete chatbot with RAG, truly impressive regarding GenAI capabilities Prompt Engineering Techniques This section opened my mind about effective interaction with LLMs. Techniques like Chain-of-Thought and Few-shot learning significantly improved output quality.\nNetworking Coffee breaks and rest periods were excellent opportunities to:\nExchange ideas with speakers Connect with people interested in AI/ML Share experiences and learn from the community Challenges Substantial amount of knowledge covered in one day Some concepts about RAG and Bedrock Agents were quite complex, requiring additional practice time Desired more time for hands-on labs Highlights Workshop content was very practical and applicable Enthusiastic speakers, ready to answer questions Complete documentation and resources provided Professional learning environment Next Steps After the workshop, I plan to:\nPractice the demos learned Build a small project using Bedrock Dive deeper into MLOps with SageMaker Attend upcoming AWS workshops Conclusion The \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop was an excellent learning experience, providing foundational knowledge and hands-on practice with advanced AI/ML technologies on AWS. The event not only helped me enhance technical skills but also expanded my network within the AI/ML community in Vietnam. This was an important stepping stone in my journey to explore and apply AI/ML to real-world projects.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Le Tran Anh Khiem\nPhone Number: 0839286855\nEmail: khiem120805@gmail.com\nUniversity: FPT University\nMajor: Information Security\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/08/2025 to 12/09/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Master the regulations and guidelines of the First Cloud Journey internship program. Learn about AWS, basic services, and how AWS optimizes systems. Learn how to draw infrastructure architecture on platforms like draw.io. Practice creating and configuring AWS Free Tier account. Install and configure AWS CLI with necessary IAM accounts. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Read through the internship regulations and guidelines 09/08/2025 09/08/2025 https://policies.fcjuni.com/ 3 - Learn more about AWS - Watch FCJ Bootcamp videos 09/09/2025 09/09/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 4 - Explore AWS service types: + Compute + Database + Storage + Networking 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn how to draw architecture - Learn about AWS system optimization: + Operations + Security + Cost Optimization 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Create AWS account + Create MFA for AWS account + Practice with IAM 09/12/2025 09/12/2025 https://000001.awsstudygroup.com/ Week 1 Achievements: Clear understanding of internship regulations: Mastered the rules, guidelines, and requirements of the First Cloud Journey program.\nFoundational AWS knowledge:\nUnderstood what AWS is and its importance in the technology industry. Grasped the main AWS service groups: Compute Services (EC2, Lambda, \u0026hellip;) Storage Services (S3, EBS, \u0026hellip;) Networking Services (VPC, CloudFront, \u0026hellip;) Database Services (RDS, DynamoDB, \u0026hellip;) Understood AWS system optimization principles: Operations, Security, Cost Optimization. Architecture drawing skills: Learned how to draw AWS architecture using tools like draw.io.\nOperational AWS account: Successfully created and configured AWS Free Tier account with:\nMFA (Multi-Factor Authentication) security IAM configuration for access management Ready work environment: Installed and configured AWS CLI with:\nAccess Key Secret Key Default Region "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Get familiar with AWS Management Console by exploring and configuring services on the AWS platform. Master the basic concepts of Amazon Virtual Private Cloud (VPC) including Subnets, Route Tables, Internet Gateways, and NAT Gateways. Learn about VPC security mechanisms such as Security Groups and Network Access Control Lists (NACLs). Practice building a complete VPC environment from scratch, including creating VPC, Subnets, Internet Gateways, Route Tables, Security Groups, and enabling VPC Flow Logs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: + Explore and configure AWS Management Console 09/15/2025 09/15/2025 https://000001.awsstudygroup.com/ 3 - Learn about Amazon Virtual Private Cloud (VPC): + Subnets + Route Table + Internet Gateway + NAT Gateway 09/16/2025 09/16/2025 https://000003.awsstudygroup.com/ 4 - Learn about Firewalls in VPC: + Security Group + Network ACLs + VPC Resource Map 09/17/2025 09/17/2025 https://000003.awsstudygroup.com/ 5 - Practice: + Create VPC + Create Subnet + Create Internet Gateway 09/18/2025 09/18/2025 https://000003.awsstudygroup.com/ 6 - Practice: + Create Route Table + Create Security Group + Enable VPC Flow Logs 09/19/2025 09/19/2025 https://000003.awsstudygroup.com/ Week 2 Achievements: Proficient with AWS Management Console:\nLearned how to search, access, and use AWS services through the web interface Became familiar with the layout and main features of AWS Management Console Understood how to navigate between different services Mastered core Amazon VPC concepts:\nSubnets: Understood how to divide VPC into public and private subnets Route Tables: Learned how to route traffic between subnets and outside VPC Internet Gateway: Understood the role of Internet Gateway in connecting VPC to the internet NAT Gateway: Learned how to allow instances in private subnets to connect outbound without receiving inbound connections Deep understanding of VPC security:\nSecurity Groups: Learned to use Security Groups as instance-level firewalls, controlling inbound and outbound traffic Network Access Control Lists (NACLs): Understood how to use NACLs as subnet-level firewalls, providing more granular traffic control Successfully deployed complete VPC environment:\nCreated VPC with appropriate CIDR block Created and configured public and private Subnets Created and attached Internet Gateway to VPC Created Route Tables and configured routes Created and configured Security Groups with appropriate security rules Enabled VPC Flow Logs to monitor and log traffic in VPC Gained network architecture knowledge:\nBuilt secure network architecture Established scalable infrastructure Understood best practices in VPC management on AWS "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn about methods to connect VPCs and on-premises networks:\nVPC Peering - direct connection between two VPCs Transit Gateway - centralized connection for multiple VPCs Direct Connect Gateway - dedicated connection from on-premises VPN Site-to-Site - VPN connection from on-premises network VPN Client-to-Site - VPN connection for individual clients Master the types of Load Balancers in AWS:\nApplication Load Balancer (ALB) Network Load Balancer (NLB) Classic Load Balancer (CLB) Gateway Load Balancer (GWLB) Practice creating and configuring EC2 instances with advanced features.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about VPC Peering - Learn about Transit Gateway 09/15/2025 09/15/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 3 - Learn about Direct Connect Gateway - Learn about VPN Site to Site: + Virtual Private Gateway + Customer Gateway - Learn about VPN Client to Site 09/23/2025 09/23/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 4 - Learn about Elastic Load Balancing: + Application Load Balancer + Network Load Balancer + Classic Load Balancer + Gateway Load Balancer 09/24/2025 09/24/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 5 - Practice: + Create and configure EC2 instance + Test connectivity between instances 09/25/2025 09/25/2025 https://000003.awsstudygroup.com/ 6 - Practice: + Create Multi-AZ NAT Gateway + Create EC2 Instance Connect Endpoint 09/26/2025 09/26/2025 https://000003.awsstudygroup.com/ Week 3 Achievements: Clear understanding of advanced VPC connectivity methods:\nVPC Peering: Learned how to directly connect two VPCs to allow private network traffic Transit Gateway: Understood how to use Transit Gateway as a central hub for multiple VPCs and on-premises networks Direct Connect Gateway: Mastered how to establish dedicated connections from on-premises networks to AWS Mastered VPN methods on AWS:\nVPN Site-to-Site: Understood how to connect entire on-premises network with VPC Virtual Private Gateway (VGW) Customer Gateway (CGW) VPN Client-to-Site: Learned how to allow individual clients to securely connect to VPC Proficient in Elastic Load Balancing:\nApplication Load Balancer (ALB): Understood how to use for web applications, with URL/hostname-based routing Network Load Balancer (NLB): Learned how to use for ultra-high performance and low latency traffic Classic Load Balancer (CLB): Understood basic load balancer type for legacy applications Gateway Load Balancer (GWLB): Mastered how to use for virtual appliances Successfully practiced with EC2:\nCreated and configured EC2 instances with appropriate settings Tested network connectivity between EC2 instances Deployed Multi-AZ architecture to ensure high availability Created NAT Gateway to allow instances to connect outbound Used EC2 Instance Connect Endpoint for secure access Comprehensive knowledge:\nUnderstood how to design complex connectivity infrastructure on AWS Mastered how to choose appropriate connectivity methods for each use case Learned how to load balance and optimize application performance "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn about Route 53 - DNS service and domain management on AWS Master AWS CloudFormation - Infrastructure as Code (IaC) tool for automating resource deployment Learn about AWS Directory Service - centralized directory management service Learn about AWS Quick Starts - rapid deployment templates Practice setting up Hybrid DNS with Route 53 Resolver Practice setting up VPC Peering with Cross-Peer DNS enabled Practice deploying AWS Transit Gateway (TGW) with attachments and route tables Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Route 53 - Learn about AWS Quick Starts - Learn about AWS CloudFormation - Learn about AWS Directory Service 09/29/2025 09/29/2025 https://000010.awsstudygroup.com/vi/ https://000037.awsstudygroup.com/vi/ 3 - Practice: Setting up Hybrid DNS with Route 53 Resolver + Create keypair + Initialize CloudFormation Template + Configure Security Group + Connect to Remote Desktop Gateway (RDGW) server + Deploy Microsoft AD + Setup DNS + Clean up resources 09/30/2025 09/30/2025 https://000010.awsstudygroup.com/vi/ 4 - Practice: Setting up VPC Peering + Initialize CloudFormation Template + Create Security Group and EC2 Instance + Update Network ACL + Create Peering connection + Enable Cross-Peer DNS + Clean up resources 10/01/2025 10/01/2025 https://000019.awsstudygroup.com/vi/ 5 - Learn more about Transit Gateway (TGW): + TGW Attachment + TGW Policy Tables + TGW Route Tables + TGW Multicast 10/02/2025 10/02/2025 https://000020.awsstudygroup.com/vi/ 6 - Practice: AWS Transit Gateway Overview + Initialize CloudFormation Template + Create Transit Gateway + Create Transit Gateway Attachment + Create Transit Gateway Route Table + Add Transit Gateway Routes to VPC Route Tables + Clean up resources 10/03/2025 10/03/2025 https://000020.awsstudygroup.com/vi/ Week 4 Achievements: Mastered DNS and management services on AWS:\nRoute 53: Understood how to use fully managed DNS service by AWS Route 53 Resolver: Learned how to set up Hybrid DNS to connect on-premises and AWS DNS Understood how to manage domains and DNS records Deep understanding of Infrastructure as Code (IaC):\nAWS CloudFormation: Mastered how to use templates to automate and manage AWS resources Learned how to write CloudFormation templates to deploy complex architectures Understood main components: Resources, Parameters, Outputs, Conditions Learned about resource management:\nAWS Directory Service: Understood how to use centralized directory management service AWS Quick Starts: Learned how to leverage available rapid deployment templates Successfully practiced Hybrid DNS Setup:\nCreated keypair for Remote Desktop Gateway (RDGW) Deployed CloudFormation template to automate configuration Configured appropriate Security Groups Deployed Microsoft AD on AWS Set up DNS Resolver for Hybrid connection Cleaned up resources after completion Successfully practiced VPC Peering:\nCreated Security Groups and EC2 instances for VPCs Updated Network ACL to allow traffic Created VPC Peering connection between VPCs Enabled Cross-Peer DNS resolution Tested connectivity between VPCs Successfully practiced Transit Gateway:\nUnderstood Transit Gateway components: Attachments, Route Tables, Routes Deployed Transit Gateway using CloudFormation Created Transit Gateway attachments for VPC Configured Transit Gateway route tables Added routes to VPC route tables to route through TGW Tested connectivity through Transit Gateway Comprehensive knowledge:\nLearned how to choose between VPC Peering and Transit Gateway for different scenarios Understood how to set up complex multi-VPC network architecture Gained ability to automate deployment using CloudFormation "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn about AWS Compute services: EC2 and Lightsail Master AWS Storage services: EFS, FSx, S3, and advanced features Understand AWS Application Migration Service (MGN) for application migration Learn about AWS Storage Gateway and AWS Backup Master Disaster Recovery concepts: RTO and RPO Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Amazon Elastic Compute Cloud (EC2) - Learn about Amazon Lightsail 10/06/2025 10/06/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 3 - Learn about Amazon EFS/FSX - Learn about AWS Application Migration Service (MGN) 10/07/2025 10/07/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 4 - Learn about Amazon S3: + S3 Access Point + S3 Control Access + S3 Endpoint + S3 Glacier + S3 CORS 10/08/2025 10/08/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 5 - Learn about AWS Storage Gateway - Learn about AWS Backup 10/09/2025 10/09/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 6 - Learn about Disaster Recovery + Recovery Time Objective (RTO) + Recovery Point Objective (RPO) 10/10/2025 10/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Mastered AWS Compute services:\nAmazon EC2: Understood scalable virtual server service, instance types, pricing models Amazon Lightsail: Learned how to use simple VPS service for small applications Deep understanding of File Storage services:\nAmazon EFS: Mastered auto-scaling file system for Linux Amazon FSx: Understood fully managed file systems (FSx for Windows, FSx for Lustre) Proficient in Amazon S3 and advanced features:\nS3 Access Point: Learned how to simplify access management to S3 buckets S3 Control Access: Understood granular access control S3 Endpoint: Mastered private connectivity to S3 from VPC S3 Glacier: Understood long-term storage with low cost S3 CORS: Learned how to configure Cross-Origin Resource Sharing Understood Migration and Backup:\nAWS Application Migration Service (MGN): Mastered how to migrate applications from on-premises to AWS AWS Storage Gateway: Understood how to connect on-premises storage with AWS cloud AWS Backup: Learned how to create and manage centralized backup for AWS services Mastered Disaster Recovery concepts:\nRecovery Time Objective (RTO): Understood maximum acceptable time to restore system Recovery Point Objective (RPO): Learned maximum acceptable data loss during incidents Understood DR strategies: Backup \u0026amp; Restore, Pilot Light, Warm Standby, Multi-Site "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Practice deploying AWS Backup for data protection Practice VM Import/Export to migrate VMs between on-premises and AWS Deploy File Storage Gateway to connect on-premises storage with AWS Join project team and propose ideas Determine group project topic: Travel Guide Web App Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: Deploy AWS Backup for system + Create Backup plan + Setup notifications 10/13/2025 10/13/2025 https://000013.awsstudygroup.com/vi/ 3 - Practice: VM Import/Export + Import VM to AWS + Export EC2 Instance from AWS 10/14/2025 10/14/2025 https://000014.awsstudygroup.com/vi/ 4 - Practice: Deploy File Storage Gateway + Create Storage Gateway + Create File Shares + Connect File Shares from On-premise machine 10/15/2025 10/15/2025 https://000024.awsstudygroup.com/vi/ 5 - Join team and propose project ideas 10/16/2025 10/16/2025 https://www.datacamp.com/blog/top-aws-projects 6 - Proposal: Decided to build a web app with Travel Guide theme. 10/17/2025 10/17/2025 Week 6 Achievements: Successfully deployed AWS Backup:\nCreated Backup plan with automated backup rules Configured backup vault to store backups Set up SNS notifications for backup completion or failure Understood how to restore data from backup Mastered best practices for backup strategy Successfully practiced VM Import/Export:\nImported VMs from on-premises environment to AWS as AMI or EC2 instance Understood requirements and limitations for VM import (file format, size, OS) Exported EC2 instance from AWS to on-premises environment Mastered workload migration process between environments Deployed File Storage Gateway:\nCreated and configured Storage Gateway in hybrid environment Created File Shares to share data between on-premises and S3 Connected File Shares from on-premises machine via SMB/NFS protocol Understood how data is synchronized between local cache and S3 Mastered Storage Gateway use cases in hybrid architecture Initiated group project:\nJoined team and got acquainted with members Proposed and discussed project ideas Decided on project topic: Travel Guide Web Application Defined project scope and objectives Started planning for development phases "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Practice deploying Amazon S3 with advanced features: static website, CloudFront, versioning Practice deploying Amazon FSx for Windows with enterprise features Learn about Shared Responsibility Model in AWS Master AWS Identity and Access Management (IAM) and security services Support finalizing architecture diagram for group project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: Getting Started with Amazon S3 + Create S3 Bucket and upload data + Enable static website feature + Configure Block Public Access and Public Object + Test Website + Accelerate Static Website with Cloudfront + Bucket Versioning + Move Object and copy Object to another region 10/20/2025 10/20/2025 https://000057.awsstudygroup.com/vi/ 3 - Practice: Deploy FSx on Windows: + Create practice environment + Create SSD and HDD Multi-AZ file system + Create file share + Test and monitor performance + Enable data deduplication + Enable shadow copies + Manage user Sessions and open files + Enable user storage quotas + Enable continuously available shares + Scale throughput capacity and storage capacity 10/21/2025 10/21/2025 https://000025.awsstudygroup.com/vi/ 4 - Learn about Shared Responsibility Model - Learn about AWS Identity and Access Management (IAM): + Root Account + What is IAM? + IAM Principal + IAM Policy + IAM Role 10/22/2025 10/22/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 5 - Learn about Amazon Cognito - Learn about AWS Organization - Learn about Identity Center (SSO) - Learn about AWS KMS - Learn about AWS Security Hub 10/23/2025 10/23/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 6 - Proposal: Help Leader complete project architecture diagram 10/24/2025 10/24/2025 Week 7 Achievements: Proficient in Amazon S3 with advanced features:\nSuccessfully created S3 Bucket and uploaded data Enabled Static Website Hosting to host static website Configured Block Public Access and Public Object policies Integrated CloudFront to accelerate global website access Enabled Bucket Versioning to manage object versions Performed object migration and replication between regions Understood S3 lifecycle policies and storage classes Successfully deployed Amazon FSx for Windows:\nCreated practice environment with VPC and Active Directory Created SSD and HDD Multi-AZ file system for high availability Created and managed file shares Tested and monitored file system performance Enabled data deduplication to optimize storage Enabled shadow copies for point-in-time recovery Managed user sessions and open files Enabled user storage quotas Enabled continuously available shares Scaled throughput capacity and storage capacity Mastered Security and Identity Management:\nShared Responsibility Model: Clearly understood security responsibilities between AWS and customers AWS IAM: Mastered identity and access management Root Account and best practices IAM Users, Groups, Roles IAM Policies (identity-based and resource-based) IAM Principal Amazon Cognito: Understood user authentication service for applications AWS Organizations: Learned how to manage multiple AWS accounts Identity Center (SSO): Understood single sign-on for AWS AWS KMS: Mastered encryption key management AWS Security Hub: Learned how to aggregate and manage security findings Contributed to group project:\nSupported Leader in finalizing system architecture diagram Applied knowledge of S3, CloudFront to project design Proposed security solutions for application "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn about Database concepts and AWS database services Master Amazon RDS, Aurora, ElastiCache, and Redshift Practice deploying Amazon RDS with backup and restore Review and consolidate knowledge for midterm exam Complete midterm exam Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Database Concept - Learn about Amazon RDS - Learn about Amazon Aurora - Learn about Amazon ElastiCache - Learn about Amazon Redshift 10/27/2025 10/27/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://cloudjourney.awsstudygroup.com/ 3 - Practice: Amazon Relational Database Service (Amazon RDS) + Create necessary services: VPC, EC2 Security Group, RDS Security Group, DB Subnet Group + Create EC2 instance + Create RDS database instance + Deploy application + Backup and restore 10/28/2025 10/28/2025 https://000005.awsstudygroup.com/vi/ 4 - Focus on reviewing and consolidating knowledge for midterm exam 10/29/2025 10/29/2025 https://cloudjourney.awsstudygroup.com/ 5 - Focus on reviewing and consolidating knowledge for midterm exam 10/30/2025 10/30/2025 https://cloudjourney.awsstudygroup.com/ 6 - Midterm exam 10/31/2025 10/31/2025 Week 8 Achievements: Mastered Database concepts:\nUnderstood Relational Database (RDBMS) and NoSQL Database Learned differences between OLTP and OLAP workloads Mastered concepts: normalization, indexing, transactions, ACID properties Deep understanding of AWS Database services:\nAmazon RDS: Fully managed relational database service Supports multiple database engines: MySQL, PostgreSQL, MariaDB, Oracle, SQL Server Multi-AZ deployment for high availability Read replicas for scaling reads Automated backups and manual snapshots Amazon Aurora: MySQL/PostgreSQL-compatible database with high performance 5x faster than MySQL and 3x faster than PostgreSQL Auto-scaling storage Aurora Serverless for intermittent workloads Amazon ElastiCache: In-memory caching service Supports Redis and Memcached Improves application performance Amazon Redshift: Data warehouse for big data analytics Columnar storage Massively parallel processing (MPP) Successfully practiced with Amazon RDS:\nCreated VPC and necessary network components Created EC2 Security Group and RDS Security Group with appropriate rules Created DB Subnet Group for Multi-AZ deployment Created EC2 instance as application server Created RDS database instance with optimal configuration Deployed application connected to RDS Performed database backup (automated and manual snapshot) Restored database from snapshot Understood RDS maintenance windows and parameter groups Completed midterm exam:\nReviewed and consolidated knowledge of AWS services learned Mastered concepts: VPC, EC2, S3, RDS, IAM, CloudFormation Understood best practices and use cases for each service Successfully completed midterm exam "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn about AWS Rekognition - AI/ML service for image and video processing Explore AWS Rekognition demos and features Deploy Detect Labels feature to Travel Guide project Optimize Detect Labels function with label filtering capability Test and ensure feature works well on website Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS Rekognition 11/03/2025 11/03/2025 https://cloudjourney.awsstudygroup.com/ 3 - Test AWS Rekognition demos 11/04/2025 11/04/2025 https://docs.aws.amazon.com/rekognition/ 4 - Start deploying AWS Rekognition Detect Labels to project 11/05/2025 11/05/2025 5 - Redeploy Detect Labels with label filtering feature 11/06/2025 11/06/2025 6 - Test Detect Labels feature on website 11/07/2025 11/07/2025 Week 9 Achievements: Mastered AWS Rekognition:\nUnderstood AWS AI/ML service for computer vision Learned main features: Object and scene detection (Detect Labels) Facial analysis and face comparison Text detection in images (Detect Text) Content moderation Celebrity recognition Video analysis Understood confidence scores and usage Mastered pricing model and best practices Explored and tested AWS Rekognition:\nTried available AWS Rekognition demos Tested detection feature accuracy Evaluated applicability to Travel Guide project Understood API calls and response format Successfully deployed Detect Labels to project:\nIntegrated AWS Rekognition SDK into backend Created Lambda function to process images Configured IAM roles and permissions for Rekognition Connected with S3 to retrieve images for analysis Implemented API endpoint to call Rekognition service Optimized Detect Labels function:\nRedeployed with label filtering by confidence threshold Added feature to filter labels by categories suitable for Travel Guide Optimized response time and error handling Implemented caching to reduce API calls Handled edge cases and error scenarios Tested and ensured quality:\nTested Detect Labels feature on website Tested with various image types Ensured UI/UX displays labels intuitively Verified performance and response time Collected feedback for improvement "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This worklog documents my 12-week internship journey at the First Cloud Journey program, where I learned and practiced with AWS services, from basic to advanced knowledge, while deploying a real-world project: Travel Guide Web Application.\nPhase 1: AWS Fundamentals (Weeks 1-4) In the initial phase, I focused on building foundational AWS knowledge, including core services for networking, compute, and infrastructure.\nWeek 1: Getting Started with AWS and Environment Setup - Learned about AWS, basic services, created AWS Free Tier account, configured IAM and AWS CLI.\nWeek 2: Exploring Amazon VPC and Networking - Learned about VPC, Subnets, Route Tables, Internet Gateway, NAT Gateway, Security Groups, and Network ACLs.\nWeek 3: VPC Connectivity and Load Balancing - Explored VPC Peering, Transit Gateway, VPN, Direct Connect, and Load Balancer types (ALB, NLB, CLB, GWLB).\nWeek 4: DNS, CloudFormation, and Transit Gateway - Practiced with Route 53, CloudFormation (IaC), Hybrid DNS setup, VPC Peering, and Transit Gateway.\nPhase 2: Storage, Security, and Database (Weeks 5-8) This phase focused on storage services, security, and databases, while initiating the group project.\nWeek 5: Storage Services and Disaster Recovery - Learned about EC2, Lightsail, EFS, FSx, S3 (Access Point, Glacier, CORS), Storage Gateway, AWS Backup, and RTO/RPO concepts.\nWeek 6: Backup, Migration Practice, and Project Kickoff - Deployed AWS Backup, VM Import/Export, File Storage Gateway. Joined team and decided on Travel Guide Web App project.\nWeek 7: S3 Advanced, FSx, and Security - Practiced S3 Static Website with CloudFront, FSx for Windows with enterprise features. Learned about IAM, Cognito, Organizations, KMS, Security Hub. Finalized project architecture.\nWeek 8: Database Services and Midterm Exam - Learned about RDS, Aurora, ElastiCache, Redshift. Practiced RDS deployment with backup/restore. Reviewed and completed midterm exam.\nPhase 3: AI/ML and Messaging Services (Weeks 9-11) This phase focused on integrating AI/ML and messaging services into the real-world project.\nWeek 9: AWS Rekognition - Computer Vision - Explored AWS Rekognition, deployed Detect Labels feature to analyze travel images, optimized with label filtering.\nWeek 10: Amazon SQS - Message Queue - Learned about SQS (Standard and FIFO Queue), deployed to project for asynchronous image processing, ensured sequential processing. Attended AWS Cloud Mastery Series #1.\nWeek 11: SNS and SES - Notification Services - Explored SNS and SES, deployed email system for users (welcome, booking confirmation, recommendations). Attended AWS Cloud Mastery Series #2.\nPhase 4: Finalization and Demo (Week 12) Week 12: Testing, Bug Fixes, and Project Completion - Comprehensive website and feature testing, bug fixes, demo preparation, completed personal workshop. Attended AWS Cloud Mastery Series #3.\nSummary After 12 weeks of internship, I have:\nMastered core AWS services: VPC, EC2, S3, RDS, Lambda, CloudFront Practiced with advanced services: Rekognition, SQS, SNS, SES, Transit Gateway, CloudFormation Successfully deployed Travel Guide Web Application with serverless architecture Participated in AWS Cloud Mastery Series events and networked with the community Completed personal workshop with comprehensive documentation "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.1-workshop-overview/","title":"5.1 Overview of the Workshop","tags":[],"description":"","content":"Introduction This workshop provides an overview of the Travel Journal Web Application built during the project.\nThe main goal is to help learners understand how to design and deploy a modern web system using a serverless architecture on AWS, integrating services such as AWS Rekognition, AWS Location Service, DynamoDB, and Amazon S3 to create a scalable, cost-optimized, and practical product.\nThe platform allows users to record their travel experiences by uploading photos, tagging locations, creating posts, and sharing personal journeys.\nWhen a photo is uploaded, Amazon Rekognition analyzes its content and automatically assigns labels.\nMetadata of each post is saved to DynamoDB, images are stored in S3, and static content is delivered through Amazon CloudFront to ensure fast performance globally.\nKey Feature ‚Äì Location Mapping A unique highlight of the system is the ability to visualize user travel routes using AWS Location Service, displaying the geographic points of each post on an interactive map.\nServerless Backend Architecture The backend is fully built on a serverless stack:\nAWS Lambda ‚Äì handles business logic API Gateway ‚Äì exposes REST APIs DynamoDB ‚Äì stores post and user metadata S3 + SQS + Lambda pipeline ‚Äì processes uploaded images Rekognition ‚Äì automatic labeling CloudFront ‚Äì high-performance content delivery This architecture eliminates server management and automatically scales based on workload.\nFrontend Implementation The frontend is a React-based SPA deployed on either:\nAWS CloudFront ensuring easy deployment and fast global access.\nWorkshop Goal By the end of this workshop, learners will understand:\nHow to integrate multiple AWS services to build a complete web system How serverless architecture works How to process media data with AI Rekognition How to deploy, secure, and evaluate system performance "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.6-cloudfront-s3-location/5.6.2-cloudfront-cdn/","title":"CloudFront CDN Configuration","tags":[],"description":"","content":"Amazon CloudFront (Static + Dynamic) In the Travel Guide project, CloudFront serves as the CDN for:\nStatic web (React build) from StaticSiteBucket Images from ArticleImagesBucket with path patterns The \u0026ldquo;dynamic\u0026rdquo; part (API) currently runs directly through API Gateway URL. This workshop describes (1) deployed static setup + (2) recommended dynamic model if you want to route API through CloudFront.\nArchitecture Overview The system uses CloudFront as a unified entry point for both static content and media files, providing:\nGlobal content delivery HTTPS enforcement Compression Caching optimization CloudFront Static ‚Äî Host React from Private S3 Origin 1: S3 StaticSiteBucket (Private) Configuration:\nAccess via Origin Access Identity (OAI) DefaultRootObject: index.html ViewerProtocolPolicy: redirect-to-https Compression enabled Origin Access Identity (OAI) Setup OAI allows CloudFront to access private S3 buckets securely without making them public.\nS3 Bucket Policy for OAI:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontOAI\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity \u0026lt;OAI-ID\u0026gt;\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::static-site-bucket/*\u0026#34; } ] } Deploying Frontend Build and Upload cd travel-guide-frontend npm install npm run build # Upload build/ to StaticSiteBucket aws s3 sync build/ s3://\u0026lt;StaticSiteBucketName\u0026gt;/ --delete Invalidation After Deploy After each frontend deployment, create an invalidation so users receive the latest version:\naws cloudfront create-invalidation \\ --distribution-id \u0026lt;DISTRIBUTION_ID\u0026gt; \\ --paths \u0026#34;/*\u0026#34; Best Practice:\nInvalidate only changed paths to save costs Use versioned filenames (e.g., app.v123.js) to avoid invalidation Invalidate index.html and other non-hashed files CloudFront for Images ‚Äî Cache and Accelerate Media Loading Origin 2: S3 ArticleImagesBucket (Private) Cache behaviors by path:\narticles/* thumbnails/* avatars/* covers/* Cache Behaviors Configuration When frontend needs to display images:\nConstruct URL like: https://\u0026lt;cloudfront-domain\u0026gt;/\u0026lt;key\u0026gt; Example: https://d1zx7zcxy3hbwd.cloudfront.net/articles/... Benefits:\nImages cached at edge locations globally Reduced S3 GET requests Faster load times for users Automatic HTTPS Cache Strategy Recommendations Static Web index.html:\nTTL: Low (0-60 seconds) or use custom cache-control Reason: Ensure users get latest app version Hashed assets (js/css):\nTTL: High (1 week - 1 year) Reason: Filenames change with content, safe to cache long-term Example Cache-Control Headers:\n# index.html Cache-Control: no-cache, must-revalidate # app.abc123.js Cache-Control: public, max-age=31536000, immutable Images Configuration:\nTTL: High (images rarely change) If image changes with same key ‚Üí change key (best practice) or invalidate Recommended TTL:\nMin: 1 day Default: 7 days Max: 1 year CloudFront Dynamic ‚Äî Recommended Model (Optional) If you want \u0026ldquo;dynamic\u0026rdquo; (API) to run through CloudFront, add:\nOrigin 3: API Gateway Endpoint (Custom Origin) Configuration:\nBehavior: /api/* (or /Prod/*) ‚Üí API origin Cache: Disabled or very low TTL Forward headers: Authorization + query strings Forward cookies: As needed Goals:\nSingle domain for both web + API Reduce global latency for API (depending on use case) Note: Configuring API through CloudFront requires proper path normalization and CORS setup. Also consider caching strategy to avoid \u0026ldquo;wrong cache\u0026rdquo; based on tokens.\nMonitoring and Metrics CloudWatch Metrics Key metrics to monitor:\nCache Hit Ratio\nTarget: \u0026gt; 80% for static content Low ratio indicates caching issues 4xx Error Rate\nCommon: 403 (access denied), 404 (not found) Check OAI permissions and file paths 5xx Error Rate\nOrigin errors Check S3 availability Bytes Downloaded\nTrack bandwidth usage Identify popular content Requests\nTotal requests per time period Identify traffic patterns Operations \u0026amp; Troubleshooting Common Issues 1. 403 Access Denied Causes:\nOAI not configured correctly S3 bucket policy missing CloudFront permission Object doesn\u0026rsquo;t exist Solution:\nVerify OAI ID in bucket policy Check object exists in S3 Test direct S3 access (temporarily) 2. Stale Content After Deploy Causes:\nNo invalidation created Browser cache Solution:\nCreate CloudFront invalidation Use versioned filenames Set appropriate Cache-Control headers 3. Slow First Load Cause:\nCold cache (first request to edge location) Expected behavior:\nFirst request: Slower (origin fetch) Subsequent requests: Fast (cached) 4. High Origin Requests Causes:\nLow cache hit ratio TTL too short Query strings not handled properly Solution:\nIncrease TTL for static content Configure query string forwarding Use cache key normalization Security Best Practices HTTPS Only\nRedirect HTTP to HTTPS Use TLS 1.2 minimum Origin Access Identity\nNever make S3 buckets public Use OAI for CloudFront access Geo Restrictions (Optional)\nWhitelist/blacklist countries if needed AWS WAF Integration (Optional)\nProtect against common web exploits Rate limiting Signed URLs/Cookies (Optional)\nFor premium/private content Time-limited access Cost Optimization Strategies:\nHigh Cache Hit Ratio\nReduces origin requests Lower S3 GET costs Compression\nReduces data transfer Faster load times Price Class Selection\nUse fewer edge locations if users are regional Price Class 100 (US, Europe, Asia) Reserved Capacity (High traffic)\nContact AWS for discounts Monitor Invalidations\nFirst 1,000 paths/month free Use versioned filenames instead Key Takeaways CloudFront accelerates content delivery globally OAI secures private S3 access Cache behaviors optimize different content types Invalidation ensures users get latest content Monitoring helps identify and resolve issues Proper TTL balances freshness and performance "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.7-security/5.7.2-input-sanitization/","title":"Input Sanitization","tags":[],"description":"","content":"Input Sanitization - XSS Prevention What is Input Sanitization? Input Sanitization is the process of cleaning and validating user input to prevent malicious code injection.\nCommon attacks prevented:\nXSS (Cross-Site Scripting) - Injecting JavaScript SQL/NoSQL Injection - Manipulating database queries Path Traversal - Accessing unauthorized files Command Injection - Executing system commands Implementation Overview We created a comprehensive security utilities module with 9 functions to sanitize and validate all user inputs.\nFile Created: travel-guide-backend/shared/layers/common/python/security_utils.py\nSecurity Functions 1. HTML Sanitization Function: sanitize_html(text)\nPurpose: Escape HTML entities to prevent XSS attacks\nBefore:\ntitle = data.get(\u0026#34;title\u0026#34;, \u0026#34;\u0026#34;).strip() # ‚ùå Vulnerable: \u0026lt;script\u0026gt;alert(\u0026#39;XSS\u0026#39;)\u0026lt;/script\u0026gt; After:\nfrom security_utils import sanitize_html title = sanitize_html(data.get(\u0026#34;title\u0026#34;, \u0026#34;\u0026#34;)) # ‚úÖ Safe: \u0026amp;lt;script\u0026amp;gt;alert(\u0026#39;XSS\u0026#39;)\u0026amp;lt;/script\u0026amp;gt; Implementation:\nimport html def sanitize_html(text: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Escape HTML entities to prevent XSS\u0026#34;\u0026#34;\u0026#34; if not text: return \u0026#34;\u0026#34; # Escape HTML special characters sanitized = html.escape(text) # Remove null bytes sanitized = sanitized.replace(\u0026#39;\\x00\u0026#39;, \u0026#39;\u0026#39;) return sanitized.strip() 2. String Sanitization Function: sanitize_string(text, max_length=1000)\nPurpose: Clean and validate string inputs\ndef sanitize_string(text: str, max_length: int = 1000) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Sanitize general string input\u0026#34;\u0026#34;\u0026#34; if not text: return \u0026#34;\u0026#34; # Remove control characters sanitized = \u0026#39;\u0026#39;.join(char for char in text if char.isprintable() or char.isspace()) # Trim whitespace sanitized = sanitized.strip() # Enforce max length if len(sanitized) \u0026gt; max_length: sanitized = sanitized[:max_length] return sanitized 3. Coordinate Validation Function: validate_coordinates(latitude, longitude)\nPurpose: Ensure coordinates are valid\ndef validate_coordinates(latitude: float, longitude: float) -\u0026gt; tuple: \u0026#34;\u0026#34;\u0026#34;Validate geographic coordinates\u0026#34;\u0026#34;\u0026#34; try: lat = float(latitude) lng = float(longitude) # Check ranges if not (-90 \u0026lt;= lat \u0026lt;= 90): raise ValueError(\u0026#34;Latitude must be between -90 and 90\u0026#34;) if not (-180 \u0026lt;= lng \u0026lt;= 180): raise ValueError(\u0026#34;Longitude must be between -180 and 180\u0026#34;) return (lat, lng) except (ValueError, TypeError) as e: raise ValueError(f\u0026#34;Invalid coordinates: {e}\u0026#34;) 4. Email Validation Function: validate_email(email)\nPurpose: Validate email format\nimport re def validate_email(email: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Validate email address format\u0026#34;\u0026#34;\u0026#34; if not email: raise ValueError(\u0026#34;Email is required\u0026#34;) # Basic email regex pattern = r\u0026#39;^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\u0026#39; if not re.match(pattern, email): raise ValueError(\u0026#34;Invalid email format\u0026#34;) return email.lower().strip() 5. S3 Key Validation Function: validate_s3_key(key)\nPurpose: Prevent path traversal attacks\ndef validate_s3_key(key: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Validate S3 object key to prevent path traversal\u0026#34;\u0026#34;\u0026#34; if not key: raise ValueError(\u0026#34;S3 key is required\u0026#34;) # Check for path traversal attempts if \u0026#39;..\u0026#39; in key or key.startswith(\u0026#39;/\u0026#39;): raise ValueError(\u0026#34;Invalid S3 key: path traversal detected\u0026#34;) # Check for null bytes if \u0026#39;\\x00\u0026#39; in key: raise ValueError(\u0026#34;Invalid S3 key: null byte detected\u0026#34;) return key.strip() 6. Article Ownership Validation Function: validate_article_ownership(article, user_id)\nPurpose: Ensure user owns the article\ndef validate_article_ownership(article: dict, user_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Validate that user owns the article\u0026#34;\u0026#34;\u0026#34; if not article: raise ValueError(\u0026#34;Article not found\u0026#34;) article_owner = article.get(\u0026#39;owner_id\u0026#39;) or article.get(\u0026#39;ownerId\u0026#39;) if article_owner != user_id: raise PermissionError(\u0026#34;You don\u0026#39;t have permission to modify this article\u0026#34;) return True 7. Tag Sanitization Function: sanitize_tags(tags)\nPurpose: Validate and limit tags\ndef sanitize_tags(tags: list) -\u0026gt; list: \u0026#34;\u0026#34;\u0026#34;Sanitize and validate tags\u0026#34;\u0026#34;\u0026#34; if not tags: return [] # Limit number of tags MAX_TAGS = 10 if len(tags) \u0026gt; MAX_TAGS: tags = tags[:MAX_TAGS] # Sanitize each tag sanitized_tags = [] for tag in tags: if isinstance(tag, str): # Clean tag clean_tag = sanitize_string(tag, max_length=50) if clean_tag: sanitized_tags.append(clean_tag) return sanitized_tags 8. Image Key Validation Function: validate_image_key(key, article_id, owner_id)\nPurpose: Validate image ownership\ndef validate_image_key(key: str, article_id: str, owner_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Validate that image belongs to article and user\u0026#34;\u0026#34;\u0026#34; # Check S3 key format validate_s3_key(key) # Check if key starts with correct article path expected_prefix = f\u0026#34;articles/{article_id}/\u0026#34; if not key.startswith(expected_prefix): raise PermissionError(\u0026#34;Image does not belong to this article\u0026#34;) return True 9. Rate Limiting Key Function: rate_limit_key(user_id, action)\nPurpose: Generate key for rate limiting\ndef rate_limit_key(user_id: str, action: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate rate limit key for user action\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;rate_limit:{user_id}:{action}\u0026#34; Integration with Lambda Functions Updated: create_article.py Before (Vulnerable):\ndef lambda_handler(event, context): data = json.loads(event[\u0026#39;body\u0026#39;]) # ‚ùå No sanitization title = data.get(\u0026#34;title\u0026#34;, \u0026#34;\u0026#34;).strip() content = data.get(\u0026#34;content\u0026#34;, \u0026#34;\u0026#34;).strip() tags = data.get(\u0026#34;tags\u0026#34;, []) # Store directly in DynamoDB # ‚Üí XSS vulnerable! After (Secure):\nfrom security_utils import ( sanitize_html, sanitize_tags, validate_coordinates, validate_image_key ) def lambda_handler(event, context): data = json.loads(event[\u0026#39;body\u0026#39;]) user_id = event[\u0026#39;requestContext\u0026#39;][\u0026#39;authorizer\u0026#39;][\u0026#39;claims\u0026#39;][\u0026#39;sub\u0026#39;] # ‚úÖ Sanitize all inputs title = sanitize_html(data.get(\u0026#34;title\u0026#34;, \u0026#34;\u0026#34;)) content = sanitize_html(data.get(\u0026#34;content\u0026#34;, \u0026#34;\u0026#34;)) tags = sanitize_tags(data.get(\u0026#34;tags\u0026#34;, [])) # ‚úÖ Validate coordinates lat, lng = validate_coordinates( data.get(\u0026#34;latitude\u0026#34;), data.get(\u0026#34;longitude\u0026#34;) ) # ‚úÖ Validate image ownership for image_key in data.get(\u0026#34;imageKeys\u0026#34;, []): validate_image_key(image_key, article_id, user_id) # Now safe to store Testing Test XSS Protection # Attempt XSS attack curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;\u0026lt;script\u0026gt;alert(\\\u0026#34;XSS\\\u0026#34;)\u0026lt;/script\u0026gt;\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026lt;img src=x onerror=alert(1)\u0026gt;\u0026#34;, \u0026#34;latitude\u0026#34;: 10.8231, \u0026#34;longitude\u0026#34;: 106.6297 }\u0026#39; # Expected: HTML escaped # Title: \u0026#34;\u0026amp;lt;script\u0026amp;gt;alert(\\\u0026#34;XSS\\\u0026#34;)\u0026amp;lt;/script\u0026amp;gt;\u0026#34; # Content: \u0026#34;\u0026amp;lt;img src=x onerror=alert(1)\u0026amp;gt;\u0026#34; Test Coordinate Validation # Invalid coordinates curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;latitude\u0026#34;: 999, \u0026#34;longitude\u0026#34;: -999 }\u0026#39; # Expected: 400 Bad Request # Error: \u0026#34;Invalid coordinates: Latitude must be between -90 and 90\u0026#34; Test Tag Limits # Too many tags curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;latitude\u0026#34;: 10, \u0026#34;longitude\u0026#34;: 106, \u0026#34;tags\u0026#34;: [\u0026#34;tag1\u0026#34;, \u0026#34;tag2\u0026#34;, ..., \u0026#34;tag20\u0026#34;] }\u0026#39; # Expected: Only first 10 tags saved Validation Rules Title Max length: 200 characters HTML escaped No control characters Content Max length: 10,000 characters HTML escaped No control characters Tags Max count: 10 tags Max length per tag: 50 characters Sanitized strings Coordinates Latitude: -90 to 90 Longitude: -180 to 180 Must be numbers Images Max size: 10 MB Allowed types: JPEG, PNG, GIF, WebP Ownership validated Best Practices 1. Sanitize at Entry Point ‚úÖ Do: Sanitize in Lambda handler\nBefore processing Before storing in database ‚ùå Don\u0026rsquo;t: Sanitize in frontend only\nCan be bypassed Not secure 2. Validate Everything ‚úÖ Do: Validate all user inputs\nType checking Range checking Format validation 3. Use Allowlists ‚úÖ Do: Define allowed values\nAllowed file types Allowed characters Allowed ranges ‚ùå Don\u0026rsquo;t: Use blocklists\nEasy to bypass Incomplete protection 4. Escape Output ‚úÖ Do: Escape when displaying\nHTML escape in templates URL encode in links JSON encode in APIs Key Takeaways Input sanitization prevents XSS and injection attacks HTML escaping is essential for user-generated content Validation should happen server-side Security utils provide reusable sanitization functions All inputs must be validated - never trust user data Defense in depth - sanitize at multiple layers "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.3-backend-articles/5.3.2-dynamotable/","title":" DynamoDB Tables Overview","tags":[],"description":"","content":"5.4 Overview of DynamoDB Tables This section explains all DynamoDB tables defined inside the backend architecture.\nEach table serves a specific role in powering article management, user interactions, gallery rendering, location services, and user profiles.\nComplete DynamoDB Tables Architecture 1. ArticlesTable Purpose:\nStores all articles created by users. Each record represents a complete travel post including metadata, visibility settings, timestamps, and ownership.\nPrimary Key:\narticleId (HASH) Attributes:\nvisibility ‚Äî public/private createdAt ‚Äî ISO timestamp ownerId ‚Äî ID of the user who created the article status ‚Äî draft/published/flagged Indexes:\ngsi_visibility_createdAt (visibility, createdAt) ‚Üí For listing public articles in the home feed. gsi_owner_createdAt (ownerId, createdAt) ‚Üí For user profile pages. gsi_status_createdAt (status, createdAt) ‚Üí For admin filtering and moderation. 2. UserFavoritesTable Purpose:\nStores the relationship between a user and the articles they have favorited.\nPrimary Key (Composite):\nuserId (HASH) articleId (RANGE) This structure allows efficient:\nChecking whether a user has favorited an article Listing all favorites of the current user 3. GalleryPhotosTable Purpose:\nStores metadata for individual images extracted from articles.\nUsed for gallery browsing, filtering by tags, and building trending metrics.\nPrimary Key:\nphoto_id (HASH) Metadata may include:\narticleId ‚Äî which article the photo belongs to S3 image key detected tags geolocation or timestamps 4. GalleryTrendsTable Purpose:\nMaintains tag popularity statistics for the gallery.\nThe article/image ingestion process updates counts within this table.\nPrimary Key:\ntag_name (HASH) Usage:\nProvides trending tags for the homepage Enables tag-based article recommendation 5. UserProfilesTable Purpose:\nStores public profile information of users displayed in article feeds and profile pages.\nPrimary Key:\nuserId (HASH) Attributes include:\navatar image URL full name bio/description This table keeps profile data separate from Cognito, allowing customization and public display without exposing private identity data.\n6. LocationCacheTable Purpose:\nCaches results from AWS Location Service\u0026rsquo;s reverse-geocoding to reduce cost and improve performance.\nPrimary Key:\ngeohash (HASH) Attributes:\nhuman-readable location TTL ‚Äî automatically deletes outdated entries Summary Together, these DynamoDB tables form the storage foundation for:\nArticle CRUD Favorites Gallery features User profile rendering Location lookup optimization They are optimized for serverless workloads with high scalability and low latency.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.5-auth-cognito-iam/5.5.2-iam-roles-policies/","title":"IAM Roles &amp; Policies","tags":[],"description":"","content":"IAM - Identity \u0026amp; Access Management Overview IAM is the service for managing access permissions (identity \u0026amp; access). In the Travel Guide workshop, IAM is used to:\nCreate users/groups for developers (standard credentials, MFA) Create roles for Lambda, ECS, EC2 with necessary permissions (least privilege) Create policies allowing Lambda to access S3/DynamoDB/CloudWatch/Cognito Admin Configure trust policies (who can assume roles) Grant API Gateway permission to invoke Lambda (resource-based permission) (Optional) Create roles/permissions for CI/CD What is IAM used for in our project? In the Travel Guide architecture:\nLambda reads/writes DynamoDB Lambda uploads images to S3 Lambda calls Rekognition to analyze images Lambda writes logs to CloudWatch Preparation Steps AWS Account Requirements Prerequisites:\n1 AWS account Permissions to: Create IAM Roles Create Lambda functions Create DynamoDB/S3 resources Role for API Gateway Typically, API Gateway doesn\u0026rsquo;t need a separate IAM role to authorize User Pool. However, for API Gateway to invoke Lambda from integration, you need to add permission to Lambda (resource-based) ‚Äî usually done via CLI or Console auto-create.\nRole for Lambda Functions Purpose: Lambda needs to write logs, access S3/DynamoDB, call Cognito Admin API (if backend manages users).\nCreating Lambda Execution Role Console Steps:\nIAM ‚Üí Roles ‚Üí Create role AWS service ‚Üí Lambda ‚Üí Next Attach managed policies: AWSLambdaBasicExecutionRole (CloudWatch logs) AmazonDynamoDBFullAccess (replace with restricted table-specific policy) AmazonS3ReadOnlyAccess or custom S3 policy DynamoDB Permissions Policy for Article CRUD operations:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/TravelGuide-*\u0026#34; } Why these permissions?\nLambda creates/edits/deletes articles Only access specific tables needed Follows least privilege principle S3 Permissions (Images) Policy for image upload/download:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::travel-guide-*/\\*\u0026#34; } Use cases:\nUpload user-submitted images Retrieve images for display Process images for thumbnails Rekognition Permissions Policy for image analysis:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;rekognition:DetectLabels\u0026#34;, \u0026#34;rekognition:DetectModerationLabels\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } Use cases:\nDetect labels in travel photos Content moderation for inappropriate images Auto-tagging images CloudWatch Logs Permissions Policy for logging:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } Why needed?\nDebug Lambda functions Monitor application behavior Track errors and performance Complete Lambda Execution Role Combined policy example:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/TravelGuide-*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::travel-guide-*/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;rekognition:DetectLabels\u0026#34;, \u0026#34;rekognition:DetectModerationLabels\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Trust Policy Who can assume this role?\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } This allows Lambda service to assume the role and use its permissions.\nBest Practices 1. Least Privilege Principle Only grant permissions actually needed Avoid using * in Resource ARNs when possible Use specific actions instead of * 2. Separate Roles by Function Different Lambda functions = different roles Article service role ‚â† Media service role Easier to audit and manage 3. Use Managed Policies When Appropriate AWSLambdaBasicExecutionRole for logging Custom policies for business logic Combine managed + custom policies 4. Regular Audits Review unused permissions Check for overly permissive policies Use IAM Access Analyzer 5. Tag Resources { \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;ArticleService\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;Environment\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;staging\u0026#34; } ] } Resource-Based Permissions API Gateway Invoke Lambda Lambda resource policy:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;apigateway.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:*:*:function:CreateArticle\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ArnLike\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:execute-api:*:*:*/*/POST/articles\u0026#34; } } } This allows API Gateway to invoke the Lambda function.\nKey Takeaways IAM Roles define what AWS services can do Policies specify exact permissions Trust Policies define who can assume roles Least Privilege is critical for security Separate roles for different services Regular audits prevent permission creep Resource-based policies for cross-service access Common Pitfalls ‚ùå Using Administrator Access\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ‚úÖ Use Specific Permissions\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/Articles\u0026#34; } ‚ùå Sharing Roles Across Services\nOne role for all Lambda functions ‚úÖ Separate Roles\nArticleServiceRole MediaServiceRole AIServiceRole "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.2-iac-multistack/","title":"Infrastructure as Code - Multi-Stack Pattern","tags":[],"description":"","content":"Infrastructure as Code with Multi-Stack Deployment This section explains the Infrastructure as Code (IaC) strategy used to build the Travel Guide Application infrastructure from scratch using CloudFormation/SAM with a multi-stack deployment pattern.\nOverview The Travel Guide Application is built using a microservices architecture deployed across multiple AWS CloudFormation stacks. This approach provides better isolation, faster deployments, and reduced blast radius when making changes.\nKey Concepts Infrastructure as Code: All infrastructure defined in CloudFormation templates Multi-Stack Pattern: Separation of core resources and service-specific resources Cross-Stack References: Sharing resources between stacks using CloudFormation Exports/Imports Deployment Orchestration: Automated deployment using Bash scripts Environment Management: Parameter-based configuration for multiple environments Content IaC Strategy \u0026amp; Tool Selection Multi-Stack Architecture Cross-Stack References Deployment Orchestration Parameter Management Lessons Learned \u0026amp; Best Practices "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-image-processing/5.4.2-detect-labels/","title":"Lambda Detect Labels","tags":[],"description":"","content":"Purpose Analyze images and automatically add descriptive labels such as:\nLocations (beach, mountain, city\u0026hellip;) Objects (people, food, building\u0026hellip;) Activities (swimming, hiking, dining\u0026hellip;) Main Code Explanation a. Receive message from SQS for sqs_record in event.get(\u0026#39;Records\u0026#39;, []): try: # Parse S3 event from SQS message body s3_event = json.loads(sqs_record[\u0026#39;body\u0026#39;]) # Process each S3 record in the event for s3_record in s3_event.get(\u0026#39;Records\u0026#39;, []): try: # Extract S3 information bucket = s3_record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = s3_record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] b. Call rekognition.detect_labels Detect and prioritize important labels:\n# Detect and prioritize labels labels_data = detect_labels_in_image(bucket, key) if not labels_data: print(\u0026#34;No labels detected after prioritization\u0026#34;) results[\u0026#39;failed\u0026#39;] += 1 continue c. Save labels to ArticlesTable and Gallery Update article with tags and save to Gallery:\n# Update article success = update_article_with_tags(article_id, labels_data) if success: results[\u0026#39;succeeded\u0026#39;] += 1 # Save to Gallery tables try: import sys import os # Add current directory to path current_dir = os.path.dirname(os.path.abspath(__file__)) if current_dir not in sys.path: sys.path.insert(0, current_dir) from save_to_gallery import save_photo_to_gallery, update_trending_tags tag_names = [label[\u0026#39;name\u0026#39;] for label in labels_data] image_url = key # S3 key photo_id = key # Use S3 key as unique identifier save_photo_to_gallery( photo_id, image_url, tag_names, status=\u0026#39;public\u0026#39;, article_id=article_id ) update_trending_tags(tag_names, image_url) print(\u0026#34;‚úì Saved to Gallery tables\u0026#34;) Demo: Data After Processing After successful processing, data is stored in DynamoDB tables:\nProcessing Flow Receive moderated image from SQS Call Rekognition detect_labels Filter and prioritize important labels Update ArticlesTable with tags Save metadata to GalleryPhotosTable Update statistics in GalleryTrendsTable Result Articles are automatically tagged Images appear in Gallery with tags Trending tags are updated Users can search by tags "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.2-iac-multistack/5.2.2-multistack-architecture/","title":"Multi-Stack Architecture","tags":[],"description":"","content":"Multi-Stack Pattern Deep Dive What is Multi-Stack Pattern? Instead of deploying all infrastructure in a single CloudFormation stack, we separate resources into multiple stacks based on their lifecycle, dependencies, and blast radius.\nStack Separation Strategy Core Stack (Stateful Resources) Contains shared, stateful resources that rarely change:\nDynamoDB Tables: ArticlesTable, UserProfilesTable, GalleryPhotosTable, etc. S3 Buckets: Image storage, deployment artifacts Cognito User Pool: User authentication VPC \u0026amp; Networking: If needed IAM Roles: Shared execution roles Characteristics:\nChanges infrequently Shared across multiple services High cost of recreation Long deployment time (5-10 minutes) Service Stacks (Stateless Resources) Each microservice has its own stack:\nLambda Functions: Service-specific business logic API Gateway: Service endpoints EventBridge Rules: Service-specific events SQS Queues: Service-specific queues SNS Topics: Service-specific notifications Characteristics:\nChanges frequently Independent deployment Fast deployment (2-3 minutes) Low recreation cost Stack Separation Criteria Criteria Core Stack Service Stack Change Frequency Rarely (monthly) Often (daily/weekly) State Stateful (data) Stateless (compute) Sharing Shared across services Service-specific Deployment Time Long (5-10 min) Short (2-3 min) Blast Radius High (affects all) Low (affects one service) Recreation Cost High (data loss risk) Low (no data loss) Benefits of Multi-Stack Pattern 1. Blast Radius Reduction Problem: In a monolithic stack, any change affects all resources.\nSolution: With multi-stack, updating Article Service doesn\u0026rsquo;t impact Auth or Media services.\nExample:\n# Update only Article Service ./deploy-service.sh article-service staging # Other services (auth, media, gallery) remain untouched Impact: üî¥ High - Prevents cascading failures\n2. Independent Deployment Problem: Can\u0026rsquo;t deploy services independently in monolithic stack.\nSolution: Each service can be deployed, rolled back, or updated independently.\nExample:\n# Deploy new feature to Article Service ./deploy-service.sh article-service staging # Rollback if issues found aws cloudformation delete-stack --stack-name travel-guide-article-service-staging # Other services continue running normally Impact: üî¥ High - Enables continuous deployment\n3. Faster Deployment Problem: Monolithic stack takes 15-20 minutes to deploy.\nSolution: Service stacks deploy in 2-3 minutes.\nComparison:\nMonolithic Stack: 15-20 minutes (all resources) Core Stack: 5-10 minutes (one-time) Service Stack: 2-3 minutes (frequent) Impact: üü° Medium - Improves developer productivity\n4. Parallel Development Problem: Teams block each other when working on same stack.\nSolution: Teams can work on different service stacks simultaneously.\nExample:\nTeam A: Updates Article Service Team B: Updates Media Service Team C: Updates Gallery Service No conflicts, no waiting Impact: üî¥ High - Enables team scalability\n5. Easier Rollback Problem: Rolling back monolithic stack affects all services.\nSolution: Rollback only the affected service stack.\nExample:\n# Bug found in Article Service aws cloudformation delete-stack --stack-name travel-guide-article-service-staging # Redeploy previous version git checkout v1.2.3 ./deploy-service.sh article-service staging # Auth, Media, Gallery services unaffected Impact: üî¥ High - Reduces downtime\n6. Resource Limit Management Problem: CloudFormation has 500 resource limit per stack.\nSolution: Distribute resources across multiple stacks.\nExample:\nCore Stack: ~50 resources (tables, buckets, pools) Each Service Stack: ~20 resources (lambdas, APIs) Total: 6 services √ó 20 = 120 resources (well under limit) Impact: üü¢ Low - Prevents hitting limits\n7. Cost Optimization Problem: Can\u0026rsquo;t optimize costs per service in monolithic stack.\nSolution: Tag and track costs per service stack.\nExample:\nTags: - Key: Service Value: ArticleService - Key: Environment Value: staging - Key: CostCenter Value: Engineering Impact: üü° Medium - Enables cost attribution\nComparison: Monolithic vs Multi-Stack Aspect Monolithic Stack Multi-Stack Pattern Deployment Time 15-20 minutes 2-3 minutes (per service) Blast Radius All services affected Single service affected Rollback All services rolled back Single service rolled back Team Collaboration Sequential (blocking) Parallel (non-blocking) Resource Limit 500 (can hit limit) 500 per stack (scalable) Cost Tracking Difficult Easy (per service) Complexity Low (single stack) Medium (multiple stacks) Maintenance Difficult (large template) Easier (small templates) Stack Dependencies graph TD Core[Core Stack] --\u0026gt; Auth[Auth Service] Core --\u0026gt; Article[Article Service] Core --\u0026gt; Media[Media Service] Core --\u0026gt; AI[AI Service] Core --\u0026gt; Gallery[Gallery Service] Core --\u0026gt; Notification[Notification Service] style Core fill:#ff9999 style Auth fill:#99ccff style Article fill:#99ccff style Media fill:#99ccff style AI fill:#99ccff style Gallery fill:#99ccff style Notification fill:#99ccff Deployment Order:\nDeploy Core Stack first (provides shared resources) Deploy Service Stacks in any order (parallel possible) Key Takeaways Separate stateful and stateless resources into different stacks Core Stack contains shared, rarely-changing resources Service Stacks contain service-specific, frequently-changing resources Benefits include faster deployments, reduced blast radius, and better team collaboration Trade-off is increased complexity in managing multiple stacks Cross-stack references enable resource sharing between stacks When to Use Multi-Stack ‚úÖ Use Multi-Stack when:\nBuilding microservices architecture Multiple teams working on different services Frequent deployments required Need independent service lifecycle ‚ùå Avoid Multi-Stack when:\nSimple monolithic application Single team, infrequent deployments All resources tightly coupled Overhead not justified "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"DevOps on AWS Workshop Date: Monday, November 17, 2025\nTime: 8:30 AM - 5:00 PM\nLocation: AWS Office\nSpeaker Lineup Bao Huynh AWS Community Builder\nAWS Community Builder with experience in DevOps practices and cloud architecture on AWS.\nThinh Nguyen AWS Community Builder\nAWS Community Builder specializing in CI/CD, automation, and infrastructure optimization.\nVi Tran AWS Community Builder\nAWS Community Builder with expertise in container services and modern DevOps workflows.\nEvent Purpose The \u0026ldquo;DevOps on AWS\u0026rdquo; workshop was organized with the following objectives:\nProvide comprehensive knowledge about DevOps practices on AWS Guide building complete CI/CD pipelines Introduce Infrastructure as Code (IaC) with CloudFormation and CDK Practice deploying container services on AWS Learn about monitoring and observability Share best practices and real-world case studies Provide career path guidance in DevOps field Highlights AWS DevOps Services - CI/CD Pipeline Source Control:\nAWS CodeCommit Git strategies: GitFlow and Trunk-based development Build \u0026amp; Test:\nCodeBuild configuration Testing pipelines Deployment:\nCodeDeploy with strategies: Blue/Green deployment Canary deployment Rolling updates Orchestration:\nCodePipeline automation Demo: Full CI/CD pipeline walkthrough\nInfrastructure as Code (IaC) AWS CloudFormation:\nTemplates Stacks management Drift detection AWS CDK (Cloud Development Kit):\nConstructs Reusable patterns Language support (TypeScript, Python, Java, etc.) Demo: Deploying with CloudFormation and CDK\nDiscussion: Comparing and choosing between IaC tools\nContainer Services on AWS Docker Fundamentals:\nMicroservices architecture Containerization concepts Amazon ECR (Elastic Container Registry):\nImage storage Security scanning Lifecycle policies Amazon ECS \u0026amp; EKS:\nDeployment strategies Auto-scaling Orchestration AWS App Runner:\nSimplified container deployment Demo \u0026amp; Case Study: Comparing microservices deployment methods\nMonitoring \u0026amp; Observability CloudWatch:\nMetrics collection Logs aggregation Alarms configuration Custom dashboards AWS X-Ray:\nDistributed tracing Performance insights Bottleneck identification Demo: Full-stack observability setup\nBest Practices:\nAlerting strategies Dashboard design On-call processes DevOps Best Practices \u0026amp; Case Studies Deployment Strategies:\nFeature flags A/B testing Progressive delivery Automated Testing:\nCI/CD integration Test automation frameworks Incident Management:\nIncident response procedures Postmortem analysis Case Studies:\nStartup DevOps transformations Enterprise-scale implementations Q\u0026amp;A \u0026amp; Wrap-up DevOps career pathways AWS certification roadmap Resources and learning paths Knowledge Gained 1. CI/CD Pipeline with AWS DevOps Services Source Control \u0026amp; Git Strategies:\nUnderstanding AWS CodeCommit and Git integration Comparing GitFlow vs Trunk-based development Best practices for branching strategies Build \u0026amp; Test Automation:\nConfiguring CodeBuild for different project types Setting up automated testing pipelines Integration and unit testing in CI/CD Deployment Strategies:\nBlue/Green deployment: Zero-downtime deployments Canary deployment: Gradual rollout with monitoring Rolling updates: Suitable for stateless applications Pipeline Orchestration:\nEnd-to-end CodePipeline automation Multi-stage pipelines Approval gates and manual interventions 2. Infrastructure as Code (IaC) AWS CloudFormation:\nWriting and managing templates Stack management and dependencies Drift detection to identify changes outside IaC Nested stacks for complex architectures AWS CDK:\nUsing programming languages (TypeScript, Python) instead of JSON/YAML Reusable constructs and patterns Higher-level abstractions Testing infrastructure code Tool Selection:\nWhen to use CloudFormation vs CDK Terraform comparison Trade-offs and use cases 3. Container Services Docker \u0026amp; Microservices:\nContainer fundamentals Microservices architecture patterns Containerization best practices Amazon ECR:\nPrivate container registry Image scanning for security vulnerabilities Lifecycle policies for image management ECS vs EKS:\nECS: AWS-native, simpler setup, good AWS services integration EKS: Kubernetes-based, portable, suitable for multi-cloud Deployment strategies for each service Auto-scaling configurations AWS App Runner:\nSimplified container deployment Automatic scaling Suitable use cases 4. Monitoring \u0026amp; Observability CloudWatch:\nCustom and standard metrics Log aggregation from multiple sources Alarm configuration with SNS notifications Dashboard design for different stakeholders AWS X-Ray:\nDistributed tracing for microservices Service map visualization Performance bottleneck identification Application code integration Observability Best Practices:\nThe three pillars: Metrics, Logs, Traces Alerting strategies: Avoiding alert fatigue Dashboard design principles On-call rotation and incident response 5. DevOps Best Practices Progressive Delivery:\nFeature flags for controlled rollouts A/B testing strategies Canary analysis Testing Automation:\nTest pyramid: Unit, Integration, E2E CI/CD integration Test coverage and quality gates Incident Management:\nIncident response procedures Blameless postmortems Learning from failures Real-world Case Studies:\nStartup: Rapid iteration with minimal resources Enterprise: Scaling DevOps practices across teams Personal Experience Overall Impression The DevOps on AWS workshop was an intensive and practical experience. Unlike the previous AI/ML workshop, this one focused more on hands-on practices and real-world scenarios.\nCI/CD Pipeline Demo The CI/CD pipeline demo was the morning\u0026rsquo;s highlight. Seeing a complete pipeline from code commit to production deployment helped me better understand:\nHow AWS DevOps services integrate together Automation at each stage Error handling and rollback strategies Particularly impressed with the Blue/Green deployment demo - zero downtime and instant rollback capability.\nInfrastructure as Code The IaC section opened my mind about \u0026ldquo;infrastructure as software\u0026rdquo;:\nCloudFormation:\nTemplates seem verbose but powerful Drift detection is very useful for production environments AWS CDK:\nGame changer! Writing infrastructure in familiar programming languages Type safety and IDE support Reusable constructs help standardize infrastructure The comparison between CloudFormation and CDK helped me understand when to use which tool.\nContainer Services The containers section was very comprehensive:\nDocker Fundamentals:\nReview of containerization concepts Dockerfile best practices Multi-stage builds ECS vs EKS Comparison:\nECS: Simpler, AWS-native, suitable for AWS-centric workloads EKS: Kubernetes standard, portable, higher learning curve Deployment demos on both platforms clearly showed differences App Runner:\nSurprisingly simple! Deploy containers with just a few clicks Automatic scaling Good fit for simple containerized applications Monitoring \u0026amp; Observability This section was very practical and immediately applicable:\nCloudWatch:\nCustom metrics for business KPIs Log Insights queries are very powerful Dashboard design tips X-Ray:\nImpressive distributed tracing visualization Helps identify bottlenecks in microservices architecture Straightforward integration The full-stack observability demo showed the importance of monitoring in DevOps.\nBest Practices \u0026amp; Case Studies Deployment Strategies:\nFeature flags for progressive rollout A/B testing integration Risk mitigation strategies Case Studies:\nStartup case: Scrappy DevOps with limited resources Enterprise case: Scaling DevOps culture across organization Lessons learned from real failures Networking \u0026amp; Q\u0026amp;A The Q\u0026amp;A session was very valuable:\nDevOps career pathways discussion AWS certification roadmap (DevOps Engineer Professional) Tips from practitioners Networking with DevOps engineers from different companies Challenges Many services and concepts in one day Somewhat limited hands-on time for topic complexity Wanted more practice with CDK Container orchestration needs time to master Highlights Practical, hands-on approach Very relatable real-world case studies Comprehensive coverage of DevOps lifecycle Speakers with deep expertise Good balance between theory and practice Comparison with AI/ML Workshop DevOps workshop more hands-on and practical AI/ML workshop more cutting-edge and exploratory Both valuable but different focus areas DevOps skills more immediately applicable Next Steps After the workshop, I plan to:\nImmediate Actions:\nSetup personal CI/CD pipeline for projects Practice with AWS CDK Implement monitoring for existing applications Short-term Goals:\nBuild complete DevOps pipeline for a project Deep dive into container orchestration (ECS/EKS) Practice IaC with real scenarios Long-term Goals:\nAWS Certified DevOps Engineer - Professional Contribute to DevOps culture in team Build reusable IaC patterns Conclusion The \u0026ldquo;DevOps on AWS\u0026rdquo; workshop provided a solid foundation in DevOps practices on the AWS platform. From CI/CD pipelines to container orchestration, from IaC to observability - the workshop comprehensively and practically covered the entire DevOps lifecycle.\nThe knowledge gained includes not just technical skills but also the mindset of automation, continuous improvement, and collaboration. Case studies and best practices from real-world scenarios were particularly valuable.\nThis workshop was an important next step after the AI/ML workshop, helping me better understand how to deploy and operate applications on AWS professionally and at scale. DevOps skills are essential foundations for any cloud engineer, and this workshop provided an excellent starting point.\nCombined with knowledge from the previous AI/ML workshop, I now have a comprehensive understanding of both development and operations aspects of cloud applications on AWS.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"In this section, you need to summarize the contents of the workshop that you plan to conduct.\nüìÑ Download Full Proposal: Proposal Template.docx\nTravel Journal A Unified AWS Serverless Solution for Travel Journal 1. Executive Summary Travel Journal Web was created to help people preserve their life journeys ‚Äî not only the trips they take but also the memories, emotions, and stories behind each photo. The application connects people to their experiences, turning every journey into part of their own ‚Äúmemory map.‚Äù\nUsers can upload photos, add location notes, and the system automatically recognizes the scene type (beach, mountain, city, etc.) using Amazon Rekognition. Travel routes are displayed visually and in real-time on an interactive map powered by Amazon Location Service, delivering a vivid and engaging experience.\nThe platform leverages the power of AWS Serverless Architecture ‚Äî including Lambda, API Gateway, S3, DynamoDB, and Cognito ‚Äî ensuring high performance, strong security, and scalable flexibility at an optimized cost.\n2. Problem Statement What‚Äôs the Problem? Many travel enthusiasts want to document their journeys, yet existing platforms only allow posting scattered images or notes without an intuitive connection between emotions, photos, and actual locations. It becomes difficult to organize memories, view trips on a map, or analyze travel data such as destinations, durations, and experiences. Moreover, popular cloud storage solutions fail to personalize the user experience.\nThe Solution Travel Journal Web is built as a web application leveraging the AWS Serverless architecture to optimize performance and cost efficiency.\nThe system uses Amazon S3 to store images and static data, distributed globally through Amazon CloudFront. Amazon Cognito manages secure user authentication, while API Gateway and AWS Lambda handle backend logic. Uploaded images in S3 are analyzed by Amazon Rekognition, with results and location data stored in Amazon DynamoDB and visualized using Amazon Location Service.\nThe entire system is monitored by Amazon CloudWatch, tracking throughput, errors, latency, and database capacity; alerts are sent via SNS. Data and encryption keys are securely managed using AWS KMS and Secrets Manager.\nThis solution delivers a smart, secure, and cost-effective travel application, allowing users to easily capture, store, and revisit their journeys anytime, anywhere..\nBenefits and Return on Investment The solution allows users to easily record and share their journeys while establishing a data foundation for potential expansion into a social travel platform. With an estimated cost of only USD 14.55/month, the app can support 100‚Äì200 users without physical servers. The expected ROI period is 6‚Äì8 months, achieved by saving maintenance and storage costs.\n3. Solution Architecture The Travel Journal Web is built entirely on an AWS Serverless architecture, optimizing performance, security, and scalability. The static web interface is hosted on Amazon S3, distributed globally through Amazon CloudFront, and protected by AWS WAF, ACM, and Route 53. User authentication is managed by Amazon Cognito, while Amazon API Gateway and AWS Lambda handle backend business logic. Uploaded images are stored in Amazon S3 and automatically processed through Amazon SQS, AWS Lambda, Amazon Rekognition, and Amazon Location Service. The results are stored in Amazon DynamoDB and redistributed via S3. The system supports retry and DLQ mechanisms, sends notifications through Amazon SNS, provides centralized monitoring with Amazon CloudWatch and AWS X-Ray, and secures data using AWS IAM, KMS, and Secrets Manager.. The architecture is detailed below:\nAWS Services Used Amazon Route 53: Global domain management and routing. AWS Certificate Manager (ACM): Issues and manages SSL/TLS certificates for secure endpoints Amazon CloudFront: Low-latency content delivery for static and dynamic assets. AWS WAF: Protects the application from common web threats. AWS Lambda: Event-driven serverless compute for backend logic. Amazon API Gateway: Acts as the interface between the frontend and Lambda backend. Amazon S3: Stores user data, images, and activity logs Amazon DynamoDB: NoSQL database for trip records and user data, optimized for fast queries. Amazon Cognito: User authentication and authorization management. Amazon Rekognition: Image analysis and labeling. Amazon Location Service: Geolocation and map visualization. Amazon SNS: Sends notifications to users and administrators. Amazon SQS (Main Queue): Buffers image processing requests from S3 uploads before invoking Lambda. Dead Letter Queue (DLQ): Captures failed or unprocessed messages from SQS for later review and troubleshooting. Amazon CloudWatch: Logs, metrics, and performance monitoring. AWS IAM: Manages access roles and permissions for AWS services. AWS KMS: Encrypts data at rest and in transit, enhancing overall security. AWS Secrets Manager: Securely stores and encrypts confidential credentials. AWS CodeBuild: Automatically compiles, tests, and packages source code. AWS CodePipeline: Automates the entire CI/CD process ‚Äî from commit, build, and test to deployment in AWS environments. Component Design User Authentication: Managed by Amazon Cognito for login, token management, and access control. Application Logic: AWS Lambda handles travel data submissions, image uploads, and analytics requests from API Gateway. Data Management: Amazon DynamoDB stores journey details; OpenSearch provides fast indexing and search. Queue Processing: Amazon SQS (Main Queue) receives image processing requests from S3 before invoking Lambda; the Dead Letter Queue (DLQ) captures failed messages for later handling. Image Analysis: Amazon Rekognition automatically labels and classifies photo content. Maps \u0026amp; Location Data: Amazon Location Service tracks and displays GPS data on interactive maps. Content Storage \u0026amp; Delivery: Amazon S3 stores images, user data, and static assets; content is distributed globally via Amazon CloudFront (protected by AWS WAF, SSL/TLS via ACM, and routed through Route 53). Monitoring \u0026amp; Notifications: Amazon CloudWatch monitors logs and performance metrics; Amazon SNS sends alerts and notifications to users. Security \u0026amp; Access Management: AWS IAM manages service permissions, while AWS Secrets Manager and KMS protect sensitive information. Deployment \u0026amp; CI/CD: AWS CodeBuild and CodePipeline automate the build, test, and deployment processes. 4. Technical Implementation Implementation Phases The project is divided into two main areas ‚Äî web application development and AWS infrastructure integration ‚Äî across three stages:\nRequirement Analysis \u0026amp; Architecture Design: Identify suitable AWS services (CloudFront, WAF, Cognito, DynamoDB, Lambda, API Gateway, S3, etc.) and design the overall architecture (Month 1). Cost Estimation \u0026amp; System Simulation: Calculate costs using AWS Pricing Calculator, test image and location processing workflows, and optimize system resources (Month 2). Development, Testing \u0026amp; Deployment: Build the web application, integrate AWS SDKs, test WAF protection, and monitor operations using CloudWatch (Month 3). Technical Requirements\nFrontend: Built with React.js, deployed as static files on Amazon S3, and globally distributed via CloudFront for fast load times and reduced backend load. Security \u0026amp; Access: AWS WAF protects against common web attacks; AWS Certificate Manager (ACM) provides SSL/TLS certificates; Amazon Cognito manages user authentication (email, OAuth2). Backend: API Gateway routes user requests to AWS Lambda, which executes business logic such as trip logging, image uploads, and data queries. Queue Processing: Amazon SQS (Main Queue) buffers image processing requests triggered from S3, while the Dead Letter Queue (DLQ) stores failed messages for later handling. Database \u0026amp; Search: Amazon DynamoDB stores user journeys and notes Image Analysis: Amazon Rekognition detects scenes, faces, and automatically suggests image labels. Geolocation \u0026amp; Maps: Amazon Location Service visualizes GPS coordinates on an interactive map. Monitoring \u0026amp; Security: Amazon CloudWatch collects logs; AWS Secrets Manager and KMS secure sensitive information Notifications: Amazon SNS sends alerts for system errors or new user events. Deployment \u0026amp; CI/CD: AWS CodeBuild and AWS CodePipeline automate the build, test, and deployment workflow. 5. Timeline \u0026amp; Milestones Project Timeline\nInternship (Months 1-3): 3 months. Month 1: Learn AWS fundamentals. Month 2: Design architecture, estimate costs, and refine infrastructure. Month 3: Implement, test, and deploy the application. Post-Launch:Continue system research and improvements over the following year. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator\nInfrastructure Costs AWS Services: Amazon Route 53: 0,50 USD/month AWS Certificate Manager: 0,0 USD/month Amazon CloudFront: 0,61 USD/month AWS WAF: 0,6 USD/month AWS Lambda: 0,01 USD/month Amazon API Gateway: 0,45 USD/month Amazon S3: 1,47 USD/month Amazon DynamoDB: 16,35 USD/month Amazon Cognito: 5,00 USD/month Amazon Rekognition: 10,08 USD/month Amazon Location Service: 4,35 USD/month Amazon SNS: 2,58 USD/month Amazon SQS (Main Queue): 3,1 USD/month Amazon CloudWatch: 6,87 USD/month AWS IAM: 0,00 USD/month AWS Secrets Manager: 0,4 USD/month AWS KMS: 2,3 USD/month AWS CodeBuild: 0,8 USD/month AWS CodePipeline: 0,00 USD/month Total: 61.78 USD/month, or $946,56 USD/year\n7. Risk Assessment Risk Matrix Security breach or user access loss: High impact, very low probability. Cost increase due to Rekognition usage: High impact, medium probability. GPS data inconsistency: High impact, low probability Mitigation Strategies Network: Use CloudFront for global caching and stable performance across regions. Infrastructure: Leverage Lambda‚Äôs automatic retry mechanism and browser caching to minimize downtime. Security: Apply multi-layer authentication via Cognito and strict IAM/S3 permissions. Cost: Set budget alerts in AWS Budgets, and optimize Lambda/S3 usage based on access patterns. Contingency Plans Integrate SNS and CloudWatch Alerts to notify admins immediately of failures (e.g., Lambda errors, API overloads, budget overruns). Use CloudFormation for rapid infrastructure recovery Enable S3 Versioning to preserve image and log backups. 8. Expected Outcomes Technical Improvements: The app automates storage, analysis, and visualization of travel data on a map, eliminating manual note-taking.\nLong-term Value Builds a rich dataset of travel routes, photos, and emotions ‚Äî forming a foundation for future applications like personalized travel recommendations and experience analytics.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Learn about Amazon SQS (Simple Queue Service) - message queue service Practice with SQS demos to understand how it works Deploy SQS to project for asynchronous processing Test SQS integration between Lambda functions Ensure sequential image processing with SQS Attend AWS Cloud Mastery Series #1 event Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about SQS 11/10/2025 11/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn more about SQS - Test SQS demos 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ https://000083.awsstudygroup.com/vi/ 4 - Start deploying SQS to project 11/12/2025 11/12/2025 5 - Test SQS between lambda functions in project 11/13/2025 11/13/2025 6 - Test sequential image processing with SQS 11/14/2025 11/14/2025 7 - Attend AWS Cloud Mastery Series #1 event 11/15/2025 11/15/2025 https://luma.com/imkevnow?tk=v3n25y Week 10 Achievements: Mastered Amazon SQS:\nUnderstood message queue and asynchronous processing Learned differences between Standard Queue and FIFO Queue Mastered concepts: Message retention period Visibility timeout Dead Letter Queue (DLQ) Long polling vs Short polling Message deduplication Understood use cases: decoupling microservices, buffering requests, load leveling Practiced with SQS demos:\nTested creating and configuring SQS queues Sent and received messages via console and CLI Experimented with visibility timeout Tested Dead Letter Queue functionality Evaluated performance with different queue types Successfully deployed SQS to project:\nIntegrated SQS into serverless architecture Created SQS queues for asynchronous image processing Configured Lambda triggers from SQS Implemented message producers and consumers Set up IAM permissions for SQS access Integrated SQS with Lambda functions:\nTested message processing flow between Lambda functions Ensured error handling and retry logic Implemented Dead Letter Queue for failed messages Monitored queue metrics: messages in flight, age of oldest message Optimized batch size and concurrency settings Ensured sequential image processing:\nUsed FIFO queue to ensure processing order Implemented message group ID for ordering Tested with multiple images upload Verified correct processing order Handled edge cases and race conditions Attended AWS Cloud Mastery Series #1:\nLearned best practices from AWS experts Networked with AWS community Updated knowledge about new services Applied insights to project "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Attend AWS Cloud Mastery Series #2 event Learn about Amazon SNS (Simple Notification Service) and Amazon SES (Simple Email Service) Practice with SNS and SES demos Deploy SNS and SES to project for sending notifications Test email sending functionality for users Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attend AWS Cloud Mastery Series #2 event 11/17/2025 11/17/2025 https://luma.com/39t066sy?tk=eIbtFB 3 - Learn about SNS and SES 11/18/2025 11/18/2025 https://cloudjourney.awsstudygroup.com/ 4 - Demo SNS and SES 11/19/2025 11/19/2025 https://000077.awsstudygroup.com/vi/ 5 - Deploy SNS and SES to project 11/20/2025 11/20/2025 6 - Test email sending functionality for users 11/21/2025 11/21/2025 Week 11 Achievements: Attended AWS Cloud Mastery Series #2:\nLearned about advanced AWS services and architectures Networked with AWS community and experts Updated knowledge about best practices Got ideas to improve project Mastered Amazon SNS:\nUnderstood pub/sub messaging pattern Learned subscription types: Email, SMS, HTTP/HTTPS, Lambda, SQS Mastered concepts: Topics and subscriptions Message filtering Message attributes Delivery policies and retry logic Fan-out pattern with SQS Understood use cases: application alerts, push notifications, distributed systems Mastered Amazon SES:\nUnderstood email sending service Learned how to verify email addresses and domains Mastered features: Sending emails (SMTP or API) Email templates Configuration sets Bounce and complaint handling Email receiving Understood reputation management and sending limits Practiced with SNS and SES demos:\nCreated SNS topics and subscriptions Tested sending notifications through different channels Verified email addresses in SES Tested sending emails with templates Tested bounce and complaint notifications Successfully deployed SNS and SES to project:\nIntegrated SNS to send notifications for events Set up SES to send transactional emails Created email templates for use cases: Welcome emails Password reset Booking confirmations Travel recommendations Configured IAM permissions for SNS and SES Implemented error handling and retry logic Tested email sending functionality:\nTested email delivery for different scenarios Verified email formatting and content Tested bounce and complaint handling Monitored email sending metrics Ensured emails don\u0026rsquo;t get caught by spam filters Tested with multiple recipients Verified email tracking and analytics "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Comprehensive testing of website and project features Test email system for users Fix bugs that arise when running website Prepare demo for presentation Complete personal workshop Attend AWS Cloud Mastery Series #3 event Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Start testing main website and features 11/24/2025 11/30/2025 3 - Test email system for users 11/25/2025 11/25/2025 4 - Fix minor bugs when running main website 11/26/2025 12/05/2025 5 - Run website to prepare for demo recording 11/27/2025 12/08/2025 6 - Complete personal workshop 11/28/2025 12/05/2025 7 - Attend AWS Cloud Mastery Series #3 event 11/29/2025 11/29/2025 https://luma.com/0nl16e1p?tk=HjQFde Week 12 Achievements: Completed comprehensive website testing:\nTested all pages and navigation Tested responsive design on different devices Verified all links and buttons work Tested performance and loading time Tested with multiple browsers Ensured UI/UX consistency Verified security configurations Tested email system:\nTested email delivery for all scenarios Verified email templates display correctly Tested email tracking and analytics Tested with multiple email providers Ensured emails don\u0026rsquo;t get marked as spam Verified bounce and complaint handling Monitored SES metrics and quotas Fixed bugs and optimized:\nIdentified and fixed arising bugs Optimized performance bottlenecks Improved error handling Refactored code for easier maintenance Updated documentation Implemented logging and monitoring Security hardening Prepared demo presentation:\nRan website to ensure everything works Prepared demo scenarios Created test data for demo Practiced presentation flow Prepared backup plans for demo Documented key features and highlights Completed personal workshop:\nWrote workshop documentation Created step-by-step instructions Included architecture diagrams Added code samples and configurations Tested workshop with fresh AWS account Published workshop to platform Attended AWS Cloud Mastery Series #3:\nLearned advanced topics and best practices Networked with AWS community Shared experiences and lessons learned Got feedback for project Updated latest AWS knowledge Project summary:\nCompleted Travel Guide Web Application with full features Successfully integrated AWS services: S3, Lambda, Rekognition, SQS, SNS, SES, RDS Website runs stably and ready for demo Complete documentation for deployment and maintenance Gained practical experience with AWS cloud architecture "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.6-cloudfront-s3-location/5.6.3-location-service/","title":"AWS Location Service Integration","tags":[],"description":"","content":"AWS Location Service (Map/Geocoding) Components Used in the Project The project uses Place Index for geocoding, without requiring Map tiles.\nPlace Index: TravelGuidePlaceIndex (Esri)\nMain APIs:\nSearchPlaceIndexForPosition (reverse geocoding) SearchPlaceIndexForText (forward geocoding) Place Index Configuration Creating Place Index AWS Console Steps:\nNavigate to Amazon Location Service Create Place Index Configure: Name: TravelGuidePlaceIndex Data provider: Esri Pricing plan: RequestBasedUsage Intended use: SingleUse SAM Template:\nTravelGuidePlaceIndex: Type: AWS::Location::PlaceIndex Properties: IndexName: TravelGuidePlaceIndex DataSource: Esri PricingPlan: RequestBasedUsage DataSourceConfiguration: IntendedUse: SingleUse Backend Integration How Backend Calls Location Service In functions/utils/location_service.py:\nLogic flow:\nCheck DynamoDB cache If cache miss ‚Üí call AWS Location If error/no results ‚Üí fallback to Nominatim Save to cache with TTL (default 30 days) Lambda Functions Using Location Service Functions:\nCreateArticleFunction UpdateArticleFunction Environment Variables:\nEnvironment: Variables: PLACE_INDEX_NAME: !Ref TravelGuidePlaceIndex LOCATION_CACHE_TABLE: !Ref LocationCacheTable USE_AWS_LOCATION: \u0026#39;true\u0026#39; Python Code Example import boto3 import json from datetime import datetime, timedelta location_client = boto3.client(\u0026#39;location\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) cache_table = dynamodb.Table(os.environ[\u0026#39;LOCATION_CACHE_TABLE\u0026#39;]) PLACE_INDEX = os.environ[\u0026#39;PLACE_INDEX_NAME\u0026#39;] def reverse_geocode(latitude, longitude): \u0026#34;\u0026#34;\u0026#34;Convert coordinates to place name\u0026#34;\u0026#34;\u0026#34; # Generate cache key cache_key = f\u0026#34;reverse:{latitude:.6f},{longitude:.6f}\u0026#34; # Check cache try: response = cache_table.get_item(Key={\u0026#39;cacheKey\u0026#39;: cache_key}) if \u0026#39;Item\u0026#39; in response: cached_data = response[\u0026#39;Item\u0026#39;] # Check if not expired if datetime.now().timestamp() \u0026lt; cached_data[\u0026#39;expiresAt\u0026#39;]: print(f\u0026#34;Cache HIT: {cache_key}\u0026#34;) return json.loads(cached_data[\u0026#39;data\u0026#39;]) except Exception as e: print(f\u0026#34;Cache read error: {e}\u0026#34;) print(f\u0026#34;Cache MISS: {cache_key}\u0026#34;) # Call AWS Location Service try: response = location_client.search_place_index_for_position( IndexName=PLACE_INDEX, Position=[longitude, latitude], # Note: [lng, lat] MaxResults=1 ) if response[\u0026#39;Results\u0026#39;]: place = response[\u0026#39;Results\u0026#39;][0][\u0026#39;Place\u0026#39;] result = { \u0026#39;label\u0026#39;: place.get(\u0026#39;Label\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;municipality\u0026#39;: place.get(\u0026#39;Municipality\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;country\u0026#39;: place.get(\u0026#39;Country\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;region\u0026#39;: place.get(\u0026#39;Region\u0026#39;, \u0026#39;\u0026#39;) } # Save to cache save_to_cache(cache_key, result) return result except Exception as e: print(f\u0026#34;AWS Location error: {e}\u0026#34;) # Fallback to Nominatim return fallback_nominatim(latitude, longitude) return None def forward_geocode(text): \u0026#34;\u0026#34;\u0026#34;Convert place name to coordinates\u0026#34;\u0026#34;\u0026#34; cache_key = f\u0026#34;forward:{text}\u0026#34; # Check cache (similar to reverse) # ... # Call AWS Location Service try: response = location_client.search_place_index_for_text( IndexName=PLACE_INDEX, Text=text, MaxResults=5 ) results = [] for item in response[\u0026#39;Results\u0026#39;]: place = item[\u0026#39;Place\u0026#39;] results.append({ \u0026#39;label\u0026#39;: place.get(\u0026#39;Label\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;position\u0026#39;: place[\u0026#39;Geometry\u0026#39;][\u0026#39;Point\u0026#39;], # [lng, lat] \u0026#39;country\u0026#39;: place.get(\u0026#39;Country\u0026#39;, \u0026#39;\u0026#39;) }) # Save to cache save_to_cache(cache_key, results) return results except Exception as e: print(f\u0026#34;AWS Location error: {e}\u0026#34;) return [] def save_to_cache(key, data, ttl_days=30): \u0026#34;\u0026#34;\u0026#34;Save geocoding result to DynamoDB cache\u0026#34;\u0026#34;\u0026#34; try: expires_at = int((datetime.now() + timedelta(days=ttl_days)).timestamp()) cache_table.put_item( Item={ \u0026#39;cacheKey\u0026#39;: key, \u0026#39;data\u0026#39;: json.dumps(data), \u0026#39;expiresAt\u0026#39;: expires_at, \u0026#39;createdAt\u0026#39;: int(datetime.now().timestamp()) } ) print(f\u0026#34;Saved to cache: {key}\u0026#34;) except Exception as e: print(f\u0026#34;Cache write error: {e}\u0026#34;) def fallback_nominatim(latitude, longitude): \u0026#34;\u0026#34;\u0026#34;Fallback to Nominatim if AWS Location fails\u0026#34;\u0026#34;\u0026#34; import requests try: url = f\u0026#34;https://nominatim.openstreetmap.org/reverse\u0026#34; params = { \u0026#39;lat\u0026#39;: latitude, \u0026#39;lon\u0026#39;: longitude, \u0026#39;format\u0026#39;: \u0026#39;json\u0026#39; } headers = {\u0026#39;User-Agent\u0026#39;: \u0026#39;TravelGuideApp/1.0\u0026#39;} response = requests.get(url, params=params, headers=headers, timeout=5) if response.status_code == 200: data = response.json() return { \u0026#39;label\u0026#39;: data.get(\u0026#39;display_name\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;municipality\u0026#39;: data.get(\u0026#39;address\u0026#39;, {}).get(\u0026#39;city\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;country\u0026#39;: data.get(\u0026#39;address\u0026#39;, {}).get(\u0026#39;country\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;region\u0026#39;: data.get(\u0026#39;address\u0026#39;, {}).get(\u0026#39;state\u0026#39;, \u0026#39;\u0026#39;) } except Exception as e: print(f\u0026#34;Nominatim fallback error: {e}\u0026#34;) return None DynamoDB Cache for Cost Optimization LocationCacheTable Structure Table Schema:\nLocationCacheTable: Type: AWS::DynamoDB::Table Properties: TableName: LocationCache BillingMode: PAY_PER_REQUEST AttributeDefinitions: - AttributeName: cacheKey AttributeType: S KeySchema: - AttributeName: cacheKey KeyType: HASH TimeToLiveSpecification: Enabled: true AttributeName: expiresAt Attributes:\ncacheKey (String, Primary Key): reverse:lat,lng or forward:text data (String): JSON-encoded geocoding result expiresAt (Number): Unix timestamp for TTL createdAt (Number): Creation timestamp Benefits:\nReduces AWS Location Service requests Saves costs (Location Service is request-based) Faster response times Automatic cleanup via TTL IAM Permissions Lambda Execution Role Minimum IAM policy for Lambda:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;geo:SearchPlaceIndexForPosition\u0026#34;, \u0026#34;geo:SearchPlaceIndexForText\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:geo:ap-southeast-1:*:place-index/TravelGuidePlaceIndex\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-southeast-1:*:table/LocationCache\u0026#34; } ] } SAM Policy Templates:\nPolicies: - DynamoDBCrudPolicy: TableName: !Ref LocationCacheTable - Statement: - Effect: Allow Action: - geo:SearchPlaceIndexForPosition - geo:SearchPlaceIndexForText Resource: !GetAtt TravelGuidePlaceIndex.Arn Operations \u0026amp; Cost Optimization Monitoring Cache Performance Metrics to track:\nCache hit ratio Cache miss ratio AWS Location Service request count Average response time CloudWatch Custom Metrics:\nimport boto3 cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) def log_cache_metric(metric_name, value): cloudwatch.put_metric_data( Namespace=\u0026#39;TravelGuide/Location\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: metric_name, \u0026#39;Value\u0026#39;: value, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39; } ] ) # Usage log_cache_metric(\u0026#39;CacheHit\u0026#39;, 1) log_cache_metric(\u0026#39;CacheMiss\u0026#39;, 1) log_cache_metric(\u0026#39;LocationServiceCall\u0026#39;, 1) Cost Optimization Strategies Increase Cache TTL\nFor stable locations: 90 days For changing data: 7-30 days Cache Popular Queries\nPre-populate cache for common locations Reduce cold start costs Batch Requests (if applicable)\nGroup multiple geocoding requests Reduce API calls Monitor Usage\nSet CloudWatch alarms for high request counts Review monthly Location Service costs Fallback Strategy\nUse Nominatim for non-critical requests Reserve AWS Location for premium features Frontend Integration (Optional) Using AWS Location Maps If you want to use AWS Location Maps (instead of OSM/Esri public tiles), you have two approaches:\n1. API Key (Amazon Location API Key) Easy integration Suitable for public web Has quota/limits 2. Cognito + SigV4 Signed Requests More secure More complex setup Better for authenticated users Leaflet Integration Example import L from \u0026#39;leaflet\u0026#39;; import { Signer } from \u0026#39;@aws-amplify/core\u0026#39;; // With API Key const map = L.map(\u0026#39;map\u0026#39;).setView([10.8231, 106.6297], 13); L.tileLayer( \u0026#39;https://maps.geo.ap-southeast-1.amazonaws.com/maps/v0/maps/{mapName}/tiles/{z}/{x}/{y}?key={apiKey}\u0026#39;, { attribution: \u0026#39;¬© Amazon Location Service\u0026#39;, mapName: \u0026#39;TravelGuideMap\u0026#39;, apiKey: \u0026#39;YOUR_API_KEY\u0026#39; } ).addTo(map); // With Cognito (SigV4) // More complex - requires signing each tile request // See AWS documentation for full implementation Troubleshooting Common Issues 1. No Results from Location Service Causes:\nInvalid coordinates Location not in Esri database Incorrect Place Index configuration Solution:\nValidate lat/lng ranges (-90 to 90, -180 to 180) Use fallback to Nominatim Check Place Index data source 2. High Costs Causes:\nLow cache hit ratio Too many unique queries Short cache TTL Solution:\nIncrease cache TTL Pre-populate common locations Review query patterns 3. Slow Response Times Causes:\nCold Lambda start Cache miss Network latency Solution:\nUse provisioned concurrency for Lambda Pre-warm cache Implement async geocoding for non-critical paths 4. Cache Not Working Causes:\nDynamoDB permissions missing TTL not configured Cache key mismatch Solution:\nVerify IAM permissions Check DynamoDB TTL settings Debug cache key generation Best Practices Always Use Cache\nReduces costs significantly Improves performance Implement Fallback\nNominatim for reliability Handle service outages gracefully Validate Input\nCheck coordinate ranges Sanitize text queries Monitor Usage\nTrack request counts Set cost alerts Optimize TTL\nLonger for stable data Shorter for dynamic data Log Cache Performance\nHit/miss ratios Identify optimization opportunities Key Takeaways AWS Location Service provides geocoding without managing infrastructure DynamoDB cache dramatically reduces costs Fallback strategy ensures reliability Proper IAM permissions are essential Monitoring helps optimize performance and costs Cache TTL balances freshness and cost "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.7-security/5.7.3-s3-ownership-validation/","title":"S3 Ownership Validation","tags":[],"description":"","content":"S3 Ownership Validation - Prevent Unauthorized Access The Problem Before implementation:\nUsers could reference ANY S3 object in their articles No ownership validation Users could use other users\u0026rsquo; images Potential data leakage and storage abuse Attack scenario:\n1. User A uploads image: articles/user-a-id/photo.jpg 2. User B creates article with User A\u0026#39;s image key 3. User B\u0026#39;s article displays User A\u0026#39;s private photo 4. ‚ùå Unauthorized access! The Solution Implement ownership validation to ensure users can only use their own images.\nValidation checks:\nImage key format validation Article ID matching File size limits File type validation Ownership verification Implementation Updated: create_article.py Added validation logic:\nfrom security_utils import validate_image_key import boto3 s3_client = boto3.client(\u0026#39;s3\u0026#39;) BUCKET_NAME = os.environ[\u0026#39;ARTICLE_IMAGES_BUCKET\u0026#39;] def lambda_handler(event, context): data = json.loads(event[\u0026#39;body\u0026#39;]) user_id = event[\u0026#39;requestContext\u0026#39;][\u0026#39;authorizer\u0026#39;][\u0026#39;claims\u0026#39;][\u0026#39;sub\u0026#39;] article_id = str(uuid.uuid4()) # Get image keys from request image_keys = data.get(\u0026#34;imageKeys\u0026#34;, []) # ‚úÖ Validate each image for key_str in image_keys: try: # 1. Validate key format and ownership validate_image_key(key_str, article_id, user_id) # 2. Check if image exists response = s3_client.head_object( Bucket=BUCKET_NAME, Key=key_str ) # 3. Validate file size file_size = response[\u0026#39;ContentLength\u0026#39;] if file_size \u0026gt; 10 * 1024 * 1024: # 10 MB return error(400, f\u0026#34;Image {key_str} is too large (max 10MB)\u0026#34;) # 4. Validate file type content_type = response.get(\u0026#39;ContentType\u0026#39;, \u0026#39;\u0026#39;) allowed_types = [\u0026#39;image/jpeg\u0026#39;, \u0026#39;image/png\u0026#39;, \u0026#39;image/gif\u0026#39;, \u0026#39;image/webp\u0026#39;] if content_type not in allowed_types: return error(400, f\u0026#34;Invalid image type: {content_type}\u0026#34;) except s3_client.exceptions.NoSuchKey: return error(404, f\u0026#34;Image not found: {key_str}\u0026#34;) except PermissionError as e: return error(403, str(e)) except Exception as e: return error(500, f\u0026#34;Error validating image: {str(e)}\u0026#34;) # All images validated - proceed with article creation # ... Validation Functions 1. Key Format Validation Function: validate_image_key(key, article_id, owner_id)\ndef validate_image_key(key: str, article_id: str, owner_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Validate that image key belongs to the article and user Expected format: articles/{article_id}/raw/{filename} or: articles/{article_id}/thumbnails/{filename} \u0026#34;\u0026#34;\u0026#34; # Basic S3 key validation validate_s3_key(key) # Check if key starts with correct article path expected_prefix = f\u0026#34;articles/{article_id}/\u0026#34; if not key.startswith(expected_prefix): raise PermissionError( f\u0026#34;Image does not belong to this article. \u0026#34; f\u0026#34;Expected prefix: {expected_prefix}\u0026#34; ) # Additional checks for path traversal if \u0026#39;..\u0026#39; in key or \u0026#39;//\u0026#39; in key: raise PermissionError(\u0026#34;Invalid image path detected\u0026#34;) return True 2. File Size Validation def validate_file_size(file_size: int, max_size: int = 10 * 1024 * 1024) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Validate file size (default max: 10 MB)\u0026#34;\u0026#34;\u0026#34; if file_size \u0026gt; max_size: raise ValueError(f\u0026#34;File too large: {file_size} bytes (max: {max_size})\u0026#34;) if file_size == 0: raise ValueError(\u0026#34;File is empty\u0026#34;) return True 3. File Type Validation def validate_file_type(content_type: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Validate file MIME type\u0026#34;\u0026#34;\u0026#34; allowed_types = [ \u0026#39;image/jpeg\u0026#39;, \u0026#39;image/jpg\u0026#39;, \u0026#39;image/png\u0026#39;, \u0026#39;image/gif\u0026#39;, \u0026#39;image/webp\u0026#39; ] if content_type not in allowed_types: raise ValueError( f\u0026#34;Invalid file type: {content_type}. \u0026#34; f\u0026#34;Allowed: {\u0026#39;, \u0026#39;.join(allowed_types)}\u0026#34; ) return True S3 Object Metadata Storing Ownership Information When uploading images, store metadata:\ndef upload_image_with_metadata(file, article_id, user_id): \u0026#34;\u0026#34;\u0026#34;Upload image with ownership metadata\u0026#34;\u0026#34;\u0026#34; s3_client.put_object( Bucket=BUCKET_NAME, Key=f\u0026#34;articles/{article_id}/raw/{filename}\u0026#34;, Body=file, ContentType=content_type, Metadata={ \u0026#39;article-id\u0026#39;: article_id, \u0026#39;owner-id\u0026#39;: user_id, \u0026#39;uploaded-at\u0026#39;: datetime.now().isoformat() } ) Verifying Metadata def verify_image_ownership(key: str, user_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Verify image ownership using S3 metadata\u0026#34;\u0026#34;\u0026#34; try: response = s3_client.head_object( Bucket=BUCKET_NAME, Key=key ) metadata = response.get(\u0026#39;Metadata\u0026#39;, {}) owner = metadata.get(\u0026#39;owner-id\u0026#39;) if owner and owner != user_id: raise PermissionError(\u0026#34;You don\u0026#39;t own this image\u0026#34;) return True except s3_client.exceptions.NoSuchKey: raise FileNotFoundError(f\u0026#34;Image not found: {key}\u0026#34;) Testing Test 1: Valid Image (Own Article) # Upload image first curl -X POST https://api.example.com/upload-url \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; # Response: { \u0026#34;uploadUrl\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;articles/abc123/raw/image.jpg\u0026#34;, \u0026#34;articleId\u0026#34;: \u0026#34;abc123\u0026#34; } # Upload image to S3 curl -X PUT \u0026#34;$uploadUrl\u0026#34; \\ --upload-file image.jpg # Create article with own image curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;My Article\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Content\u0026#34;, \u0026#34;latitude\u0026#34;: 10, \u0026#34;longitude\u0026#34;: 106, \u0026#34;imageKeys\u0026#34;: [\u0026#34;articles/abc123/raw/image.jpg\u0026#34;] }\u0026#39; # Expected: 200 OK - Article created Test 2: Invalid Image (Other User\u0026rsquo;s Image) # Try to use another user\u0026#39;s image curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Stolen Image\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Content\u0026#34;, \u0026#34;latitude\u0026#34;: 10, \u0026#34;longitude\u0026#34;: 106, \u0026#34;imageKeys\u0026#34;: [\u0026#34;articles/other-user-id/raw/image.jpg\u0026#34;] }\u0026#39; # Expected: 403 Forbidden # Error: \u0026#34;Image does not belong to this article\u0026#34; Test 3: File Too Large # Try to upload 20MB image curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;Large Image\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Content\u0026#34;, \u0026#34;latitude\u0026#34;: 10, \u0026#34;longitude\u0026#34;: 106, \u0026#34;imageKeys\u0026#34;: [\u0026#34;articles/abc123/raw/large-image.jpg\u0026#34;] }\u0026#39; # Expected: 400 Bad Request # Error: \u0026#34;Image is too large (max 10MB)\u0026#34; Test 4: Invalid File Type # Try to use PDF file curl -X POST https://api.example.com/articles \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -d \u0026#39;{ \u0026#34;title\u0026#34;: \u0026#34;PDF File\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Content\u0026#34;, \u0026#34;latitude\u0026#34;: 10, \u0026#34;longitude\u0026#34;: 106, \u0026#34;imageKeys\u0026#34;: [\u0026#34;articles/abc123/raw/document.pdf\u0026#34;] }\u0026#39; # Expected: 400 Bad Request # Error: \u0026#34;Invalid image type: application/pdf\u0026#34; Security Benefits 1. Prevent Unauthorized Access ‚úÖ Before: Users could reference any S3 object ‚ùå After: Users can only use their own images\n2. Prevent Data Leakage ‚úÖ Before: Private images could be exposed ‚ùå After: Ownership validation prevents leakage\n3. Prevent Storage Abuse ‚úÖ Before: Users could link to unlimited images ‚ùå After: File size limits prevent abuse\n4. Prevent Malware Uploads ‚úÖ Before: Any file type accepted ‚ùå After: Only image types allowed\nBest Practices 1. Validate at Multiple Layers ‚úÖ Do: Validate in Lambda AND S3 bucket policy\nDefense in depth Multiple checkpoints 2. Use S3 Metadata ‚úÖ Do: Store ownership in metadata\nEasy to verify Immutable after upload 3. Implement Rate Limiting ‚úÖ Do: Limit upload frequency\nPrevent abuse Reduce costs # Example rate limiting from datetime import datetime, timedelta def check_rate_limit(user_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check if user exceeded upload rate limit\u0026#34;\u0026#34;\u0026#34; key = f\u0026#34;rate_limit:upload:{user_id}\u0026#34; # Get current count from cache count = cache.get(key) or 0 # Limit: 10 uploads per hour if count \u0026gt;= 10: raise PermissionError(\u0026#34;Upload rate limit exceeded\u0026#34;) # Increment counter cache.set(key, count + 1, ex=3600) # 1 hour TTL return True 4. Log All Validation Failures ‚úÖ Do: Log suspicious activity\nTrack attack attempts Identify patterns Alert on anomalies import logging logger = logging.getLogger() def log_validation_failure(user_id: str, key: str, reason: str): \u0026#34;\u0026#34;\u0026#34;Log validation failure for security monitoring\u0026#34;\u0026#34;\u0026#34; logger.warning( f\u0026#34;Validation failed - User: {user_id}, Key: {key}, Reason: {reason}\u0026#34;, extra={ \u0026#39;user_id\u0026#39;: user_id, \u0026#39;image_key\u0026#39;: key, \u0026#39;failure_reason\u0026#39;: reason, \u0026#39;timestamp\u0026#39;: datetime.now().isoformat() } ) Monitoring CloudWatch Metrics Track:\nValidation failures per user Invalid image attempts File size violations File type violations Create alarms:\n# Alert on high validation failure rate aws cloudwatch put-metric-alarm \\ --alarm-name high-validation-failures \\ --metric-name ValidationFailures \\ --namespace TravelGuide/Security \\ --statistic Sum \\ --period 300 \\ --threshold 10 \\ --comparison-operator GreaterThanThreshold CloudTrail Logging Monitor S3 access:\nTrack who accessed what Identify unauthorized attempts Audit trail for compliance Cost Impact Storage Costs Before:\nUsers could link unlimited images No size limits Potential abuse After:\n10 MB per image limit Controlled storage growth Predictable costs API Costs Additional S3 API calls:\nhead_object per image validation ~$0.0004 per 1,000 requests Negligible cost increase Key Takeaways Ownership validation prevents unauthorized image access File size limits prevent storage abuse File type validation prevents malware uploads S3 metadata stores ownership information Multiple validation layers provide defense in depth Logging and monitoring detect attack attempts "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.3-backend-articles/","title":"Backend - Article Service","tags":[],"description":"","content":"Backend Architecture Overview This section covers the complete backend implementation for the Article Management System. The backend is built using AWS serverless services including Lambda, DynamoDB, S3, Cognito, and AWS Location Service.\nThe architecture supports:\nFull CRUD operations for articles Image upload and management User favorites and interactions Gallery and trending features Location-based services with caching User profile management Content Lambda Service Backend DynamoDB Tables "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.2-iac-multistack/5.2.3-cross-stack-references/","title":"Cross-Stack References","tags":[],"description":"","content":"Cross-Stack References Implementation Overview Cross-stack references allow one CloudFormation stack to use resources created by another stack. This is essential for the multi-stack pattern where service stacks need to reference resources from the core stack.\nCloudFormation Exports Mechanism The exporting stack (Core Stack) exposes resources using Outputs with Export:\n# Core Stack - template.yaml Outputs: ArticlesTableName: Description: Articles DynamoDB Table Name Value: !Ref ArticlesTable Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableName\u0026#39; ArticlesTableArn: Description: Articles DynamoDB Table ARN Value: !GetAtt ArticlesTable.Arn Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableArn\u0026#39; ImagesBucketName: Description: S3 Bucket for Images Value: !Ref ImagesBucket Export: Name: !Sub \u0026#39;${AWS::StackName}-ImagesBucketName\u0026#39; UserPoolId: Description: Cognito User Pool ID Value: !Ref UserPool Export: Name: !Sub \u0026#39;${AWS::StackName}-UserPoolId\u0026#39; CloudFormation Imports Mechanism The importing stack (Service Stack) references exported values using !ImportValue:\n# Article Service Stack - template.yaml Parameters: CoreStackName: Type: String Description: Name of the core stack to import values from Default: travel-guide-core-staging Resources: CreateArticleFunction: Type: AWS::Serverless::Function Properties: Environment: Variables: ARTICLES_TABLE: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ArticlesTableName\u0026#39; IMAGES_BUCKET: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ImagesBucketName\u0026#39; USER_POOL_ID: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-UserPoolId\u0026#39; Policies: - DynamoDBCrudPolicy: TableName: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ArticlesTableName\u0026#39; - S3CrudPolicy: BucketName: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ImagesBucketName\u0026#39; Export Naming Conventions Best Practice: Include stack name in export name to avoid conflicts\n‚ùå Bad (can conflict across environments):\nExport: Name: ArticlesTableName # Conflicts if multiple environments ‚úÖ Good (unique per environment):\nExport: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableName\u0026#39; # Results in: travel-guide-core-staging-ArticlesTableName Naming Pattern:\n{StackName}-{ResourceType}{ResourceName} Examples: - travel-guide-core-staging-ArticlesTableName - travel-guide-core-staging-ArticlesTableArn - travel-guide-core-prod-ImagesBucketName Complete Example Core Stack Exports AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: AWS::Serverless-2016-10-31 Description: Travel Guide - Core Infrastructure Resources: ArticlesTable: Type: AWS::DynamoDB::Table Properties: TableName: !Sub \u0026#39;${AWS::StackName}-articles\u0026#39; BillingMode: PAY_PER_REQUEST AttributeDefinitions: - AttributeName: articleId AttributeType: S KeySchema: - AttributeName: articleId KeyType: HASH ImagesBucket: Type: AWS::S3::Bucket Properties: BucketName: !Sub \u0026#39;${AWS::StackName}-images\u0026#39; CorsConfiguration: CorsRules: - AllowedOrigins: [\u0026#39;*\u0026#39;] AllowedMethods: [GET, PUT, POST] AllowedHeaders: [\u0026#39;*\u0026#39;] Outputs: # Table Name Export ArticlesTableName: Description: Articles Table Name Value: !Ref ArticlesTable Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableName\u0026#39; # Table ARN Export ArticlesTableArn: Description: Articles Table ARN Value: !GetAtt ArticlesTable.Arn Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableArn\u0026#39; # Bucket Name Export ImagesBucketName: Description: Images Bucket Name Value: !Ref ImagesBucket Export: Name: !Sub \u0026#39;${AWS::StackName}-ImagesBucketName\u0026#39; # Bucket ARN Export ImagesBucketArn: Description: Images Bucket ARN Value: !GetAtt ImagesBucket.Arn Export: Name: !Sub \u0026#39;${AWS::StackName}-ImagesBucketArn\u0026#39; Service Stack Imports AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: AWS::Serverless-2016-10-31 Description: Travel Guide - Article Service Parameters: CoreStackName: Type: String Description: Core stack name Default: travel-guide-core-staging Resources: # Lambda Function using imported values CreateArticleFunction: Type: AWS::Serverless::Function Properties: CodeUri: ./src Handler: create_article.handler Runtime: python3.11 Environment: Variables: # Import table name ARTICLES_TABLE: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ArticlesTableName\u0026#39; # Import bucket name IMAGES_BUCKET: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ImagesBucketName\u0026#39; Policies: # Grant DynamoDB access using imported ARN - Statement: - Effect: Allow Action: - dynamodb:PutItem - dynamodb:GetItem - dynamodb:UpdateItem Resource: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ArticlesTableArn\u0026#39; # Grant S3 access using imported ARN - Statement: - Effect: Allow Action: - s3:PutObject - s3:GetObject Resource: - !Sub - \u0026#39;${BucketArn}/*\u0026#39; - BucketArn: !ImportValue \u0026#39;Fn::Sub\u0026#39;: \u0026#39;${CoreStackName}-ImagesBucketArn\u0026#39; Limitations and Workarounds Limitation 1: Cannot Delete Exporting Stack Problem: Cannot delete core stack while service stacks are importing its values.\nError:\nExport travel-guide-core-staging-ArticlesTableName cannot be deleted as it is in use by travel-guide-article-service-staging Workaround:\nDelete all importing stacks first Then delete the exporting stack # Delete service stacks first aws cloudformation delete-stack --stack-name travel-guide-article-service-staging aws cloudformation delete-stack --stack-name travel-guide-media-service-staging # Wait for deletion aws cloudformation wait stack-delete-complete --stack-name travel-guide-article-service-staging # Now delete core stack aws cloudformation delete-stack --stack-name travel-guide-core-staging Limitation 2: Cannot Change Export Name Problem: Cannot rename or delete an export while it\u0026rsquo;s being imported.\nWorkaround:\nCreate new export with new name Update all importing stacks to use new export Delete old export # Step 1: Add new export (keep old one) Outputs: ArticlesTableName: # Old export (keep for now) Value: !Ref ArticlesTable Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTableName\u0026#39; ArticlesTableNameV2: # New export Value: !Ref ArticlesTable Export: Name: !Sub \u0026#39;${AWS::StackName}-ArticlesTable-Name\u0026#39; # Step 2: Update service stacks to use new export # Step 3: Remove old export from core stack Limitation 3: Cross-Region Not Supported Problem: Cannot import values from stacks in different regions.\nWorkaround: Use SSM Parameter Store for cross-region sharing:\n# Core Stack (us-east-1) Resources: TableNameParameter: Type: AWS::SSM::Parameter Properties: Name: /travelguide/core/articles-table-name Type: String Value: !Ref ArticlesTable # Service Stack (us-west-2) Parameters: ArticlesTableName: Type: AWS::SSM::Parameter::Value\u0026lt;String\u0026gt; Default: /travelguide/core/articles-table-name Best Practices Always include stack name in export names\nExport: Name: !Sub \u0026#39;${AWS::StackName}-ResourceName\u0026#39; Export both name and ARN for resources\nOutputs: TableName: Value: !Ref Table Export: Name: !Sub \u0026#39;${AWS::StackName}-TableName\u0026#39; TableArn: Value: !GetAtt Table.Arn Export: Name: !Sub \u0026#39;${AWS::StackName}-TableArn\u0026#39; Document exports in README\n## Core Stack Exports - `{StackName}-ArticlesTableName`: DynamoDB table name - `{StackName}-ArticlesTableArn`: DynamoDB table ARN - `{StackName}-ImagesBucketName`: S3 bucket name Use parameters for core stack name\nParameters: CoreStackName: Type: String Default: travel-guide-core-staging Plan exports carefully - they\u0026rsquo;re hard to change later\nViewing Exports AWS Console:\nCloudFormation ‚Üí Stacks ‚Üí Select Stack ‚Üí Outputs tab AWS CLI:\n# List all exports aws cloudformation list-exports # List exports from specific stack aws cloudformation describe-stacks \\ --stack-name travel-guide-core-staging \\ --query \u0026#39;Stacks[0].Outputs\u0026#39; Key Takeaways Exports allow sharing resources between stacks Imports reference exported values using !ImportValue Naming convention prevents conflicts: {StackName}-{ResourceName} Limitations exist: can\u0026rsquo;t delete exporting stack, can\u0026rsquo;t change export names Workarounds available for most limitations Documentation is crucial for team understanding "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-image-processing/5.4.3-sqs-pipeline/","title":"SQS Pipeline","tags":[],"description":"","content":"Purpose Process images asynchronously to:\nReduce system load when multiple users upload simultaneously Ensure no message loss when Lambda encounters errors Separate processing steps (moderation ‚Üí detect labels) SQS Pipeline Architecture S3 Upload Event ‚Üì SQS: ModerationQueue ‚Üì Lambda: Content Moderation ‚Üì (if approved) SQS: DetectLabelsQueue ‚Üì Lambda: Detect Labels ‚Üì DynamoDB + Gallery Demo: SQS Queues Overview The system uses multiple SQS queues to process images in a pipeline:\nBenefits of SQS 1. Asynchronous Processing Lambda doesn\u0026rsquo;t need to wait for completion Users upload images quickly Processing happens in background 2. Automatic Retry If Lambda fails, message returns to queue Automatic retry with backoff Dead Letter Queue (DLQ) for repeatedly failed messages 3. Auto Scaling SQS automatically scales with message volume Lambda triggers in parallel for multiple messages No system overload concerns 4. Step Separation Content Moderation and Detect Labels are independent Easy to debug and monitor each step Simple to add new processing steps Queue Configuration ModerationQueue Visibility Timeout: 300s (5 minutes) Message Retention: 4 days DLQ: ModerationDLQ (after 3 retries) DetectLabelsQueue Visibility Timeout: 180s (3 minutes) Message Retention: 4 days DLQ: DetectLabelsDLQ (after 3 retries) Monitoring Track important metrics:\nApproximateNumberOfMessagesVisible - Messages waiting ApproximateAgeOfOldestMessage - Oldest message age NumberOfMessagesReceived - Total messages received NumberOfMessagesSent - Total messages sent Conclusion SQS Pipeline enables the system to:\nProcess reliably and stably Scale automatically based on demand Easy to maintain and extend "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"AWS Well-Architected Security Pillar Workshop Date: Saturday, November 29, 2025\nTime: 8:30 AM - 12:00 PM Location: AWS Vietnam Office\nSpeaker Lineup Le Vu Xuan An AWS Cloud Club Captain (HCMU/TE)\nCaptain of AWS Cloud Club at Ho Chi Minh City University of Technology, specializing in cloud architecture and security.\nTran Duc Anh AWS Cloud Club Captain (SGU)\nCaptain of AWS Cloud Club at Saigon University, with experience in AWS security services.\nTran Doan Cong Ly AWS Cloud Club Captain (PTIT)\nCaptain of AWS Cloud Club at Posts and Telecommunications Institute of Technology, specializing in infrastructure security.\nDanh Hoang Hieu Nghi AWS Cloud Club Captain (HUFLIT)\nCaptain of AWS Cloud Club at HCMC University of Foreign Languages - Information Technology, with expertise in cloud security practices.\nNguyen Tuan Thinh Cloud Engineer Trainee\nCloud Engineer in training, sharing hands-on experience with AWS security services.\nNguyen Do Thanh Cloud Engineer\nCloud Engineer with experience in deploying and operating secure cloud infrastructure.\nThinh Lam FCJer\nFirst Cloud Journey member, sharing learning experiences and security best practices application.\nViet Nguyen FCJer\nFirst Cloud Journey member with experience in AWS security implementation.\nMendel Grabski (Long) ex Head of Security \u0026amp; DevOps - Cloud Security Solution Architect\nSecurity expert with experience leading Security \u0026amp; DevOps teams, currently serving as Cloud Security Solution Architect.\nQuang Tinh Truong AWS Community Builder - Platform Engineer at TymeX\nAWS Community Builder and Platform Engineer at TymeX, specializing in security architecture and platform engineering.\nEvent Purpose The \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; workshop was organized with the following objectives:\nProvide comprehensive knowledge about 5 AWS Security pillars Guide best practices for Identity \u0026amp; Access Management Introduce detection and continuous monitoring strategies Practice infrastructure and data protection Build incident response playbooks Share common pitfalls and Vietnamese enterprise realities Provide learning path guidance for AWS Security Specialty Highlights Pillar 1 - Identity \u0026amp; Access Management Modern IAM Architecture IAM Fundamentals:\nUsers, Roles, Policies Avoiding long-term credentials Principle of least privilege IAM Identity Center:\nSingle Sign-On (SSO) Permission sets Centralized access management Multi-Account Security:\nService Control Policies (SCP) Permission boundaries Cross-account access patterns Security Best Practices:\nMulti-Factor Authentication (MFA) Credential rotation AWS IAM Access Analyzer Mini Demo: Validate IAM Policy + simulate access\nPillar 2 - Detection Detection \u0026amp; Continuous Monitoring AWS Security Services:\nCloudTrail: Organization-level logging GuardDuty: Threat detection Security Hub: Centralized security findings Comprehensive Logging:\nVPC Flow Logs Application Load Balancer logs S3 access logs CloudWatch Logs Alerting \u0026amp; Automation:\nEventBridge rules Automated responses Security incident notifications Detection-as-Code:\nInfrastructure monitoring rules Automated compliance checks Pillar 3 - Infrastructure Protection Network \u0026amp; Workload Security VPC Security:\nNetwork segmentation Private vs public subnet placement VPC endpoints Security Controls:\nSecurity Groups: Stateful firewall Network ACLs: Stateless firewall Application patterns and best practices Advanced Protection:\nAWS WAF: Web Application Firewall AWS Shield: DDoS protection Network Firewall: Advanced filtering Workload Protection:\nEC2 security basics ECS/EKS security configurations Container security Pillar 4 - Data Protection Encryption, Keys \u0026amp; Secrets AWS KMS (Key Management Service):\nKey policies Grants management Automatic key rotation Encryption Strategies:\nAt-rest encryption: Amazon S3 Amazon EBS Amazon RDS DynamoDB In-transit encryption: TLS/SSL VPN connections Secrets Management:\nAWS Secrets Manager: Automatic rotation Systems Manager Parameter Store: Configuration management Rotation patterns and best practices Data Governance:\nData classification Access guardrails Compliance requirements Pillar 5 - Incident Response IR Playbook \u0026amp; Automation Incident Response Lifecycle:\nPreparation Detection \u0026amp; Analysis Containment Eradication Recovery Post-Incident Activity Practical Playbooks:\nCompromised IAM Key:\nDetection methods Immediate actions Key rotation procedures S3 Public Exposure:\nIdentifying exposed buckets Remediation steps Prevention measures EC2 Malware Detection:\nDetection via GuardDuty Instance isolation Forensics collection Forensics \u0026amp; Evidence:\nSnapshot creation Instance isolation Evidence collection procedures Chain of custody Automated Response:\nLambda functions for auto-remediation Step Functions workflows EventBridge integration Wrap-Up \u0026amp; Q\u0026amp;A 5 Pillars Summary:\nIdentity \u0026amp; Access Management Detection Infrastructure Protection Data Protection Incident Response Common Pitfalls:\nOverly permissive IAM policies Insufficient logging Lack of encryption No incident response plan Vietnamese Enterprise Reality:\nCompliance requirements Budget constraints Skills gap challenges Security Learning Roadmap:\nAWS Certified Security - Specialty AWS Certified Solutions Architect - Professional Hands-on practice resources Knowledge Gained 1. Identity \u0026amp; Access Management Modern IAM Practices:\nDeep understanding of IAM roles vs users Avoiding long-term credentials Implementing least privilege principle IAM Identity Center:\nCentralized SSO solution Permission sets for multi-account Corporate directory integration Advanced IAM:\nService Control Policies (SCP) for organization-wide controls Permission boundaries to limit maximum permissions IAM Access Analyzer to identify unintended access Security Hardening:\nMFA enforcement strategies Credential rotation automation Session policies 2. Detection \u0026amp; Monitoring Security Services Integration:\nCloudTrail: Audit trail for all API calls GuardDuty: ML-based threat detection Security Hub: Aggregated security findings Comprehensive Logging Strategy:\nVPC Flow Logs for network traffic analysis Application logs (ALB, CloudFront) S3 access logs Database audit logs Automated Detection:\nEventBridge rules for real-time alerting Detection-as-Code approach Custom detection rules Monitoring Best Practices:\nLog retention policies Log analysis with CloudWatch Insights Security metrics and KPIs 3. Infrastructure Protection Network Security:\nVPC segmentation strategies Public vs private subnet design VPC endpoints for private connectivity Layered Security:\nSecurity Groups: Instance-level protection NACLs: Subnet-level protection When to use which tool Advanced Protection Services:\nAWS WAF: Protection against web exploits AWS Shield: DDoS mitigation Network Firewall: Stateful inspection Workload Security:\nEC2 security best practices Container security (ECS/EKS) Patch management 4. Data Protection Encryption Fundamentals:\nEncryption at-rest for all data stores Encryption in-transit with TLS Key management best practices AWS KMS:\nCustomer managed keys vs AWS managed keys Key policies and grants Automatic rotation Multi-region keys Secrets Management:\nSecrets Manager for database credentials Parameter Store for configuration Automatic rotation patterns Data Classification:\nSensitive data identification Access controls based on classification Compliance requirements (GDPR, PCI-DSS) 5. Incident Response IR Framework:\nAWS incident response lifecycle Preparation importance Playbook development Practical Playbooks:\nCompromised IAM Key: Detection, rotation, impact assessment S3 Public Exposure: Identification, remediation, prevention EC2 Malware: Isolation, forensics, cleanup Forensics:\nEvidence collection procedures Snapshots and memory dumps Chain of custody maintenance Automation:\nLambda-based auto-remediation Step Functions workflows Integration with ticketing systems Personal Experience Overall Impression The Security Pillar workshop was a morning session packed with valuable content. Unlike the previous two workshops (AI/ML and DevOps), this one focused on security - a critical aspect that\u0026rsquo;s often overlooked.\nFormat and Structure Half-day format:\nConcentrated content in 3.5 hours Fast-paced but well-structured Each pillar covered systematically 5 Pillars Approach:\nLogical flow from IAM ‚Üí Detection ‚Üí Infrastructure ‚Üí Data ‚Üí IR Each pillar builds on previous ones Comprehensive coverage of security landscape Pillar 1: IAM - Foundation of Security Key Takeaways:\nIAM is the foundation of AWS security Roles over users - clear best practice IAM Identity Center is a game-changer for multi-account Mini Demo:\nIAM Policy Simulator very useful Validate permissions before deployment Troubleshooting access issues Practical Insights:\nLong-term credentials = security risk Permission boundaries prevent privilege escalation Access Analyzer identifies unintended access Pillar 2: Detection - Know What\u0026rsquo;s Happening Security Services:\nCloudTrail: Must-have for auditing GuardDuty: Impressive ML-based threat detection Security Hub: Valuable centralized view Logging Strategy:\nLog everything, analyze selectively VPC Flow Logs for network forensics Application logs for business logic Automation:\nEventBridge for real-time response Interesting Detection-as-Code approach Automated alerting reduces response time Pillar 3: Infrastructure Protection - Defense in Depth Network Security:\nVPC segmentation critical Private subnets by default VPC endpoints reduce internet exposure Security Groups vs NACLs:\nSecurity Groups: Stateful, preferred NACLs: Stateless, subnet-level Layered approach best Advanced Services:\nWAF: Protect against OWASP Top 10 Shield: DDoS protection included Network Firewall: Advanced filtering Pillar 4: Data Protection - Protect What Matters Encryption:\nEncrypt everything at-rest TLS for all in-transit No excuses with AWS KMS KMS Deep Dive:\nKey policies powerful but complex Automatic rotation recommended Multi-region keys for DR Secrets Management:\nSecrets Manager for sensitive data Automatic rotation critical Parameter Store for non-sensitive config Data Classification:\nKnow your data Apply appropriate controls Compliance-driven approach Pillar 5: Incident Response - Be Prepared IR Playbooks:\nPreparation prevents panic Documented procedures essential Practice makes perfect Practical Scenarios:\nCompromised IAM Key:\nClear detection methods Step-by-step remediation Prevention measures S3 Public Exposure:\nCommon mistake Quick remediation Preventive controls EC2 Malware:\nGuardDuty detection Isolation procedures Forensics collection Automation:\nPowerful Lambda auto-remediation Step Functions for complex workflows Significantly reduce response time Common Pitfalls Discussion Overly Permissive Policies:\n\u0026ldquo;*\u0026rdquo; in policies dangerous Least privilege hard but necessary Regular policy reviews Insufficient Logging:\nCan\u0026rsquo;t detect what you don\u0026rsquo;t log Storage costs vs security value Log retention policies No IR Plan:\nPanic during incidents Slow response time Lack of documentation Vietnamese Enterprise Reality Challenges:\nBudget constraints for security tools Skills gap in security Growing compliance requirements Opportunities:\nAWS Free Tier for many security services Available training resources Community support Recommendations:\nStart with IAM and logging Incremental security improvements Leverage AWS managed services Comparison with Previous Workshops AI/ML Workshop:\nCutting-edge technology Innovation-focused Future-oriented DevOps Workshop:\nOperational excellence Automation-focused Efficiency-driven Security Workshop:\nRisk mitigation Protection-focused Compliance-driven Integration:\nSecurity enables innovation DevOps needs security All three pillars essential Challenges Many services and concepts in short time Security complexity initially overwhelming Limited hands-on practice due to time constraint Need more real-world scenarios Highlights Comprehensive coverage of security pillars Immediately applicable practical playbooks Valuable real-world pitfalls discussion Clear learning roadmap provided Focus on Vietnamese enterprise context Next Steps Immediate Actions:\nReview IAM policies in projects Enable CloudTrail and GuardDuty Implement encryption at-rest Document IR procedures Short-term Goals:\nComplete security audit of current infrastructure Implement automated detection rules Practice IR playbooks Enable Security Hub Long-term Goals:\nAWS Certified Security - Specialty Build comprehensive security framework Contribute to security culture Mentor others on AWS security Conclusion The \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; workshop provided a comprehensive framework for AWS security. Covering 5 critical pillars - IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response - the workshop equipped participants with the knowledge and tools necessary to build secure applications on AWS.\nSecurity is not an afterthought but a fundamental requirement. This workshop emphasized the importance of a security-first mindset and provided practical guidance for implementing security best practices.\nCombined with knowledge from the AI/ML and DevOps workshops, I now have a holistic understanding of the AWS platform:\nAI/ML: Innovation and intelligent applications DevOps: Efficient delivery and operations Security: Protection and compliance These three workshops together form a complete picture of modern cloud engineering on AWS. The Security workshop was particularly valuable as it ensures that innovation and efficiency don\u0026rsquo;t compromise security and compliance.\nThe half-day format, though short, was content-dense and immediately applicable. This is an essential foundation for any AWS professional, and I highly recommend it for anyone working with AWS infrastructure.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.2-iac-multistack/5.2.4-deployment-orchestration/","title":"Deployment Orchestration","tags":[],"description":"","content":"Deployment Orchestration with Bash Scripts Overview Deployment orchestration automates the process of deploying multiple CloudFormation stacks in the correct order with proper error handling and validation.\nDeployment Scripts Architecture scripts/ ‚îú‚îÄ‚îÄ deploy.sh # Main orchestration script ‚îú‚îÄ‚îÄ deploy-core.sh # Core stack deployment ‚îú‚îÄ‚îÄ deploy-service.sh # Service stack deployment template ‚îî‚îÄ‚îÄ utils.sh # Shared utility functions Main Orchestration Script deploy.sh - Orchestrates the entire deployment:\n#!/bin/bash set -e # Exit on error set -o pipefail # Catch errors in pipes # Configuration ENVIRONMENT=${1:-staging} REGION=${2:-us-east-1} CORE_STACK_NAME=\u0026#34;travel-guide-core-${ENVIRONMENT}\u0026#34; # Colors for output RED=\u0026#39;\\033[0;31m\u0026#39; GREEN=\u0026#39;\\033[0;32m\u0026#39; YELLOW=\u0026#39;\\033[1;33m\u0026#39; NC=\u0026#39;\\033[0m\u0026#39; # No Color # Error handling trap \u0026#39;echo -e \u0026#34;${RED}‚ùå Deployment failed${NC}\u0026#34;; exit 1\u0026#39; ERR echo -e \u0026#34;${GREEN}üöÄ Starting deployment to ${ENVIRONMENT}${NC}\u0026#34; # Step 1: Setup deployment bucket echo \u0026#34;üì¶ Setting up deployment bucket...\u0026#34; ./scripts/setup-bucket.sh $ENVIRONMENT $REGION # Step 2: Deploy Core Stack echo \u0026#34;üèóÔ∏è Deploying Core Stack...\u0026#34; ./scripts/deploy-core.sh $ENVIRONMENT $REGION # Wait for core stack to complete echo \u0026#34;‚è≥ Waiting for Core Stack...\u0026#34; aws cloudformation wait stack-create-complete \\ --stack-name $CORE_STACK_NAME \\ --region $REGION || \\ aws cloudformation wait stack-update-complete \\ --stack-name $CORE_STACK_NAME \\ --region $REGION # Verify core stack status CORE_STATUS=$(aws cloudformation describe-stacks \\ --stack-name $CORE_STACK_NAME \\ --region $REGION \\ --query \u0026#39;Stacks[0].StackStatus\u0026#39; \\ --output text) if [[ ! $CORE_STATUS =~ (CREATE_COMPLETE|UPDATE_COMPLETE) ]]; then echo -e \u0026#34;${RED}‚ùå Core stack deployment failed: $CORE_STATUS${NC}\u0026#34; exit 1 fi echo -e \u0026#34;${GREEN}‚úÖ Core Stack deployed successfully${NC}\u0026#34; # Step 3: Deploy Service Stacks SERVICES=(\u0026#34;auth\u0026#34; \u0026#34;articles\u0026#34; \u0026#34;media\u0026#34; \u0026#34;ai\u0026#34; \u0026#34;gallery\u0026#34; \u0026#34;notification\u0026#34;) for service in \u0026#34;${SERVICES[@]}\u0026#34;; do echo \u0026#34;üîß Deploying ${service} service...\u0026#34; ./scripts/deploy-service.sh $service $ENVIRONMENT $REGION # Optional: Wait for each service (sequential) # Or deploy all in parallel and wait at the end done echo -e \u0026#34;${GREEN}‚úÖ All services deployed successfully${NC}\u0026#34; echo -e \u0026#34;${YELLOW}üìä Deployment Summary:${NC}\u0026#34; echo \u0026#34; Environment: $ENVIRONMENT\u0026#34; echo \u0026#34; Region: $REGION\u0026#34; echo \u0026#34; Core Stack: $CORE_STACK_NAME\u0026#34; echo \u0026#34; Services: ${SERVICES[@]}\u0026#34; Core Stack Deployment Script deploy-core.sh:\n#!/bin/bash set -e ENVIRONMENT=$1 REGION=$2 STACK_NAME=\u0026#34;travel-guide-core-${ENVIRONMENT}\u0026#34; TEMPLATE_FILE=\u0026#34;infrastructure/core/template.yaml\u0026#34; PARAMS_FILE=\u0026#34;infrastructure/parameters/${ENVIRONMENT}.json\u0026#34; echo \u0026#34;Deploying Core Stack: $STACK_NAME\u0026#34; # Validate template echo \u0026#34;Validating template...\u0026#34; aws cloudformation validate-template \\ --template-body file://$TEMPLATE_FILE \\ --region $REGION # Convert parameters params_override=$(python3 -c \u0026#34;import json, sys; \\ data=json.load(open(\u0026#39;$PARAMS_FILE\u0026#39;)); \\ print(\u0026#39; \u0026#39;.join([f\u0026#39;ParameterKey={k},ParameterValue={v}\u0026#39; \\ for k,v in data.items()]))\u0026#34;) # Package SAM template (if using SAM) echo \u0026#34;Packaging template...\u0026#34; sam package \\ --template-file $TEMPLATE_FILE \\ --s3-bucket travel-guide-deployment-${ENVIRONMENT} \\ --output-template-file /tmp/core-packaged.yaml \\ --region $REGION # Deploy stack echo \u0026#34;Deploying stack...\u0026#34; aws cloudformation deploy \\ --template-file /tmp/core-packaged.yaml \\ --stack-name $STACK_NAME \\ --parameter-overrides $params_override \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \\ --region $REGION \\ --no-fail-on-empty-changeset echo \u0026#34;‚úÖ Core Stack deployed: $STACK_NAME\u0026#34; Service Stack Deployment Script deploy-service.sh:\n#!/bin/bash set -e SERVICE=$1 ENVIRONMENT=$2 REGION=$3 CORE_STACK_NAME=\u0026#34;travel-guide-core-${ENVIRONMENT}\u0026#34; STACK_NAME=\u0026#34;travel-guide-${SERVICE}-service-${ENVIRONMENT}\u0026#34; TEMPLATE_FILE=\u0026#34;infrastructure/services/${SERVICE}/template.yaml\u0026#34; echo \u0026#34;Deploying Service: $SERVICE\u0026#34; # Validate template aws cloudformation validate-template \\ --template-body file://$TEMPLATE_FILE \\ --region $REGION # Package template sam package \\ --template-file $TEMPLATE_FILE \\ --s3-bucket travel-guide-deployment-${ENVIRONMENT} \\ --output-template-file /tmp/${SERVICE}-packaged.yaml \\ --region $REGION # Deploy with core stack reference aws cloudformation deploy \\ --template-file /tmp/${SERVICE}-packaged.yaml \\ --stack-name $STACK_NAME \\ --parameter-overrides \\ CoreStackName=$CORE_STACK_NAME \\ Environment=$ENVIRONMENT \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \\ --region $REGION \\ --no-fail-on-empty-changeset echo \u0026#34;‚úÖ Service deployed: $SERVICE\u0026#34; Deployment Flow Diagram flowchart TD Start[Start Deployment] --\u0026gt; Setup[Setup Deployment Bucket] Setup --\u0026gt; ValidateCore[Validate Core Template] ValidateCore --\u0026gt; PackageCore[Package Core Template] PackageCore --\u0026gt; DeployCore[Deploy Core Stack] DeployCore --\u0026gt; WaitCore[Wait for Core Complete] WaitCore --\u0026gt; CheckCore{Core Success?} CheckCore --\u0026gt;|No| ErrorCore[Show Error \u0026amp; Exit] CheckCore --\u0026gt;|Yes| DeployAuth[Deploy Auth Service] DeployAuth --\u0026gt; DeployArticle[Deploy Article Service] DeployArticle --\u0026gt; DeployMedia[Deploy Media Service] DeployMedia --\u0026gt; DeployAI[Deploy AI Service] DeployAI --\u0026gt; DeployGallery[Deploy Gallery Service] DeployGallery --\u0026gt; DeployNotif[Deploy Notification Service] DeployNotif --\u0026gt; Success[Deployment Complete ‚úÖ] ErrorCore --\u0026gt; End[End] Success --\u0026gt; End Error Handling Comprehensive error handling:\n#!/bin/bash # Exit on any error set -e # Exit on undefined variable set -u # Catch errors in pipes set -o pipefail # Cleanup function cleanup() { local exit_code=$? if [ $exit_code -ne 0 ]; then echo \u0026#34;‚ùå Deployment failed with exit code: $exit_code\u0026#34; echo \u0026#34;üìã Check CloudFormation events for details:\u0026#34; echo \u0026#34;aws cloudformation describe-stack-events --stack-name $STACK_NAME\u0026#34; fi } # Register cleanup on exit trap cleanup EXIT # Error handler for specific commands deploy_with_retry() { local max_attempts=3 local attempt=1 while [ $attempt -le $max_attempts ]; do echo \u0026#34;Attempt $attempt of $max_attempts...\u0026#34; if aws cloudformation deploy \u0026#34;$@\u0026#34;; then echo \u0026#34;‚úÖ Deployment successful\u0026#34; return 0 fi echo \u0026#34;‚ö†Ô∏è Attempt $attempt failed\u0026#34; attempt=$((attempt + 1)) sleep 10 done echo \u0026#34;‚ùå All attempts failed\u0026#34; return 1 } Rollback Strategies Automatic Rollback:\n# CloudFormation automatically rolls back on failure aws cloudformation deploy \\ --template-file template.yaml \\ --stack-name my-stack \\ --disable-rollback false # Default behavior Manual Rollback:\n# Delete failed stack aws cloudformation delete-stack --stack-name my-stack # Redeploy previous version git checkout v1.2.3 ./deploy.sh staging Rollback to Previous Version:\n# List stack versions aws cloudformation list-stacks \\ --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE # Rollback using previous template aws cloudformation update-stack \\ --stack-name my-stack \\ --use-previous-template Validation Steps Pre-deployment validation:\n# 1. Validate template syntax aws cloudformation validate-template \\ --template-body file://template.yaml # 2. Lint template (using cfn-lint) cfn-lint template.yaml # 3. Check parameter file jq empty \u0026lt; parameters/staging.json # 4. Verify AWS credentials aws sts get-caller-identity # 5. Check stack dependencies aws cloudformation list-exports \\ --query \u0026#34;Exports[?Name==\u0026#39;travel-guide-core-staging-ArticlesTableName\u0026#39;]\u0026#34; Parallel Deployment Deploy services in parallel (faster):\n#!/bin/bash SERVICES=(\u0026#34;auth\u0026#34; \u0026#34;articles\u0026#34; \u0026#34;media\u0026#34; \u0026#34;ai\u0026#34; \u0026#34;gallery\u0026#34; \u0026#34;notification\u0026#34;) # Deploy all services in background for service in \u0026#34;${SERVICES[@]}\u0026#34;; do ./scripts/deploy-service.sh $service $ENVIRONMENT $REGION \u0026amp; done # Wait for all background jobs wait echo \u0026#34;‚úÖ All services deployed\u0026#34; Key Takeaways Orchestration scripts automate multi-stack deployment Error handling prevents partial deployments Validation catches issues before deployment Rollback strategies enable quick recovery Parallel deployment speeds up process Logging helps debug issues Best Practices Always validate templates before deployment Use set -e to exit on errors Implement retry logic for transient failures Log all operations for debugging Test scripts in staging before production Document deployment process in README "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-image-processing/","title":"Image Processing - Lambda, Rekognition, SQS, SNS, SES","tags":[],"description":"","content":"Overview In the Travel Guide project, the automated image processing system includes the following key components:\nLambda Content Moderation - Content moderation for images Lambda Detect Labels - Automatic label detection SQS Queue - Asynchronous processing SNS Topic \u0026amp; SES - Email notifications Lambda Functions Architecture Processing Flow User uploads image ‚Üí S3 S3 Event ‚Üí SQS Queue Lambda Content Moderation checks content If valid ‚Üí Lambda Detect Labels adds tags If violated ‚Üí SNS + SES sends alert email Content Lambda Content Moderation Lambda Detect Labels SQS Pipeline SNS \u0026amp; SES Notification "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-image-processing/5.4.4-sns-ses-notification/","title":"SNS &amp; SES Notification","tags":[],"description":"","content":"Purpose Send automatic email notifications when:\nContent policy violations are detected Admin needs to be notified about violations Users need to know their images were removed/quarantined SNS \u0026amp; SES Architecture Lambda Content Moderation ‚Üì (if violation detected) SNS Topic: ImageModerationAlerts ‚Üì ‚îú‚îÄ‚Üí SES: Email to Admin ‚îî‚îÄ‚Üí SES: Email to User Demo: SNS Topic Configuration SNS Topic is configured to distribute notifications to multiple subscribers:\nDemo: SES Verified Emails Before sending emails, addresses must be verified in SES:\nSNS Topic ImageModerationAlerts Topic Subscriptions:\nAdmin email (verified in SES) User email (verified in SES) Message Format:\n{ \u0026#34;articleId\u0026#34;: \u0026#34;article-123\u0026#34;, \u0026#34;imageKey\u0026#34;: \u0026#34;uploads/image.jpg\u0026#34;, \u0026#34;ownerId\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;violationType\u0026#34;: \u0026#34;explicit-content\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;deleted\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-12-08T10:30:00Z\u0026#34; } Email Templates Admin Notification Email Subject: ‚ö†Ô∏è Content Moderation Alert - Image Violation Detected Dear Admin, An image has been flagged for content policy violation: Article ID: {articleId} Image: {imageKey} Owner: {ownerId} Violation Type: {violationType} Severity: {severity} Action Taken: {action} Please review this case in the admin dashboard. --- Travel Guide Moderation System User Notification Email Subject: Your image was removed - Content Policy Violation Hello, We detected that one of your uploaded images violates our content policy: Article ID: {articleId} Violation Type: {violationType} Action: Image has been removed Please review our content guidelines and ensure future uploads comply with our policies. If you believe this was a mistake, please contact support. --- Travel Guide Team Demo Flow 1. Upload violating image User uploads inappropriate content S3 ‚Üí SQS ‚Üí Lambda Content Moderation 2. Detect violation Rekognition detect_moderation_labels returns violation Lambda determines severity level 3. Publish to SNS def send_admin_notification(article_id, key, moderation_result, owner_id): message = { \u0026#39;articleId\u0026#39;: article_id, \u0026#39;imageKey\u0026#39;: key, \u0026#39;ownerId\u0026#39;: owner_id, \u0026#39;violationType\u0026#39;: moderation_result[\u0026#39;labels\u0026#39;], \u0026#39;severity\u0026#39;: moderation_result[\u0026#39;maxSeverity\u0026#39;], \u0026#39;action\u0026#39;: \u0026#39;deleted\u0026#39;, \u0026#39;timestamp\u0026#39;: datetime.utcnow().isoformat() } sns.publish( TopicArn=SNS_TOPIC_ARN, Subject=\u0026#39;‚ö†Ô∏è Content Moderation Alert\u0026#39;, Message=json.dumps(message) ) 4. SES sends emails Admin receives alert email User receives notification about removed image Demo Results ‚úÖ Admin Email:\nReceives violation notification Has complete information for review Can view details in dashboard ‚úÖ User Email:\nNotified about image removal Understands violation reason Knows how to contact support if needed Monitoring Track important metrics:\nSNS NumberOfMessagesPublished - Messages sent SES Send - Emails sent successfully SES Bounce - Bounced emails SES Complaint - Emails marked as spam Conclusion Automated notification system:\nEnsures admin is immediately notified of violations Users are transparently informed Reduces manual work Increases system professionalism "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in three events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: 8:30 AM - 12:00 PM, Saturday, November 15, 2025\nLocation: AWS Office\nRole: Attendee\nDescription: Workshop providing comprehensive knowledge about AI/ML services on AWS, introducing Amazon SageMaker and Amazon Bedrock, hands-on chatbot building with GenAI, and exploring Prompt Engineering and RAG.\nEvent 2 Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: 8:30 AM - 5:00 PM, Monday, November 17, 2025\nLocation: AWS Office\nRole: Attendee\nDescription: Comprehensive workshop on DevOps practices on AWS, covering CI/CD pipelines, Infrastructure as Code (CloudFormation and CDK), container services (ECS/EKS), and monitoring \u0026amp; observability.\nEvent 3 Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: 8:30 AM - 12:00 PM, Saturday, November 29, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nDescription: Workshop covering 5 pillars of AWS Security: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response, with practical playbooks and best practices.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.4-image-processing/5.4.5-gallery-service/","title":"Gallery Service - Photo Discovery","tags":[],"description":"","content":"Gallery Service Overview Gallery Service provides photo discovery functionality through trending tags and tag-based search. It consumes data from AI Service\u0026rsquo;s label detection and presents it through user-friendly APIs.\nPurpose and Use Cases Main Functions:\nTrending Tags: Display most popular tags based on photo count Photo Gallery: Search photos by specific tags Position in System:\nConsumes data from AI Service (auto-detected tags) Provides API for Frontend to display gallery Reads from DynamoDB tables populated by AI Service Architecture Components Overview The Gallery Service consists of:\n2 Lambda Functions: GetTrendingTags, GetArticlesByTag 1 API Gateway: REST API with 2 endpoints 1 Lambda Layer: Shared dependencies (boto3, CORS utilities) 2 DynamoDB Tables: GalleryPhotosTable, GalleryTrendsTable (from Core Stack) Data Flow User Uploads Image ‚Üì Article Service ‚Üí S3 Bucket ‚Üì S3 Event ‚Üí AI Service ‚Üì Label Detection ‚Üì ‚îú‚îÄ‚Üí Save to GalleryPhotosTable ‚îî‚îÄ‚Üí Update GalleryTrendsTable (increment count) ‚Üì Gallery Service (Read-only) ‚Üì Frontend Display Dependencies with Core Stack Gallery Service imports from Core Stack:\n# From core-infra stack Imports: - GalleryPhotosTableName - GalleryTrendsTableName Deployment Order:\nCore Stack (creates tables) AI Service (populates tables) Gallery Service (reads tables) Lambda Functions Function 1: GetTrendingTagsFunction Purpose: Return list of trending tags sorted by popularity\nConfiguration:\nRuntime: Python 3.11 Timeout: 30 seconds Memory: 512 MB Handler: get_trending_tags.lambda_handler Logic Flow:\n1. Parse query parameters (limit) 2. Scan all tags from GalleryTrendsTable 3. Sort by count (descending) in-memory 4. Return top N tags Code Structure:\ndef lambda_handler(event, context): # Parse parameters limit = int(params.get(\u0026#34;limit\u0026#34;, 20)) # Scan all tags all_tags = [] while True: response = table.scan(**scan_kwargs) all_tags.extend(response.get(\u0026#39;Items\u0026#39;, [])) if \u0026#39;LastEvaluatedKey\u0026#39; not in response: break # Sort by count all_tags.sort(key=lambda x: x.get(\u0026#39;count\u0026#39;, 0), reverse=True) # Return top N return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;items\u0026#39;: all_tags[:limit], \u0026#39;total_tags\u0026#39;: len(all_tags) }) } Response Example:\n{ \u0026#34;items\u0026#34;: [ { \u0026#34;tag_name\u0026#34;: \u0026#34;Beach\u0026#34;, \u0026#34;count\u0026#34;: 150, \u0026#34;cover_image\u0026#34;: \u0026#34;articles/abc123/image.jpg\u0026#34;, \u0026#34;last_updated\u0026#34;: \u0026#34;2024-01-15T10:30:00Z\u0026#34; }, { \u0026#34;tag_name\u0026#34;: \u0026#34;Mountain\u0026#34;, \u0026#34;count\u0026#34;: 120, \u0026#34;cover_image\u0026#34;: \u0026#34;articles/def456/image.jpg\u0026#34;, \u0026#34;last_updated\u0026#34;: \u0026#34;2024-01-14T15:20:00Z\u0026#34; } ], \u0026#34;total_tags\u0026#34;: 500 } Function 2: GetArticlesByTagFunction Purpose: Search photos with specific tag\nConfiguration:\nRuntime: Python 3.11 Timeout: 30 seconds Memory: 512 MB Handler: get_articles_by_tag.lambda_handler Logic Flow:\n1. Parse and validate tag parameter 2. Scan GalleryPhotosTable 3. Filter photos with matching tag (in-memory) 4. Limit results 5. Return photos Code Structure:\ndef lambda_handler(event, context): # Validate parameters tag = params.get(\u0026#34;tag\u0026#34;, \u0026#34;\u0026#34;).strip().lower() limit = int(params.get(\u0026#34;limit\u0026#34;, 50)) if not tag: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;tag parameter is required\u0026#39;}) } # Scan and filter matching_photos = [] while len(matching_photos) \u0026lt; limit: response = photos_table.scan(**scan_kwargs) for item in response.get(\u0026#39;Items\u0026#39;, []): photo_tags = [t.lower() for t in item.get(\u0026#39;tags\u0026#39;, [])] if tag in photo_tags: matching_photos.append(item) if \u0026#39;LastEvaluatedKey\u0026#39; not in response: break return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;items\u0026#39;: matching_photos[:limit]}) } Response Example:\n{ \u0026#34;items\u0026#34;: [ { \u0026#34;photo_id\u0026#34;: \u0026#34;article-123-image-1\u0026#34;, \u0026#34;image_url\u0026#34;: \u0026#34;articles/abc123/image.jpg\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;beach\u0026#34;, \u0026#34;sunset\u0026#34;, \u0026#34;ocean\u0026#34;], \u0026#34;status\u0026#34;: \u0026#34;public\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-01-15T10:30:00Z\u0026#34; } ] } Data Models GalleryPhotosTable Purpose: Store photo metadata and tags for gallery display\nSchema:\nField Type Description photo_id String (PK) Unique identifier (e.g., \u0026ldquo;article-123-image-1\u0026rdquo;) image_url String S3 key (e.g., \u0026ldquo;articles/abc123/image.jpg\u0026rdquo;) tags String[] Combined user tags + auto-detected tags autoTags String[] Deprecated (backward compatibility) status String \u0026ldquo;public\u0026rdquo; or \u0026ldquo;private\u0026rdquo; created_at String ISO 8601 timestamp createdAt String Alias for frontend Indexes:\nPrimary Key: photo_id No GSIs (Global Secondary Indexes) Data Population:\nPopulated by AI Service\u0026rsquo;s save_to_gallery Lambda Triggered when AI detects labels in images Combines user-provided tags with AI-detected tags GalleryTrendsTable Purpose: Track trending tags and popularity statistics\nSchema:\nField Type Description tag_name String (PK) Tag name (e.g., \u0026ldquo;beach\u0026rdquo;) count Number Number of photos with this tag cover_image String S3 key for cover image (optional) last_updated String ISO 8601 timestamp Indexes:\nPrimary Key: tag_name No GSIs Data Population:\nUpdated by AI Service when photos are added/removed Increment count when new photo with tag is added Decrement count when photo is deleted API Endpoints Endpoint 1: GET /gallery/trending Purpose: Get list of trending tags\nRequest:\nGET /gallery/trending?limit=20 HTTP/1.1 Host: {api-gateway-url} Query Parameters:\nParameter Type Required Default Description limit integer No 20 Maximum number of tags to return Success Response (200):\n{ \u0026#34;items\u0026#34;: [ { \u0026#34;tag_name\u0026#34;: \u0026#34;Beach\u0026#34;, \u0026#34;count\u0026#34;: 150, \u0026#34;cover_image\u0026#34;: \u0026#34;articles/abc123/image.jpg\u0026#34;, \u0026#34;last_updated\u0026#34;: \u0026#34;2024-01-15T10:30:00Z\u0026#34; } ], \u0026#34;total_tags\u0026#34;: 500 } Error Response (500):\n{ \u0026#34;error\u0026#34;: \u0026#34;Internal error: Gallery Trends table not configured\u0026#34; } Frontend Integration:\nasync function getTrendingTags(limit: number = 20) { const response = await fetch( `${API_BASE_URL}/gallery/trending?limit=${limit}` ); if (!response.ok) { throw new Error(\u0026#39;Failed to fetch trending tags\u0026#39;); } return await response.json(); } // Usage const data = await getTrendingTags(10); console.log(data.items); Endpoint 2: GET /gallery/articles Purpose: Search photos by tag\nRequest:\nGET /gallery/articles?tag=beach\u0026amp;limit=50 HTTP/1.1 Host: {api-gateway-url} Query Parameters:\nParameter Type Required Default Description tag string Yes - Tag name to search (case-insensitive) limit integer No 50 Maximum number of photos to return Success Response (200):\n{ \u0026#34;items\u0026#34;: [ { \u0026#34;photo_id\u0026#34;: \u0026#34;article-123-image-1\u0026#34;, \u0026#34;image_url\u0026#34;: \u0026#34;articles/abc123/image.jpg\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;beach\u0026#34;, \u0026#34;sunset\u0026#34;, \u0026#34;ocean\u0026#34;], \u0026#34;status\u0026#34;: \u0026#34;public\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2024-01-15T10:30:00Z\u0026#34; } ] } Error Responses:\n// 400 - Bad Request { \u0026#34;error\u0026#34;: \u0026#34;tag parameter is required\u0026#34; } // 500 - Internal Server Error { \u0026#34;error\u0026#34;: \u0026#34;Internal error: Gallery Photos table not configured\u0026#34; } Frontend Integration:\nasync function getPhotosByTag(tag: string, limit: number = 50) { const response = await fetch( `${API_BASE_URL}/gallery/articles?tag=${encodeURIComponent(tag)}\u0026amp;limit=${limit}` ); if (!response.ok) { const error = await response.json(); throw new Error(error.error || \u0026#39;Failed to fetch photos\u0026#39;); } return await response.json(); } // Usage const photos = await getPhotosByTag(\u0026#39;beach\u0026#39;, 20); console.log(photos.items); Frontend Gallery Display Trending Tags View:\nSearch Results View:\nPerformance Analysis Current Performance Metrics Lambda Configuration:\nFunction Timeout Memory Avg Execution Cold Start GetTrendingTags 30s 512 MB ~2-5s ~1-2s GetArticlesByTag 30s 512 MB ~3-8s ~1-2s DynamoDB Operations:\nBoth functions use Scan operations (full table scan) No indexes utilized In-memory filtering and sorting Performance Bottlenecks 1. Full Table Scans Issue: Scan entire table on every request\nImpact:\nSlow performance with large tables (\u0026gt;10,000 items) High DynamoDB read capacity consumption Increased costs Solution: Add Global Secondary Indexes (GSI)\n2. In-Memory Sorting (GetTrendingTags) Issue: Load all tags into memory, sort, then return top N\nImpact:\nUnnecessary memory usage Slow with large datasets Lambda timeout risk Solution: Use DynamoDB GSI with count as sort key\n# Recommended GSI GalleryTrendsTable: GlobalSecondaryIndexes: - IndexName: CountIndex KeySchema: - AttributeName: dummy_pk # All items have same value KeyType: HASH - AttributeName: count KeyType: RANGE Projection: ProjectionType: ALL 3. In-Memory Filtering (GetArticlesByTag) Issue: Scan all photos, filter by tag in Lambda\nImpact:\nVery inefficient Doesn\u0026rsquo;t scale High latency Solution: Use DynamoDB GSI or ElasticSearch\n# Recommended GSI GalleryPhotosTable: GlobalSecondaryIndexes: - IndexName: TagIndex KeySchema: - AttributeName: tag KeyType: HASH - AttributeName: created_at KeyType: RANGE Projection: ProjectionType: ALL Note: DynamoDB doesn\u0026rsquo;t support array attributes in GSI. Need to denormalize data or use ElasticSearch.\n4. No Caching Issue: Every request queries DynamoDB\nImpact:\nUnnecessary load Higher costs Slower response times Solution: Add caching layer\nOptions:\nElastiCache Redis: For frequently accessed data CloudFront: For API responses Lambda in-memory cache: For small datasets Example with ElastiCache:\nimport redis redis_client = redis.Redis(host=REDIS_HOST, port=6379) CACHE_TTL = 300 # 5 minutes def get_trending_tags_cached(limit): cache_key = f\u0026#34;trending_tags:{limit}\u0026#34; # Try cache first cached = redis_client.get(cache_key) if cached: return json.loads(cached) # Query DynamoDB result = get_trending_tags_from_db(limit) # Save to cache redis_client.setex(cache_key, CACHE_TTL, json.dumps(result)) return result 5. No Pagination Issue: Return all results (up to limit)\nImpact:\nLarge responses Slow load times Poor user experience Solution: Implement cursor-based pagination\ndef lambda_handler(event, context): params = event.get(\u0026#39;queryStringParameters\u0026#39;, {}) limit = int(params.get(\u0026#39;limit\u0026#39;, 20)) cursor = params.get(\u0026#39;cursor\u0026#39;) # LastEvaluatedKey from previous request scan_kwargs = {\u0026#39;Limit\u0026#39;: limit} if cursor: scan_kwargs[\u0026#39;ExclusiveStartKey\u0026#39;] = json.loads(base64.b64decode(cursor)) response = table.scan(**scan_kwargs) result = { \u0026#39;items\u0026#39;: response[\u0026#39;Items\u0026#39;], \u0026#39;cursor\u0026#39;: None } if \u0026#39;LastEvaluatedKey\u0026#39; in response: result[\u0026#39;cursor\u0026#39;] = base64.b64encode( json.dumps(response[\u0026#39;LastEvaluatedKey\u0026#39;]).encode() ).decode() return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(result)} Optimization Recommendations Short-term Improvements (Quick Wins) Add Caching\nImplement CloudFront caching for API responses TTL: 5 minutes for trending tags Reduces DynamoDB load by 90%+ Optimize Lambda Memory\nTest with 256 MB instead of 512 MB May reduce costs without impacting performance Add Pagination\nImplement cursor-based pagination Improve frontend UX with infinite scroll Error Handling\nAdd retry logic for DynamoDB errors Implement circuit breaker pattern Long-term Enhancements Add DynamoDB GSIs\nCountIndex for GalleryTrendsTable TagIndex for GalleryPhotosTable (requires denormalization) Implement ElasticSearch\nBetter for complex tag queries Full-text search capabilities Faceted search support Add ElastiCache\nRedis for frequently accessed data Reduce DynamoDB costs Improve response times Implement Event-Driven Updates\nUse DynamoDB Streams Update cache automatically when data changes Maintain cache consistency Cost Estimation Current Costs (Estimated) Assumptions:\n10,000 photos in gallery 500 unique tags 1,000 requests/day DynamoDB:\nRead Capacity: ~$0.25/day (full scans) Storage: ~$0.01/day Lambda:\nInvocations: ~$0.20/day Duration: ~$0.10/day API Gateway:\nRequests: ~$0.04/day Total: ~$0.60/day or ~$18/month\nOptimized Costs (With Caching) With CloudFront + ElastiCache:\nDynamoDB: ~$0.03/day (90% reduction) Lambda: ~$0.03/day (90% reduction) ElastiCache: ~$1.50/day (t3.micro) CloudFront: ~$0.02/day Total: ~$1.58/day or ~$47/month\nNote: Higher upfront cost but better performance and scalability.\nKey Takeaways Gallery Service provides photo discovery through trending tags and search Current implementation uses full table scans (inefficient) Performance bottlenecks include no caching, no indexes, in-memory operations Quick wins include adding caching and pagination Long-term should add GSIs or ElasticSearch for better scalability Cost optimization requires balancing performance vs infrastructure costs "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.5-auth-cognito-iam/","title":"Cognito &amp; IAM - Authentication &amp; Authorization","tags":[],"description":"","content":"Secure Authentication for Web/Mobile Applications This section covers the implementation of AWS Cognito for user authentication and IAM for authorization in the Travel Guide Application.\nOverview AWS Cognito provides user management and authentication services for web/mobile applications without building a login system from scratch. Combined with IAM roles and policies, it creates a secure, scalable authentication and authorization system.\nKey Components AWS Cognito:\nUser Pool for user management Sign-up / Sign-in / OTP email verification JWT Token management (id_token, access_token, refresh_token) Frontend integration (React) Backend integration (API Gateway + Cognito Authorizer) IAM (Identity \u0026amp; Access Management):\nRoles for Lambda, ECS, EC2 Policies for S3, DynamoDB, CloudWatch access Trust policies and resource-based permissions Least privilege principle Architecture The authentication flow:\nUser signs up/signs in via Cognito Hosted UI Cognito issues JWT tokens Frontend stores and manages tokens API Gateway validates tokens using Cognito Authorizer Lambda functions access AWS resources using IAM roles Content AWS Cognito Setup IAM Roles \u0026amp; Policies "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.2-iac-multistack/5.2.5-parameter-management/","title":"Parameter Management","tags":[],"description":"","content":"Parameter Management for Multiple Environments Overview Parameters allow us to deploy the same infrastructure templates to different environments (staging, production) with different configurations.\nParameter Files Structure We organize parameters in JSON files per environment:\nparameters/ ‚îú‚îÄ‚îÄ staging.json ‚îî‚îÄ‚îÄ prod.json Example Parameter File staging.json:\n{ \u0026#34;Environment\u0026#34;: \u0026#34;staging\u0026#34;, \u0026#34;CorsOrigin\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;AdminEmail\u0026#34;: \u0026#34;admin-staging@example.com\u0026#34;, \u0026#34;LogRetentionDays\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;EnableDebugLogs\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;MinCapacity\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;MaxCapacity\u0026#34;: \u0026#34;10\u0026#34; } prod.json:\n{ \u0026#34;Environment\u0026#34;: \u0026#34;prod\u0026#34;, \u0026#34;CorsOrigin\u0026#34;: \u0026#34;https://travelguide.com\u0026#34;, \u0026#34;AdminEmail\u0026#34;: \u0026#34;admin@example.com\u0026#34;, \u0026#34;LogRetentionDays\u0026#34;: \u0026#34;30\u0026#34;, \u0026#34;EnableDebugLogs\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;MinCapacity\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;MaxCapacity\u0026#34;: \u0026#34;100\u0026#34; } Parameter Passing Mechanism Parameters are converted from JSON to CloudFormation format:\n# Convert JSON to AWS CLI format params_override=$(python -c \u0026#34;import json, sys; \\ data=json.load(sys.stdin); \\ print(\u0026#39; \u0026#39;.join([f\u0026#39;ParameterKey={k},ParameterValue={v}\u0026#39; \\ for k,v in data.items()]))\u0026#34; \u0026lt; $PARAMS_FILE) # Deploy with parameters aws cloudformation deploy \\ --template-file template.yaml \\ --stack-name my-stack \\ --parameter-overrides $params_override Template Parameter Definitions In CloudFormation template:\nParameters: Environment: Type: String Default: staging AllowedValues: [staging, prod] Description: Deployment environment CorsOrigin: Type: String Default: \u0026#34;*\u0026#34; Description: CORS origin for API Gateway AdminEmail: Type: String Description: Admin email for notifications LogRetentionDays: Type: Number Default: 7 Description: CloudWatch Logs retention in days EnableDebugLogs: Type: String Default: \u0026#34;false\u0026#34; AllowedValues: [\u0026#34;true\u0026#34;, \u0026#34;false\u0026#34;] Description: Enable debug logging Using Parameters in Resources Resources: CreateArticleFunction: Type: AWS::Serverless::Function Properties: Environment: Variables: ENVIRONMENT: !Ref Environment DEBUG: !Ref EnableDebugLogs CORS_ORIGIN: !Ref CorsOrigin ApiGateway: Type: AWS::Serverless::Api Properties: Cors: AllowOrigin: !Sub \u0026#34;\u0026#39;${CorsOrigin}\u0026#39;\u0026#34; AllowHeaders: \u0026#34;\u0026#39;*\u0026#39;\u0026#34; LogGroup: Type: AWS::Logs::LogGroup Properties: RetentionInDays: !Ref LogRetentionDays Environment Separation Strategy Parameter Staging Production Reason CorsOrigin * https://domain.com Security LogRetention 7 days 30 days Cost vs compliance DebugLogs true false Performance MinCapacity 1 2 Availability MaxCapacity 10 100 Scale Sensitive Data Handling Current Approach (Basic):\n{ \u0026#34;DatabasePassword\u0026#34;: \u0026#34;hardcoded-password\u0026#34; } Recommended Approach (Secure):\n1. AWS Systems Manager Parameter Store:\nParameters: DatabasePasswordSSM: Type: AWS::SSM::Parameter::Value\u0026lt;String\u0026gt; Default: /travelguide/staging/db-password Resources: Function: Type: AWS::Serverless::Function Properties: Environment: Variables: DB_PASSWORD: !Ref DatabasePasswordSSM 2. AWS Secrets Manager:\nResources: DatabaseSecret: Type: AWS::SecretsManager::Secret Properties: Name: !Sub \u0026#39;${AWS::StackName}-db-secret\u0026#39; GenerateSecretString: SecretStringTemplate: \u0026#39;{\u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;}\u0026#39; GenerateStringKey: \u0026#34;password\u0026#34; PasswordLength: 32 Function: Type: AWS::Serverless::Function Properties: Environment: Variables: SECRET_ARN: !Ref DatabaseSecret Deployment Script with Parameters #!/bin/bash set -e ENVIRONMENT=$1 # staging or prod PARAMS_FILE=\u0026#34;parameters/${ENVIRONMENT}.json\u0026#34; if [ ! -f \u0026#34;$PARAMS_FILE\u0026#34; ]; then echo \u0026#34;Error: Parameter file not found: $PARAMS_FILE\u0026#34; exit 1 fi # Convert JSON to parameter overrides params_override=$(python -c \u0026#34;import json, sys; \\ data=json.load(sys.stdin); \\ print(\u0026#39; \u0026#39;.join([f\u0026#39;ParameterKey={k},ParameterValue={v}\u0026#39; \\ for k,v in data.items()]))\u0026#34; \u0026lt; $PARAMS_FILE) # Deploy stack aws cloudformation deploy \\ --template-file template.yaml \\ --stack-name \u0026#34;travel-guide-${ENVIRONMENT}\u0026#34; \\ --parameter-overrides $params_override \\ --capabilities CAPABILITY_IAM \\ --no-fail-on-empty-changeset echo \u0026#34;‚úÖ Deployed to ${ENVIRONMENT}\u0026#34; Best Practices Never commit secrets to Git\nUse .gitignore for sensitive parameter files Use Parameter Store or Secrets Manager Validate parameters before deployment\n# Validate JSON syntax jq empty \u0026lt; parameters/staging.json Document parameters in README\n## Parameters - `Environment`: Deployment environment (staging/prod) - `CorsOrigin`: CORS origin for API Gateway - `AdminEmail`: Email for admin notifications Use parameter constraints\nParameters: InstanceType: Type: String AllowedValues: [t3.micro, t3.small, t3.medium] Default: t3.micro Environment-specific naming\nResources: Bucket: Type: AWS::S3::Bucket Properties: BucketName: !Sub \u0026#39;travelguide-${Environment}-images\u0026#39; Key Takeaways Parameter files enable environment-specific configurations JSON format is easy to read and maintain Conversion script transforms JSON to CloudFormation format Sensitive data should use Parameter Store or Secrets Manager Validation prevents deployment errors Documentation helps team understand parameters Common Pitfalls ‚ùå Hardcoding values in templates\n# Bad Environment: Variables: API_URL: \u0026#34;https://api.staging.example.com\u0026#34; ‚úÖ Use parameters\n# Good Environment: Variables: API_URL: !Sub \u0026#34;https://api.${Environment}.example.com\u0026#34; ‚ùå Committing secrets to Git\n{ \u0026#34;DatabasePassword\u0026#34;: \u0026#34;super-secret-password\u0026#34; } ‚úÖ Use Secrets Manager\nDatabaseSecret: Type: AWS::SecretsManager::Secret Properties: GenerateSecretString: {} "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building Travel Guide Application with AWS Serverless Overview This workshop guides you through building a complete Travel Guide Application using AWS Serverless services. You\u0026rsquo;ll learn how to design, implement, and deploy a production-ready application with modern cloud architecture patterns.\nThe application allows users to share travel experiences, upload photos, and discover destinations through an AI-powered gallery system.\nKey Technologies:\nBackend: AWS Lambda, API Gateway, DynamoDB AI Processing: Amazon Rekognition, SQS, SNS/SES Authentication: Amazon Cognito, IAM Storage \u0026amp; CDN: S3, CloudFront Infrastructure: AWS SAM, CloudFormation Content Workshop Overview Infrastructure as Code \u0026amp; Multi-Stack Architecture Backend API \u0026amp; Articles Service AI Image Processing Pipeline Authentication with Cognito \u0026amp; IAM CloudFront CDN \u0026amp; Location Services Security Implementation "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.6-cloudfront-s3-location/","title":"CloudFront, S3 Upload &amp; Location Service","tags":[],"description":"","content":"CloudFront (Static + Dynamic), S3 Upload Web, AWS Location Service This section covers the implementation of Amazon CloudFront for content delivery, Amazon S3 for image uploads using pre-signed URLs, and AWS Location Service for geocoding and mapping in the Travel Guide Application.\nOverview The Travel Guide application uses three key AWS services to deliver a fast, secure, and feature-rich experience:\nAmazon CloudFront:\nCDN for static web content (React build) CDN for images (articles, thumbnails, avatars, covers) HTTPS redirect and compression Origin Access Identity (OAI) for secure S3 access Amazon S3 Upload Web:\nPre-signed URLs for secure, direct uploads No AWS credentials needed on client Time-limited upload permissions (15 minutes) CORS configuration for browser uploads AWS Location Service:\nPlace Index for geocoding/reverse geocoding Esri data source DynamoDB cache for cost optimization Nominatim fallback for reliability Architecture The system operates in three main flows:\nFlow A ‚Äî Static Web User accesses React web application CloudFront serves as CDN CloudFront fetches content from S3 StaticSiteBucket (private) via OAI Users always redirected to HTTPS with default index.html Flow B ‚Äî Image Upload \u0026amp; Display Frontend calls /upload-url API to get pre-signed URL Frontend uploads image directly to S3 ArticleImagesBucket When displaying articles/images, frontend uses CloudFront domain for fast loading CloudFront maps image paths to S3 origin Flow C ‚Äî Map/Geocoding When creating/updating articles with coordinates, backend uses AWS Location Service Place Index Reverse geocoding: lat/lng ‚Üí place name Forward geocoding: text ‚Üí lat/lng (for search) DynamoDB cache reduces Location Service costs Nominatim fallback for reliability Content S3 Pre-signed URL Upload CloudFront CDN Configuration AWS Location Service Integration "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.2-iac-multistack/5.2.6-lessons-learned/","title":"Lessons Learned &amp; Best Practices","tags":[],"description":"","content":"Lessons Learned \u0026amp; Best Practices What Worked Well ‚úÖ 1. Multi-Stack Pattern Success: Significantly reduced deployment time and risk.\nEvidence:\nService deployments: 2-3 minutes (vs 15-20 minutes monolithic) Bug fixes deployed without affecting other services Teams worked independently without conflicts Recommendation: Use multi-stack for any microservices architecture.\n2. Cross-Stack References Success: Type-safe dependencies between stacks.\nEvidence:\nCloudFormation validates imports at deployment time No hardcoded ARNs or names in code Easy to track dependencies Recommendation: Always use exports/imports over hardcoding.\n3. Bash Scripts Success: Simple but effective orchestration.\nEvidence:\nEasy to understand and modify No additional tools required Works in any CI/CD system Recommendation: Start with bash, migrate to sophisticated tools only if needed.\n4. SAM Simplification Success: Reduced boilerplate significantly.\nEvidence:\nLambda + API Gateway: 10 lines (vs 50+ lines pure CloudFormation) Automatic IAM role generation Built-in best practices Recommendation: Use SAM for all serverless applications.\n5. Environment Separation Success: Clean separation via parameters.\nEvidence:\nSame templates for staging and prod Easy to add new environments No code duplication Recommendation: Always use parameters for environment-specific config.\nChallenges Faced ‚ö†Ô∏è 1. Learning Curve Challenge: Initial learning curve for cross-stack references.\nImpact: First deployment took 2 days to get right.\nSolution:\nCreated documentation with examples Established naming conventions Built reusable templates Lesson: Invest time in documentation upfront.\n2. Debugging CloudFormation Errors Challenge: CloudFormation error messages can be cryptic.\nImpact: Spent hours debugging \u0026ldquo;Resource failed to stabilize\u0026rdquo;.\nSolution:\nCheck CloudWatch Logs immediately Use aws cloudformation describe-stack-events Enable detailed logging in Lambda Lesson: Always check CloudWatch Logs first.\n3. Stack Deletion Dependencies Challenge: Cannot delete core stack while services are running.\nImpact: Manual cleanup required in specific order.\nSolution:\n# Created cleanup script ./scripts/cleanup.sh staging # Deletes in correct order: # 1. Service stacks # 2. Core stack # 3. Deployment bucket Lesson: Plan deletion strategy from the start.\n4. Export Naming Challenge: Changed export name, broke all importing stacks.\nImpact: Had to update all service stacks simultaneously.\nSolution:\nEstablished naming convention early Documented all exports Added validation in deployment scripts Lesson: Plan export names carefully - they\u0026rsquo;re hard to change.\n5. Sequential Deployment Challenge: Sequential deployment slow for many services.\nImpact: Full deployment took 15+ minutes.\nSolution:\nImplemented parallel service deployment Reduced to 5 minutes total Lesson: Parallelize independent operations.\nBest Practices üéØ 1. Export Naming Convention # ‚úÖ Good: Include stack name Export: Name: !Sub \u0026#39;${AWS::StackName}-ResourceName\u0026#39; # ‚ùå Bad: Generic name Export: Name: ResourceName 2. Stack Separation ‚úÖ Core Stack: - DynamoDB Tables - S3 Buckets - Cognito User Pools - VPC/Networking ‚úÖ Service Stacks: - Lambda Functions - API Gateway - SQS Queues - SNS Topics 3. Template Validation # Always validate before deploy aws cloudformation validate-template \\ --template-body file://template.yaml # Use linting tools cfn-lint template.yaml 4. Error Handling # Use set -e in all scripts set -e set -o pipefail # Add cleanup trap trap cleanup ERR EXIT 5. Documentation ## Stack Exports - `{StackName}-ArticlesTableName`: DynamoDB table name - `{StackName}-ArticlesTableArn`: DynamoDB table ARN ## Deployment ```bash ./deploy.sh staging us-east-1 Rollback ./rollback.sh staging #### 6. Testing Strategy ```bash # Test in staging first ./deploy.sh staging # Verify functionality ./test.sh staging # Deploy to production ./deploy.sh prod 7. Rollback Plan # Always have rollback ready git tag v1.2.3 git push --tags # If issues: git checkout v1.2.2 ./deploy.sh prod Recommendations for Improvements üöÄ 1. CI/CD Integration Current: Manual deployment via scripts.\nImprovement: Automate with GitHub Actions/GitLab CI.\nExample:\n# .github/workflows/deploy.yml name: Deploy on: push: branches: [main] jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Deploy to staging run: ./deploy.sh staging Benefit: Automatic deployment on merge.\n2. Parallel Service Deployment Current: Sequential service deployment.\nImprovement: Deploy services in parallel.\nExample:\nfor service in \u0026#34;${SERVICES[@]}\u0026#34;; do ./deploy-service.sh $service $ENV \u0026amp; done wait Benefit: 3x faster deployment.\n3. Parameter Store for Secrets Current: Secrets in parameter files (not committed).\nImprovement: Use AWS Systems Manager Parameter Store.\nExample:\nParameters: DatabasePassword: Type: AWS::SSM::Parameter::Value\u0026lt;String\u0026gt; Default: /travelguide/staging/db-password Benefit: Centralized secret management.\n4. CloudWatch Dashboards Current: Manual monitoring via console.\nImprovement: Automated dashboards for stack health.\nExample:\nDashboard: Type: AWS::CloudWatch::Dashboard Properties: DashboardBody: !Sub | { \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Errors\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}] ] } } ] } Benefit: Proactive monitoring.\n5. CloudFormation Guard Current: Manual policy validation.\nImprovement: Automated policy enforcement.\nExample:\n# rules/security.guard AWS::S3::Bucket { Properties.PublicAccessBlockConfiguration exists Properties.BucketEncryption exists } Benefit: Prevent security misconfigurations.\n6. Auto-Generated Documentation Current: Manual documentation.\nImprovement: Generate docs from templates.\nExample:\n# Use cfn-diagram cfn-diagram template.yaml \u0026gt; architecture.png Benefit: Always up-to-date documentation.\nKey Takeaways üìù Multi-stack pattern is essential for microservices Cross-stack references provide type-safe dependencies Bash scripts are sufficient for orchestration SAM significantly reduces boilerplate Documentation is crucial for team success Testing in staging prevents production issues Rollback plan is mandatory CI/CD integration should be next step Parallel deployment speeds up process Security should be automated with Guard rules Comparison with Alternatives Aspect CloudFormation/SAM Terraform AWS CDK Pulumi Our Experience ‚úÖ Positive N/A N/A N/A Learning Curve Medium Medium High High AWS Integration Excellent Good Excellent Good State Management Automatic Manual Automatic Cloud Multi-Cloud No Yes No Yes Cost Free Free Free Free Recommendation ‚úÖ For AWS-only For multi-cloud For complex logic For full language power Final Thoughts The multi-stack pattern with CloudFormation/SAM proved to be the right choice for our Travel Guide Application. While there were challenges, the benefits far outweighed the costs:\nWins:\n‚úÖ Faster deployments (2-3 min vs 15-20 min) ‚úÖ Reduced blast radius ‚úÖ Independent team workflows ‚úÖ Easy rollbacks ‚úÖ Cost attribution per service Areas for Improvement:\nüîÑ CI/CD automation üîÑ Parallel deployments üîÑ Centralized secret management üîÑ Automated monitoring Would we do it again? Absolutely yes! ‚úÖ\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from 09/08/2025 to 12/09/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nWork Accomplished During the internship period, I participated in the following main tasks:\nTravel Guide Web App project development: Participated in building a travel guide web application with my team, utilizing AWS services to deploy and operate the system. Technical content translation and localization: Translated AWS blog posts and technical documentation from English to Vietnamese, ensuring accurate and accessible content for the Vietnamese user community Participation in community events: Participated in AWS workshops and meetups, directly engaging with the developer community AWS services research and hands-on practice: Gained in-depth knowledge of cloud computing services such as EC2, S3, Lambda, RDS, and best practices for deploying applications on AWS Skills Developed Through the internship process, I significantly improved the following skills:\nProfessional expertise: Deeper understanding of cloud computing, AWS system architecture, and best practices for using cloud services Technical translation: Enhanced ability to accurately translate technical terminology in an understandable manner Technical writing: Improved ability to write clear, well-structured documentation and blog posts Teamwork: Learned to collaborate effectively with team members, share knowledge, and support each other Communication: Improved ability to present ideas, report work progress, and interact with the community Time management: Learned to prioritize tasks and meet deadlines Work Ethic In terms of work ethic, I always strived to:\nComplete assigned tasks with a high sense of responsibility Proactively seek out and learn new knowledge related to the work Be open to feedback and make adjustments for self-improvement Contribute ideas and initiatives to team projects To objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Cloud Computing Knowledge Proficiency in AWS services, understanding cloud architecture, applying best practices ‚òê ‚úÖ ‚òê 2 Technical Translation Skills Accurate terminology translation, clear content, culturally appropriate localization ‚úÖ ‚òê ‚òê 3 Initiative \u0026amp; Creativity Self-research, proposing new ideas, willingness to experiment with technology ‚òê ‚úÖ ‚òê 4 Time Management \u0026amp; Meeting Deadlines Prioritizing tasks effectively, completing on schedule, timely reporting ‚òê ‚úÖ ‚òê 5 Process Compliance Adhering to company regulations, schedules, and work procedures ‚òê ‚úÖ ‚òê 6 Learning \u0026amp; Development Ability Quick absorption of new knowledge, applying feedback for improvement ‚òê ‚úÖ ‚òê 7 Presentation \u0026amp; Reporting Skills Clear expression, logical presentation, writing understandable documentation ‚òê ‚úÖ ‚òê 8 Teamwork \u0026amp; Colleague Support Good team coordination, knowledge sharing, willingness to help ‚úÖ ‚òê ‚òê 9 Attitude \u0026amp; Professional Ethics Respecting colleagues, maintaining positive attitude, information confidentiality ‚úÖ ‚òê ‚òê 10 Analysis \u0026amp; Problem Handling Investigating root causes, providing feasible solutions, situation management ‚òê ‚òê ‚úÖ 11 Project \u0026amp; Community Contribution Successfully completing project tasks, participating in events, supporting community ‚òê ‚úÖ ‚òê 12 Overall Assessment General work performance, level of achieving internship objectives ‚òê ‚úÖ ‚òê Strengths Excellent technical translation skills: Capable of accurately translating complex AWS terminology, localizing content appropriately for Vietnamese culture, ensuring documentation is accessible to the community Effective teamwork and colleague support: Coordinate well with team members, willing to share knowledge and help colleagues when needed Good attitude and professional ethics: Always respect colleagues, maintain a positive attitude at work, conscious of information confidentiality Strong learning and development ability: Quick to learn new AWS knowledge, open to receiving feedback and applying it for self-improvement Reasonable time management: Know how to prioritize tasks, complete assignments on schedule, and report in a timely manner Areas for Improvement Cloud Computing knowledge: Although having a good foundation, need to gain deeper understanding of cloud system architecture and best practices to apply them optimally in real projects Analysis and problem-handling ability: This is the biggest weakness that needs improvement. Need to develop systematic analytical thinking, investigate root causes of problems, and provide more feasible and effective solutions Initiative and creativity: Need to be more proactive in proposing new ideas and not hesitate to experiment with different technologies and solutions Project and community contribution: Need to increase the level of contribution to team projects and participate more actively in AWS community activities Acknowledgments I would like to sincerely thank Amazon Web Services Vietnam Co., Ltd. for providing me with the opportunity to intern and experience a professional work environment. In particular, I would like to express my deep gratitude to:\nMy mentor and team members who enthusiastically guided me, shared their experience, and created the best conditions for me to learn and develop My colleagues who always supported, encouraged, and created a friendly and positive work environment My university for equipping me with a solid knowledge foundation that I could apply in practice This internship has been an invaluable experience, helping me grow both professionally and in soft skills. I hope to have the opportunity to continue working with AWS in the future.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/5-workshop/5.7-security/","title":"Security Implementation","tags":[],"description":"","content":"Security Implementation for Travel Guide Application This section covers the implementation of critical security improvements for the Travel Guide Application, focusing on data protection, input validation, and access control.\nOverview Security is paramount in any web application. The Travel Guide application handles user-generated content, personal data, and file uploads, making it essential to implement robust security measures.\nThree Critical Security Improvements:\nEncryption at Rest - Protect data stored in DynamoDB and S3 Input Sanitization - Prevent XSS and injection attacks S3 Ownership Validation - Prevent unauthorized file access Security Threats Addressed Threat Severity Impact Mitigation Data Breach üî¥ Critical Exposed user data Encryption at rest XSS Attacks üî¥ Critical Code injection HTML sanitization Unauthorized Access üü† High Data leakage Ownership validation File Abuse üü† High Storage costs Size/type validation Tag Spam üü° Medium Poor UX Tag limits Implementation Impact Before Security Updates:\n‚ùå Data stored unencrypted ‚ùå No input validation ‚ùå Users can access others\u0026rsquo; files ‚ùå No file size/type checks After Security Updates:\n‚úÖ All data encrypted (KMS/AES256) ‚úÖ HTML sanitization prevents XSS ‚úÖ Ownership validation enforced ‚úÖ File uploads validated Cost Impact Monthly Cost Increase: $5 (~20%) Breakdown: - DynamoDB KMS encryption: +$5/month - S3 AES256 encryption: FREE - Lambda execution: No change Total: $30/month (from $25/month) Worth it? ‚úÖ Absolutely! Security is not optional.\nContent Encryption at Rest Input Sanitization S3 Ownership Validation Key Takeaways Encryption at rest protects data from breaches Input sanitization prevents XSS and injection attacks Ownership validation prevents unauthorized access Security is continuous - regular audits needed Cost of security is minimal compared to breach costs "},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe workspace at AWS Vietnam provides a comfortable and professional atmosphere. Team members always create an open environment, willing to answer questions and provide support when needed. What stands out is the flexible work culture that allows interns to proactively manage their tasks.\n2. Support from Mentor / Team Admin\nThe mentor not only guides work tasks but also shares practical experiences, helping me better understand the industry and career direction. What I appreciate most is that the mentor encourages me to research and solve problems independently, only intervening when truly necessary. The admin team always provides quick support for administrative matters, documentation, and creates favorable conditions for work.\n3. Relevance of Work to Academic Major\nThe assigned tasks have a good connection with knowledge learned at university, especially regarding cloud computing and web technology. At the same time, I was exposed to real-world technologies and workflows that the academic environment hasn\u0026rsquo;t provided. This helped me clearly see how to apply theory to practice.\n4. Learning \u0026amp; Skill Development Opportunities\nInterning at AWS opened up many learning opportunities: from technical skills like using AWS services, to soft skills like teamwork, time management, and communication. Participating in community events and workshops also helped me expand my network and better understand the cloud computing market in Vietnam.\n5. Company Culture \u0026amp; Team Spirit\nThe work culture here demonstrates mutual respect and support. Everyone works seriously but without creating pressure, always ready to share knowledge and experience. Even as an intern, I was listened to and allowed to contribute to real projects, which was very motivating.\n6. Internship Policies / Benefits\nThe company provides a reasonable internship allowance and allows flexible working hours when needed. Especially, having the opportunity to participate in internal training sessions and AWS events is a valuable opportunity for learning and development. However, having additional long-term mentorship programs would be even better.\nAdditional Questions 1. What did you find most satisfying during your internship?\nWhat I found most satisfying was working in a professional environment with experienced and passionate people. Being involved in a real project (Travel Guide Web App) and AWS community activities helped me not only learn technical knowledge but also better understand how to operate a project from idea to deployment. Additionally, the enthusiastic support from my mentor and team is something I highly value.\n2. What do you think the company should improve for future interns?\nI think the company could improve by building a clearer roadmap for interns from the start, helping them know what goals to achieve at each stage\n3. If recommending to a friend, would you suggest they intern here? Why or why not?\nYes, I would definitely recommend it. AWS Vietnam is an ideal place for those who want to learn about cloud computing and work in an international technology environment. You will learn from talented people, access advanced technology, and have the opportunity to develop both technical and soft skills. However, you need to have a proactive spirit and be ready to continuously learn.\nSuggestions \u0026amp; Expectations 1. Do you have any suggestions to improve the internship experience?\n2. Would you like to continue this program in the future?\nYes, I very much hope to have the opportunity to continue working with AWS, whether as an extended internship or full-time after graduation. The work environment here aligns with my career direction, and I believe I still have much to learn and contribute.\n3. Any other comments (free sharing):\nThank you to AWS Vietnam and especially the team for creating the best conditions for me throughout the internship. This has been an invaluable experience, helping me grow and become more confident in my career path. I hope to have the opportunity to work with AWS in the future and contribute more to the cloud computing community in Vietnam.\n"},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://johnnyle12.github.io/FCJ-Workshop-Report/tags/","title":"Tags","tags":[],"description":"","content":""}]